\documentclass{article}
\usepackage{arxiv}

\usepackage[colorlinks=true, linkcolor=blue, citecolor=red]{hyperref}



%File: anonymous-submission-latex-2025.tex
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
% \usepackage{graphicx} % DO NOT CHANGE THIS
% \urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in} % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in} % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{algpseudocode}

\usepackage{paralist}
%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2025.1)
}


\setcounter{secnumdepth}{2} %May be changed to 1 or 2 if section numbers are desired.



% Use the postscript times font!
\usepackage{times}
%\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{amsmath}
\usepackage{soul}
\usepackage{url}
% \usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage{caption}
\usepackage{graphicx}
\usepackage{graphicx}  % Include the graphicx package
\usepackage{amsmath}
\usepackage{amsthm}


\usepackage{booktabs}


\usepackage{tikz}
\usepackage{pgfplots}
% \usepackage[submission]{aaai24}  % DO NOT CHANGE THIS
% \usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
% \usepackage[hyphens]{url}  % DO NOT CHANGE THIS
% \usepackage{graphicx} % DO NOT CHANGE THIS
% \urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
% \frenchspacing  % DO NOT CHANGE THIS
% \setlength{\pdfpagewidth}{8.5in} % DO NOT CHANGE THIS
% \setlength{\pdfpageheight}{11in} % DO NOT CHANGE THIS
\usepackage{listings}
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
% \usepackage[section]{placeins}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
%\usepackage{hyperref}       % hyperlinks
% \usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
% \usepackage{algorithm2e}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
% \usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors


\usepackage{natbib} % has a nice set of citation styles and commands
    % \bibliographystyle{plainnat}
    % \renewcommand{\bibsection}{\subsubsection*{References}}
\usepackage{mathtools} % amsmath with fixes and additions
% \usepackage{siunitx} % for proper typesetting of numbers and units
\usepackage{booktabs} % commands to create good-looking tables
\usepackage{tikz} % nice language for creating drawings and diagrams
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{array}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{newfloat}
\usepackage{subcaption}
%\usepackage{subfigure}
\usepackage[export]{adjustbox}
\usepackage{amsthm}
\usepackage{soul}
%%%%%%%%%%%%% Packages added by Avyukta.
\usepackage{amssymb}
\usepackage{bibentry}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{tikz}
\usepackage{pgfplots}


\pgfplotsset{compat=newest}
\usepgfplotslibrary{units}


\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{claim}{Claim}[section]
\newtheorem{assumption}{Assumption}[section]

% \graphicspath{ {./images/} }

\newcommand{\sg}[1]{ \textcolor{brown}{{\bf SG: }{``\em #1''}}}
\newcommand{\sd}[1]{ \textcolor{orange}{{\bf SD: }{``\em #1''}}}
\newcommand{\av}[1]{ \textcolor{blue}{{\bf AV: }{``\em #1''}}}
\newcommand{\ouralgo}{\texttt{FROC}}
\newcommand{\ourdef}{-Equalized ROC}
\newcommand{\roc}{\texttt{ROC}_s}
\newcommand{\auc}{\texttt{AUC}_s}


% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
%\usepackage{subfigure}
\usepackage{booktabs} % for professional tables


% Attempt to make hyperref and algorithmic work together better:
% \newcommand{\theHalgorithm}{\arabic{algorithm}}
\newcount\mycount
\mycount=1


% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% % if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

\title{\ouralgo: Building Fair ROC from a Trained Classifier}


\author{
Avyukta Manjunatha Vummintala \\
Machine Learning Lab\\
IIIT Hyderabad\\
\texttt{avyukta.v@research.iiit.ac.in} \\
\And
Shantanu Das \\
Machine Learning Lab\\
IIIT Hyderabad\\
\texttt{shantanu.das@alumni.iiit.ac.in} \\
\And
Sujit Gujar \\
Machine Learning Lab\\
IIIT Hyderabad\\
\texttt{sujit.gujar@iiit.ac.in} \\
}

\begin{document}

\maketitle



\begin{abstract}
This paper considers the problem of fair probabilistic binary classification with binary protected groups. The classifier assigns scores, and a practitioner predicts labels using a certain cut-off threshold based on the desired trade-off between false positives vs. false negatives. It derives these thresholds from the ROC of the classifier. The resultant classifier may be unfair to one of the two protected groups in the dataset. It is desirable that no matter what threshold the practitioner uses, the classifier should be fair to both the protected groups; that is, the $\mathcal{L}_p$ norm between FPRs and TPRs of both the protected groups should be at most $\varepsilon$. We call such fairness on ROCs of both the protected attributes $\varepsilon_p$\ourdef. Given a classifier not satisfying $\varepsilon_1$\ourdef, we aim to design a post-processing method to transform the given (potentially unfair) classifier's output (score) to a suitable randomized yet fair classifier. That is, the resultant classifier must satisfy $\varepsilon_1$\ourdef. First, we introduce a threshold query model on the ROC curves for each protected group. The resulting classifier is bound to face a reduction in AUC. With the proposed query model, we provide a rigorous theoretical analysis of the minimal AUC loss to achieve $\varepsilon_1$\ourdef. To achieve this, we design a linear time algorithm, namely \ouralgo, to transform a given classifier's output to a probabilistic classifier that satisfies $\varepsilon_1$\ourdef. We prove that under certain theoretical conditions, \ouralgo\ achieves the theoretical optimal guarantees. We also study the performance of our \ouralgo\ on multiple real-world datasets with many trained classifiers.
\end{abstract}
\section{Introduction}
\label{sec:intro}
% \cite{chen2020towards}
The use of \emph{Machine Learning based Models} (MLM) in decision-making is prevalent today. Practitioners use MLMs' predictions in college admissions, credit scores, recidivism, employment, recommender systems, etc.~\cite {portugal18,berger05}. However, there have been several reports of such MLMs discriminating against individuals belonging to certain groups based on \emph{protected attribute} such as gender, age, race, color, and religion. E.g., in ~\cite{angwin16}, predictive models are found to be biased against the black population, or the Amazon recruitment team has to stop using the AI tool for shortlisting candidates as it was biased against females~\cite{dastin18}. ~\cite{bickel75};~\cite{berger05};~\cite{zhao18} show that many of such predictive models are unfair to females. Such unfair instances have driven researchers toward building a fair MLM. 

An MLM that achieves fairness with the least possible compromise on traditional performance guarantees such as accuracy is \emph{desirable} MLM. Building a desirable MLM involves two main steps: a) formalizing and quantifying a fairness measure and b) designing algorithms to train MLM for quantified fairness. 
%
Researchers proposed many fairness measures, majorly belonging to two categories: (i) \emph{individual fairness}~\cite{dwork12} -- individuals with similar input features receive similar decision treatment irrespective of their protected attribute. (ii) \emph{Group fairness} -- a particular statistical property must be similar across each protected group, e.g., \emph{Disparate Impact (DI)},\emph{Equalized odds (EO)}~\cite{madras2018learning}.\\
%, the false positive and false negative error rates should be identical across protected groups, is one of the popular group fairness notions.
\noindent\textbf{Building Fair MLM} Fair machine learning models (MLMs) can be developed by targeting different stages of the model training cycle. Approaches include:
(i) \emph{Pre-processing} methods, which act on input data to eliminate bias \citep{feldman15, zemel13}.
(ii) \emph{In-processing} algorithms, which intervene during training to incorporate fairness as a constraint or within the learning objective \citep{padala21}.
(iii) \emph{Post-processing} methods, which adjust the outputs of trained MLMs to produce fair results, requiring access to sensitive attributes.

In-processing and pre-processing methods are tailored to specific fairness criteria and models, necessitating retraining for each new fairness definition. Post-processing methods, in contrast, are model-agnostic and do not depend on the training process, making them suitable for domain experts with limited MLM knowledge \citep{sleeman95}. These methods are especially favored when retraining is infeasible, such as in large-scale systems like recommender systems \citep{nandy22}.

Given a potentially biased scoring function, this paper addresses the challenge of constructing a fair probabilistic binary classifier with a binary-protected attribute. The goal is to ensure fairness without retraining the MLM, minimizing performance loss.

\noindent\textbf{Fairness and Performance Trade-offs} For classification, one of the desired characteristics of an MLM is \emph{calibration}~\cite{kleinberg17}. Suppose a classifier predicts that a given input is accepted ($Y=1$) with probability $p$, then calibration demands that the fraction of the accepted population, with the same features, is $p$. \cite{kleinberg17,chouldechova17} have shown that calibration and equalized odds cannot be satisfied simultaneously except for highly constrained cases. Hence, researchers have been focusing on building classifiers (MLMs) with an appropriate approximate version of fairness~\cite{madras2018learning}. 
When it comes to practitioners, they focus on \emph{Receiver Operator Characteristics} (ROC) for evaluating a classifier as it best describes the classifiers. ROC measures the relative scores of the positive versus negative instances. The area under ROC-curve (AUC) is an appropriate performance metric to measure the predictive quality of such classifiers and to segregate positive and negative samples through ranking (\cite{huang05,clemencon08,zehlike21}). AUC is particularly beneficial when the classifier is expected to segregate positive and negative labels, and the predictions must be fair across all threshold scores.

To make the practitioner's job effortless, we introduce a novel fairness measure, namely $\varepsilon_p$\ourdef\ -- no matter what threshold it uses for classification, the classifier is approximately fair, i.e., for all possible thresholds, the distance between the corresponding points of the ROC curves for both the protected group should be withing $\varepsilon$ distance in the $\mathcal{L}_p$ norm.
We aim to build a new probabilistic classifier that satisfies $\varepsilon_1$\ourdef\ with the minimal loss in AUC w.r.t. to the scoring function $s$.

\noindent\textbf{Our Approach: }
We assume query access to the ROC of $s$. First, we make sufficiently large $k$ queries to the ROC for the protected groups and make a piece-wise linear approximation of the ROC curves of both the protected groups. Next, we transport ROCs within $\varepsilon$ distance of each other to minimize the loss in AUC of the resultant ROC. We can achieve such transportation by randomizing scores across certain feasible classifiers for the given ROC curve. We call the space of these classifiers as \emph{ROC Space} of $s$. The resultant classifier from such randomization across the ROC Space is a convex combination of these classifiers. In a nutshell, we \emph{transform} the given $s$ to a fair scoring function by such ROC transport. We refer to this procedure of ROC transport as \ouralgo. We then geometrically prove that under certain conditions, \ouralgo\ is \emph{optimal}.\\
\noindent\textbf{Our Contributions: }
\begin{compactitem}
    \item We introduce a novel group fairness notion $\varepsilon_p$\ourdef, enforcing fairness over all thresholds in a score-based classification, {which is extremely useful for practitioners.}
    \item Next, we model a post-processing problem as a problem of finding an optimal transformation $\mathcal{H}$ on a given scoring function $s$ to minimize the performance loss due to transformation while ensuring $\varepsilon_1$\ourdef.  
    \item To achieve $\varepsilon_1$\ourdef, we propose a ROC transport, \ouralgo, a \emph{post-processing} algorithm  (Algorithm~\ref{alg:fairroc}). {Thus, it avoids re-training the existing MLM, which might not be fair. It also helps in explaining the decisions.}
    \item We perform rigorous theoretical analysis. We prove that (under some conditions) \ouralgo\ is optimal in terms of AUC loss. (Theorem~4.2).
    \item Finally, we demonstrate the efficacy of \ouralgo\ via experiments.
\end{compactitem}


%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Related Work}
%%%%%%%%%%%%%%%%%%%%%%%%%%
\textbf{Fairness in Binary Classification and Ranking}
%%%%%%%%%%%%%%%
\emph{Demographic Parity} (DP), \emph{Disparate Impact} (DI), and \emph{Equalized Odds} (EO) are widely studied group fairness notions. DP \citep{dwork12} and DI \citep{feldman15} ensure that the fraction of positive outcomes is identical across all sensitive groups. \cite{barocas16} introduced the $80\%$ rule, requiring that the positive outcome rate for a minority group must be at least $4/5$ of that for the majority group. EO \citep{hardt16} ensures similar distributions of error rates, specifically false positives and false negatives \citep{verma18}. Techniques to achieve fair MLMs include those discussed by \cite{padala21}.
Group fairness has been shown to be inadequate for score-based classifiers, which classify across all thresholds \citep{gorantla21}. Consequently, researchers have proposed fairness notions based on the area under the curve (AUC). Examples include \emph{intra-group pairwise} AUC fairness \citep{beutel19}, \emph{BNSP} \citep{borkan19}, and \emph{inter-group pairwise} AUC (xAUC) fairness \citep{kallus19}. \cite{yang2023minimax} present a minimax learning and bias mitigation framework that integrates intra-group and inter-group AUC metrics to address algorithmic bias.
\cite{vogel2021} examine fairness in ranking problems, developing a general class of AUC-based fairness notions. They demonstrate that AUC-based fairness notions do not capture all forms of bias, as AUC summarizes classifier performance. They propose a stronger notion called point wise ROC-based fairness and design an in-processing algorithm for this purpose.

Our fairness definition ($\varepsilon_p$\ourdef) is inspired by equalized odds for all thresholds in ranking-based classification and is suitable for post-processing algorithms. It generalizes the approach of \cite{chen2020towards}, which uses the Manhattan distance as its norm. We later demonstrate the equivalency of both fairness notions {(ours $\varepsilon_1$). Note that the notion in \cite{chen2020towards} is not motivated by the same error rates at all thresholds,  and also, ours is more of a geometric approach from ROC curves, and theirs is an algebraic approach; ours is more general.}

\noindent \textbf{Post-processing for fair classification}
Post-processing techniques range from simple adjustments, such as thresholding or re-scaling, to complex methods like re-weighting or re-sampling. \cite{hardt16} argue that many existing fairness criteria are too restrictive, leading to sub-optimal solutions. They propose a fairness notion allowing some variation in prediction outcomes, defined by ``equality of opportunity'' constraints, ensuring the classifier is unbiased regarding the sensitive attribute. Their approach involves adjusting prediction thresholds for different groups based on their base rates to equalize false positive and false negative rates across groups. However, it does not involve \textit{transporting} ROC curves.
\cite{wei20} examine post-processing from the perspective of transformers, defining fairness as the expectation of scores and bounding the differences between true positive rates (TPRs) and false positive rates (FPRs) across protected groups. \cite{cui2021} propose a model-agnostic post-processing framework for balancing fairness in bipartite ranking scenarios.
\cite{zhao2024fair} introduces a novel approach using Wasserstein barycenters to quantify and address the cost of fairness, demonstrating that the complexity of learning an optimal fair predictor is comparable to learning the Bayes predictor.
\cite{tifreafrappe} propose a framework that transforms any regularized in-processing method into a post-processing approach, extending its applicability across a broader range of problem settings. 
\cite{cruz2023unprocessing} identifies two key methodological errors in prior work through empirical analysis: comparing methods with different unconstrained base models and differing levels of constraint relaxation.
\cite{jang22} introduce a method to optimize multiple fairness constraints through group-aware threshold adaptation, learning classification thresholds for each demographic group by optimizing the confusion matrix estimated from the model's probability distribution. Unlike \cite{jang22}, our approach starts with the fairness notion that differences between TPRs and FPRs of different groups must be bounded.
\cite{mishler2021fairness} use the bounded difference of counterfactual TPRs and FPRs as their fairness criterion, which differs from our $\varepsilon_p$\ourdef\ definition. Our $\varepsilon_p$\ourdef\ focuses on the bounded difference between TPRs and FPRs of different groups as the fairness criterion.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Preliminaries}
\label{sec:prelim}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Consider a practitioner interested in binary classification, each data point having a binary-protected attribute. He/she is equipped with a scoring-based classifier trained on dataset $D=\{(x_i,a_i,y_i)_{i\in 1:n}\}$. Here, for $i$th data sample, $x_i \in \mathcal{X}\subset \mathbb{R}^d$ denotes features, $y_i\in\{0,1\}$ denotes the binary label, and $a_i\in\mathcal{A}=\{0,1\}$ denotes its binary protected attribute. We consider all these three as drawn from random variables $X, A, Y$, respectively.
%$(x_i)_{i \in [1:n]}$ be our feature vector set of size $n$ and dimensionality $d$. Let $(y_i)_{i \in [1:n]}$ be the set of binary labels corresponding to $(x_i)_{i\in[1:n]}$.
%Let a variable $a_i \in  \{0,1\} \forall i \in [1:n]$ capture the protected attribute of feature vectors.
There could be two scenarios - when the protected attribute is included or excluded from training (\cite{wei20})â€”our post-processing works for both cases as long as protected attributes are accessible during post-processing.

%For the sake of our analysis, we shall model sets  $(x_i)_{i\in[1:n]} , (y_i)_{i\in[1:n]}$ and $(a_i)_{i\in[1:n]}$ as random variables $X, Y$ and $A$.
% Let $X$ be a random variable representing the input features \st{of the population} and $Y$ be the corresponding binary target variable; $\mathcal{X} \subseteq \mathbb{R}^n$. 
The random variables $X, A, Y$ are jointly distributed according to an unknown probability distribution over $(x_i, a_i, y_i)$. The cumulative conditional distributions on ${X\mid( Y=1)}$  and $X\mid (Y=0)$ are denoted by $G, H$, respectively. $ G^a, H^a$ are the corresponding distributions conditioned on $A=a$ (i.e. $G^a$ denotes the distribution of $X\mid (Y =1 , A = a)$) 
% Now, we describe the probabilistic classifier. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Probabilistic Binary Classification}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Typically a scoring-based binary classifier has a scoring function 
Probabilistic Binary Classifier is equipped with a scoring function $s:\mathcal{X} \times \mathcal{A} \rightarrow \mathbb{R}$ mapping the feature space to a score.  
A deterministic classifier returns $s(X) \in \{0,1\}$ and a randomized one returns $s(X) \in [0, 1]$.
The higher the score $s(x)$, the higher the chance of the corresponding label $y = 1$.
% Such, later, real-valued score functions are also called \emph{probabilistic} classifiers.
% In this case, the practitioner 
The model prediction $\widehat{Y}$ , based on certain threshold $t\in [0,1]$, is given by $\widehat{Y} = \mathbb{I}(s(X) \geq t)$. $\mathcal{S}$ denotes the space of such scoring functions. 

The practitioner decides the threshold $t$ depending on the corresponding true positive rate (\emph{TPR}) and false positive rate (\emph{FPR}) (\cite{provost00,zhou05}). For deciding $t$, he is supplied with ROC -- \emph{receiver operator characteristic curve} for $s$. The ROC depicts the relation between TPR ($G_s(t)$) and FPR ($H_s(t)$) for $s$ at all possible thresholds $t$.  Note that, 
$G_s(t) \triangleq \mathbb{P}(s(X) \geq t\mid Y = 1)$ and $ H_s(t) \triangleq \mathbb{P}(s(X) \geq t\mid Y = 0)$.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{ROC Curve and AUC}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\cite{vogel2021} formulate a general definition of ROC-curve. 
The plot of a ROC-curve 
(Definition \eqref{def:roc}) 
is used to visualize homogeneity between two cumulative distributions~\cite{vogel2021}. 
The ROC curve is defined as:
\begin{definition}[ROC-Curve] \label{def:roc}
For any two cumulative distributions $g_1, g_2$ defined over the set $\mathbb{R}$, the ROC-curve is defined as the plot of 
$
    ROC_{g_1,g_2}(\alpha) \triangleq 1 - g_1 \circ g_2^{-1}(1-\alpha)
$
with domain $\alpha \in [0,1]$.
\end{definition}

The area under ROC-curve, \emph{AUC}, represents a summary of point-wise dissimilarity between the concerned distributions. Formally, let $S, S'$ be two independent random variables distributed according to $g_1, g_2$ respectively, then $AUC_{g_1,g_2} = \mathbb{P}(S'>S) + \frac{1}{2} \mathbb{P}(S' = S)$.
% \begin{equation*}
%     AUC_{h,g} = \mathbb{P}(S'>S) + \frac{1}{2} \mathbb{P}(S' = S)
% \end{equation*}

For a given scoring function $s$, we get two RVs, $G_s$ and $H_s$, by varying decision thresholds. We call the corresponding ROC curve $\roc$.
The area under $\roc$, i.e., $\auc =AUC_{H_s, G_s}$, is used to measure the ranking performance of a score function $s(.)$ (\cite{cortes03}; \cite{clemencon08}). 
% It is the probability that a random input feature with a positive label ($Y=1$) is scored higher than a random input feature with a negative label ($Y=0$). 
For a perfect classifier, $\auc = 1$, but such a classifier does not exist. Therefore, the optimal scoring function $s^{*}$ maximizes the $\auc$ amongst a certain subset of $\mathcal{S'}\subset \mathcal{S}$. Formally, $ s^* \in \argmax_{s\in \mathcal{S'}} \auc$.
% \begin{equation}\label{eq:optscore}
%     s^* \in \argmax_{s\in \mathcal{S'}} \auc
% \end{equation}
In section ~\ref{ssec:randomclass}, we illustrate how a sub-optimal score function with lower TPRs can be achieved by randomizing outputs of $s(\cdot)$. This process is crucial in ensuring fairness. Let $\mathcal{S}|_s$ be the space of possible scoring functions through such randomization. We call it ROC-space of $s$.
Before designing our fair classifier, we formally define our notion of fairness in the next section.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Fairness in Classification}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The typical group fairness notions in binary classifiers such as \emph{Demographic Parity }(DP) and \emph{Equalized Odds} (EO) are defined on deterministic predictions, i.e., in score-based classification, they work with a single threshold on scoring function $s$. Let $t^*$ be the threshold set by the practitioner. The resultant classifier is said to satisfy DP if
$
    G_s^0(t^*) + H_s^0(t^*) = G_s^1(t^*) + H_s^1(t^*)
$.
It satisfies the equivalence of \emph{acceptance rates} across groups.
Similarly, EO enforces equality of positive and negative error rates across protected groups, $1 - G_s^0(t^*) = 1 - G_s^1(t^*)$ and $H_s^0(t^*) = H_s^1(t^*)$.
% \begin{equation*}
% \begin{aligned}
%     1 - G_s^0(t^*) & = 1 - G_s^1(t^*) \;\&\;
%     H_s^0(t^*) & = H_s^1(t^*) 
% \end{aligned}    
% \end{equation*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection*{$\varepsilon_p$\ourdef}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
As discussed earlier, all group fairness notions are characterized by equality of a particular statistic across both the protected groups.  
% In this work, we focus on a practitioner with a scoring function $s\in\mathcal{S}$, which is found to be unfair across protected groups in $\mathcal{A}$.
In scoring-based probabilistic classifiers, these fairness notions depend on the selected threshold.
To achieve fairness across all thresholds, the practitioner can choose to retrain the model and achieve the right trade-offs between TPR and FNR.
% One option is to retrain 
% the model to achieve the right trade-offs between fairness, TPR, and FNR. 
% But, the practitioner may not be interested in retraining the classifier again. 
However, retraining is expensive. Therefore, a desirable solution is 
% to use the given classifier and offer fair treatment to both the protected groups.
To offer fair treatment to both protected groups using the pre-trained classifier.
However, this leads to invoking the post-processing technique every time the practitioner needs to update the threshold $t^*$. 
Instead, we propose a novel fairness measure to simplify the practitioner's job.
We perform post-processing on the given classifier once, and it ensures that no matter what threshold $t^*$ they choose to make decisions, the classifier offers similar treatment to both the protected groups. That is, the individual ROCs (Here on, we shall denote the ROCs of the protected groups, i.e., $ROC_{H_s^{0}, G_s^{0}}$ and $ROC_{H_s^{1}, G_s^{1}}$ by $\roc^0$ and $\roc^1$ respectively) should be within $\varepsilon$ distance ($\mathcal{L}_p$ norm) of each other.   We call it \emph{$\varepsilon_p$\ourdef}. More formally, 
\begin{definition}[$\varepsilon_p$\ourdef] \label{def:eroc}
A scoring function for binary classification $s$ with label prediction $\widehat{Y} = \mathbb{I}(s(x) \geq t)$ is said to satisfy \ourdef\ if for all $\alpha \in (0,1)$ the following holds:
\begin{equation}
    {\mid \mid \roc^1(\alpha) - \roc^0(\alpha) \mid \mid}_p \leq \varepsilon
\end{equation}
\end{definition}


\noindent In $\varepsilon_p$\ourdef, we utilize standard metrics (i.e. $\mathcal{L}_p$ norms) as the fairness statistic to quantify fairness. Thus, $\varepsilon_p$\ourdef\ is feasible for post-processing algorithms.
Next, we formulate the problem of fair post-processing. Note: $\varepsilon_1$\ourdef\ is a generalization of Equalized Odds to all the given thresholds of the scoring function. The proofs and detailed discussion are in Appendix B. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Problem Formulation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Given $s\in \mathcal{S}$, we would like to find $h\in\mathcal{S}|_s = \mathcal{H}(s)$ -- a transformation of a given scoring function such that $h$ satisfies $\varepsilon_1$\ourdef. Additionally, we want the loss in AUC due to transformation $\mathcal{H}$ minimal. That is, $\mathcal{L}_{F} = \auc - \texttt{AUC}_h$ must be minimal to retain the maximum performance guarantee of $s$. Thus, our goal is to get transformation $\mathcal{H}$ that solves the following optimization problem and returns the optimal transformed score $h^*$: 


\begin{equation} \label{eq:fpp}
%\begin{aligned}
    h^* \in \argmax_{h \in \mathcal{S}|_s} \; \texttt{AUC}_h 
    %\end{aligned}
\end{equation}
    \[\text{s.t. } \mid \mid {\texttt{ROC}_{h}}^0(\alpha) - {\texttt{ROC}_{h}}^1(\alpha)\mid \mid_1 \leq \varepsilon , \;
    \forall \alpha \in [0,1]
\]
% In the next section, we build \ouralgo\ to solve the above optimization problem and achieve $\varepsilon_1$\ourdef\ fairness of the above optimization problem through post-processing. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Our Approach}
\label{sec:froc}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
First, we explain query access to $\roc$ to sample from the desired statistic at various thresholds and its piece-wise linear approximation in Section~\ref{ssec:query} and Section~\ref{ssec:pla}, respectively. Since we cannot sample a continuum of thresholds, our $\roc$ will be discrete. In Section~\ref{ssec:ouralgo}, we describe the transport of ROCs.
% within $\varepsilon$ distance of each other. 
Finally, we summarize our transformation as \ouralgo\ in Section~\ref{ssec:randomclass}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Query Model} \label{ssec:query}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Let $\mathcal{T} = \{ t_1, \ldots t_k \}$ be the set of thresholds at which we sample $\roc$ for each sensitive group ($t_i=\frac{i}{k}$).
Let $\mathcal{Q}^a(t_i)$ denote the query output at threshold $t_i$ for sensitive group $A = a$ on the $\roc^a$.
$
    \mathcal{Q}^a(t_i) \triangleq ROC_{H_s^{a}, G_s^{a}}(t_i)
$.

Abusing notations, we use $\mathcal{Q}^a(t_i)$ and $\mathcal{Q}^a_i$ interchangeably. Let $\mathcal{Q}^a = (\mathcal{Q}^a_1, \ldots, \mathcal{Q}^a_k)$ be the sequence of all query outputs for group $a$. In the next section, we construct the piece-wise linear approximation of the group-wise ROC curves using the group-wise query outputs $\mathcal{Q}^a$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Piece-wise Linear Approximation (PLA) of ROC-curves} \label{ssec:pla}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
To obtain the piece-wise linear approximation (PLA), we sample $k$ points from ROC and construct a straight line from $\mathcal{Q}_i^a$ to $\mathcal{Q}_{i+1}^a$ for all $i = 1 \ldots k-1$. Lastly, we join $(0,0)$ to $\mathcal{Q}_1^a$ (see Figure \ref{fig:losslpa}). Following these steps on the query sets $\mathcal{Q}^a$ will generate the PLAs for protected groups $a \in \{0,1\}$.
We denote by $\widehat{G_s^{a}}, \widehat{H_s^{a}}$, the cumulative distributions induced by the linear approximation of the ROC-curve on $s$.

Due to PLA, we incur a loss $\mathcal{L}_{LPA}$ in $AUC_{H_s, G_s}$(shaded region in Figure \eqref{fig:losslpa}). $\mathcal{L}_{LPA}$ is inversely proportional to the number of queries $k$, see Section \ref{ssec:lossplabound} for bounds on this loss. Hence, we shall ignore this loss in our fairness analysis as it can be brought  arbitrarily close to $0$ by increasing $k$.  


\begin{figure*}
    \centering
    \hspace{-1.5cm}
    \begin{minipage}{0.3\linewidth}
         \centering
         \includegraphics[scale = 0.07]{diagrams/Conv_Modified_2.jpg}
         \caption{ROCs and convex hull}
         \label{fig:conv}
     \end{minipage}
    \hspace{1.5cm}
     \begin{minipage}{0.3\linewidth}
         \centering
         \includegraphics[scale = 0.05]{diagrams/PLA_Modified_2.jpg}
         \caption{Shaded Area indicates $\mathcal{L}_{PLA}$}
         \label{fig:losslpa}
     \end{minipage}
     \begin{minipage}{0.3\linewidth}
         \centering
         \includegraphics[scale = 0.07]{diagrams/Norm_Boundary_modified_2.jpg}
         \caption{Norm Boundary}
         \label{fig:five over x}
     \end{minipage}
        \label{fig:three graphs}
        % \caption{(a) illustrates the abstraction of convex hull from ROCs. (b) uses the shaded area to illustrate the AUC loss due to Linear Interpolation. (c) introduces the notion of Norm Boundary.}
\end{figure*}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Transporting ROCs for $\varepsilon_1$\ourdef} \label{ssec:ouralgo}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


Since we are using post-processing technique to ensure fairness, it is impossible to shift any \texttt{ROC} above its current position, i.e., build a classifier corresponding to any point in the epigraph (the points above the ROC curve) of $\roc$ just with the help of $s$. Interestingly, a classifier representing a point in the hypograph (points below the curve) of $s$ $\cap \mathcal{S}$ can be obtained through randomization on the predicted scores (see Chapter 3 in~\cite{fairmlbook19}). The key idea involves abstracting out the convex hull (Fig \ref{fig:conv}) formed by the three points $(0,0)$, $(1,1)$ and $\mathcal{Q}_i^{up}$, and sampling outcomes from classifiers representing $(0,0)$, $(1,1)$\footnote{Note that $(0,0)$ and $(1,1)$ represent `always reject' and `always accept' classifiers.}  and $\mathcal{Q}_i^{up}$ with specific probabilities. By taking convex combinations of the three aforementioned points in the ROC space, we can represent any point lying in their convex hull. The exact convex combinations are described in \textbf{C2}. We leverage this property to achieve $\varepsilon_1$\ourdef. We denote this space as \emph{ROC-space of $s$} -- $\mathcal{S}|_s$. Each point in $\mathcal{S}|_s$ represents a binary classifier in terms of its performance at a certain threshold $t$. Each point is of the form $(FPR(t), TPR(t))$. 

In the realm of binary classification, it is a common occurrence for one group to be subject to discrimination. Specifically, if we plot $\roc^0,\roc^1$, we will find that one of the ROCs is notably situated below the other.
For this study, the ROC predominantly above the other will be designated as $ROC_{up}$, while the other ROC will be referred to as $ROC_{down}$. We believe this is a reasonable assumption because we observed that in most classifiers (for which present the results and others we explored on the datasets mentioned in Section~E3) the ROCs don't intersect or intersect at regions where $FPR \le 0.2$ or $TPR \ge 0.5$. Typically, no practitioner will work in those areas of ROCs. We leave for future work to address intersecting ROCs.
% It has been experimentally observed that in most cases the ROCs don't intersect or if they do, they do so in regions where $FPR \le 0.2$ or $TPR \le 0.5$. 
%So, \ouralgo~will be applied in the region where one of the ROCs is definitively above the other.

Let $\mathcal{Q}^{up}, \mathcal{Q}^{down}$ be the corresponding set of query points for $\texttt{ROC}_{up}, \texttt{ROC}_{down}$ respectively. We also denote their fair counterparts by $\widetilde{\mathcal{Q}}^{up}, \widetilde{\mathcal{Q}}^{down}$.

% We now define a few preliminary notions required to build our algorithm, \emph{\ouralgo} that returns ROCs for $h^*$ from $\roc^0,\roc^1$ and satisfies $\varepsilon_1$\ourdef.
%%%
\subsubsection{Algorithm Definitions}
%%%%
We need to transport $\texttt{ROC}_{up}$ towards $\texttt{ROC}_{down}$ such that the new ROCs are within $\varepsilon$ distance of each other. Our approach is geometric. We need to identify certain points/curves in the epigraph of $\texttt{ROC}_{down}$ as follows.
\begin{definition}[Norm Boundary] \label{def:eNBoundary}
The set of all points within $\varepsilon$ distance ($\ell_1$ norm) from $\mathcal{Q}_i^{down}$ is known as the \emph{norm set} $\mathfrak{C}_i$. Formally, we have \[\mathfrak{C}_i \triangleq \{ x: x\in [0,1]^2 , ||x - \mathcal{Q}_i^{down}||_1 \le \varepsilon\}\]
The set of all points exactly $\varepsilon$ distance (in $\mathcal{L}_1$ norm) from $\mathcal{Q}_i^a$ is known as \emph{Norm Boundary} $\mathfrak{B}_i$. Formally, 
\[\mathfrak{B}_i \triangleq \{ x: x\in [0,1]^2 , ||x - \mathcal{Q}_i^{down}||_1 = \varepsilon\}\]
Additionally, we denote the vertices of the Norm Boundary Rhombus (starting from the top most point and moving clockwise) as $U_i$, $R_i$, $D_i$, and $L_i$.
\end{definition}


We say that an index $i \in [1 ,2 , \hdots, k]$ is a Boundary Cut index when $ROC_{up}$ intersects the Norm Boundary $\mathfrak{B}_i$. Formally,
\begin{definition}[Boundary Cut] \label{def:BCut}
 Index $i \in [1 ,2 , \hdots, k]$ is a \emph{Boundary Cut index} when $\mathfrak{B}_i \cap ROC_{up} \neq \phi$.
\end{definition}

We now define the three kinds of shifts that will be used in our Algorithm:
For a given $ i \in [1 , 2, \hdots ,k]$, Upshift is the transportation of $\mathcal{Q}_i^{up}$ to the point $U_i$. 
\begin{definition}[UpShift] \label{def:csh}
    For a given $ i \in [1 , 2, \hdots ,k]$, Upshift is the transportation of $\mathcal{Q}_i^{up}$ to the point $U_i$. Formally, \emph{UpShift} can be defined as the function that returns a fair threshold $\widetilde{\mathcal{Q}}_i^{up}$ (i.e. $U_i$) by taking the $\mathcal{Q}_i^{down}$ and $\varepsilon$ as the arguments.
\end{definition}

For a given $ i \in [1 , 2, \hdots ,k]$, Leftshift is the transportation of $\mathcal{Q}_i^{up}$ to the point $L_i$. Formally,
\begin{definition}[LeftShift] \label{def:csh}
LeftShift is a function that returns a fair threshold $\widetilde{\mathcal{Q}}_i^{up}$ (i.e. $L_i$) by taking the $\mathcal{Q}_i^{down}$ and $\varepsilon$ as the arguments.
\end{definition}


\begin{definition}[CutShift] \label{def:ush}
    For a given $i \in [1 , 2, \hdots ,k]$ (representing the index of the $ROC_{down}$), we run through all the points of the $ROC_{up}$ and return the set of all points that intersect the Norm Boundary $\mathfrak{B_i}$. Formally, we define \emph{Cutshift} as a function that takes $\mathcal{Q}_i^{down}$ and $\varepsilon$ as the arguments and returns $ROC_{up} \cap \mathfrak{B}_i$. The set $ROC_{up} \cap \mathfrak{B}_i$ can be represented as $\{p_{left} , p_{right}\}$ denoting the points at the intersection of $ROC_{up}$ at the \textbf{left-side} of the Norm Boundary and the \textbf{right-side} of the Norm Boundary respectively.
\end{definition}





Now, we elaborate on the above procedure to transport points from $ROC_{up}$ towards $ROC_{down}$.

%%%%%%%%%%%%%%%%
\subsubsection{Algorithm for ROC Transport}
%%%%%%%%%%%%%%%%
We provide a geometric algorithm that returns a classifier equivalent to the scoring function $h^*$ in $\mathcal{S}|_s$.%(the solution to the practitioner's optimization problem~\ref{eq:fpp}).





\begin{algorithm}
\caption{\textsc{FairROC Algorithm}}
\label{alg:fairroc}
\begin{algorithmic}[1]
\Require $ROC_{up}$, $ROC_{down}$, $\varepsilon$
\Ensure $FairROC_{up}$, $FairROC_{down}$
\State Initialize $i \gets 1$, $k \gets \text{length}(ROC_{up})$
\State $FairROC_{up} \gets \emptyset$, $FairROC_{down} \gets ROC_{down}$

\While{$i < k-1$}
    \State $i \gets i+1$
    \If{\Call{BoundaryCut}{$i, \varepsilon$} == TRUE}
        \State $p_{left}, p_{right} \gets \Call{CutShift}{i, ROC_{up}, ROC_{down}}$
        \If{$FPR(\mathcal{Q}_i^{up}) \geq FPR(\mathcal{Q}_i^{down})$}
            \State $\widetilde{\mathcal{Q}}_i^{up} \gets p_{right}$
        \Else
            \State $\widetilde{\mathcal{Q}}_i^{up} \gets p_{left}$
        \EndIf
    \ElsIf{$\mathcal{Q}_i^{up} \in \Call{Hypograph}{ROC_{down}}$}
        \State $\widetilde{\mathcal{Q}}_i^{up} \gets \mathcal{Q}_i^{up}$
        \State \textbf{continue}
    \Else
        \If{$\text{Area}(\square \mathcal{Q}_{i+1}^{up} \mathcal{Q}_{i}^{up} \mathcal{Q}_{i-1}^{up} {L}_i) \geq \text{Area}(\square \mathcal{Q}_{i+1}^{up} \mathcal{Q}_{i}^{up} \mathcal{Q}_{i-1}^{up} {U}_i)$}
            \State $\widetilde{\mathcal{Q}}_i^{up} \gets U_i$
        \Else
            \State $\widetilde{\mathcal{Q}}_i^{up} \gets L_i$
        \EndIf
    \EndIf
    \State $FairROC_{up} \gets \Call{Append}{\widetilde{\mathcal{Q}}_i^{up}}$
\EndWhile

\end{algorithmic}
\end{algorithm}



% \SetKwComment{Comment}{/* }{ */}
% \RestyleAlgo{ruled}


% \begin{algorithm}[H]
% \caption{\ouralgo}
% \label{alg:fairroc}
% \KwData{$ROC_{up}$, $ROC_{down}$, $\varepsilon$}
% \KwResult{$FairROC_{up}$, $FairROC_{down}$}
% Initialize $i \gets 1$, $k \gets \text{length}(ROC_{up})$\;
% $FairROC_{up} \gets \emptyset$, $FairROC_{down} \gets ROC_{down}$\;

% \While{$i < k-1$}{
%     $i++$\;\\
%     \eIf{BoundaryCut($i, \varepsilon$) == TRUE}{
%         $p_{left}, p_{right} \gets \text{CutShift}(i, ROC_{up}, ROC_{down})$\;
%         \eIf{$FPR(\mathcal{Q}_i^{up}) \geq FPR(\mathcal{Q}_i^{down})$}{
%             $\widetilde{\mathcal{Q}}_i^{up} \gets p_{right}$\;
%         }{
%             $\widetilde{\mathcal{Q}}_i^{up} \gets p_{left}$\;
%         }
%     }{
%         \eIf{$\mathcal{Q}_i^{up} \in \text{Hypograph}(ROC_{down})$}{
%             $\widetilde{\mathcal{Q}}_i^{up} \gets \mathcal{Q}_i^{up}$\;
%             \textbf{continue}\;
%         }{
%             \eIf{$\text{Area}(\square \mathcal{Q}_{i+1}^{up} \mathcal{Q}_{i}^{up} \mathcal{Q}_{i-1}^{up} {L}_i) \geq \text{Area}(\square \mathcal{Q}_{i+1}^{up} \mathcal{Q}_{i}^{up} \mathcal{Q}_{i-1}^{up} {U}_i)$}{
%                 $\widetilde{\mathcal{Q}}_i^{up} \gets U_i$\;
%             }{
%                 $\widetilde{\mathcal{Q}}_i^{up} \gets L_i$\;
%             }
%         }
%     }
%     $FairROC_{up} \gets \text{Append}(\widetilde{\mathcal{Q}}_i^{up})$\;
% }
% \end{algorithm}


% \begin{algorithm}[!h]
% \caption{\ouralgo}\label{alg:fairroc}
% \begin{small}
    
% \KwData{$ROC_{up}$ , $ROC_{down}$ , $\varepsilon$ }
% \KwResult{$FairROC_{up}$ , $FairROC_{down}$}
% Initialize: $i \gets 1$ and $k \gets length(ROC_{up})$\;
% % $X \gets x$\;
% % $N \gets n$\;
% $FairROC_{up} \gets \emptyset \& FairROC_{down} \gets ROC_{down}$\;

% \While{$i \neq k$}
% {
%     $i++$\;
%     \eIf{BoundaryCut($i , \varepsilon$) == TRUE }
%     {
%         $p_{left} , p_{right} \gets CutShift(i , ROC_{up} , ROC_{down}$)\;
%         \eIf{$FPR(\mathcal{Q}_i^{up}) \ge FPR(\mathcal{Q}_i^{down})$}
%         {
%               $\widetilde{\mathcal{Q}}_i^{up} \gets p_{right}$\;
%         }
%         {
%             $\widetilde{\mathcal{Q}}_i^{up} \gets p_{left}$\;
%         }
%     }
%     {
%         \eIf{$\mathcal{Q}_i^{up} \in Hypograph(ROC_{down})$}
%         {
%             $\widetilde{\mathcal{Q}}_i^{up} \gets \mathcal{Q}_i^{up}$\;
%             continue;\
%         }
%         {
%             \eIf{$Area(\square \mathcal{Q}_{i+1}^{up}\mathcal{Q}_{i}^{up}\mathcal{Q}_{i-1}^{up}{L}_i) \ge Area(\square \mathcal{Q}_{i+1}^{up}\mathcal{Q}_{i}^{up}\mathcal{Q}_{i-1}^{up}{U}_i$)}
%             {
%                 $\widetilde{\mathcal{Q}}_i^{up} \gets U_i$\;
%             }
%             {
%                 $\widetilde{\mathcal{Q}}_i^{up} \gets L_i$\;
%             }
%         }
%     }
    
%     $FairROC_{up} \gets Append(\widetilde{\mathcal{Q}}_i^{up})$\;
    
% }
% \end{small}
% \end{algorithm}
Note that, Algorithm \ref{alg:fairroc} treats $ROC_{down}$ as \emph{implicitly} fair.
Also, by $Area(\square ABCD)$, we denote the area of the quadrilateral whose vertices are $A,B,C$, and $D$. This area is easily found in this context by splitting $\square ABCD$ into two disjoint triangles- $\Delta ABC$ and $\Delta ACD$ and using the Herons formula \cite{kendig20002000} on each triangle. 

For example, consider $Area(\Delta \mathcal{Q}_i^{up} \mathcal{Q}_{i-1}^{up} L_i)$. Let $a = ||\mathcal{Q}_i^{up} \mathcal{Q}_{i-1}^{up}||_2$, $b = ||\mathcal{Q}_i^{up} L_i||_2$ and $c = ||\mathcal{Q}_{i-1}^{up} L_i||_2$. Additionally, we define $s = \frac{a+b+c}{2}$. Then, it is true that:
\[Area(\Delta \mathcal{Q}_i^{up} \mathcal{Q}_{i-1}^{up} L_i ) = \sqrt{s(s-a)(s-b)(s-c)}\]



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Obtaining fair classifier from the updated ROCs} \label{ssec:randomclass}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The algorithm described in the previous subsection returns the fair ROC curves according to $\varepsilon_1$\ourdef. As a final step, we 
need to find the transformed classifier.  We call it \texttt{ConstructClassifier}($FairROC_{up}$,$FairROC_{down}$
,$\roc^0$,$\roc^1$) which returns a probabilistic binary classifier representing $h = \mathcal{H}(s)$ such that it represents the FairROCs. We construct one using the procedure explained in Section 3.3.
%Any trade-off on the TPR ($G_s(.)$) of a scoring function $s$ can be achieved through randomization. Recall that every point \st{on the ROC-space}\sg{in $\mathcal{S}|_s$}\sg{notation update} is a representative of a binary classifier. In Chapter 3 of \cite{fairmlbook19}, the authors show that the ROC-space ($\mathcal{S}|_s$) below a given ROC-curve is convex.\sg{we already stated the chap 3 of fairml book...if we dint mention convexity...state there...we need not cite it here} Extending \st{this}\sg{the} simple idea of convex ROC-space we can \sg{something seems incomplete here}
%The points $(0,0)$ and $(1,1)$ on the ROC-space represent classifier which rejects everyone ($TPR = FPR = 0$), accepts everyone ($TPR = FPR = 1$) respectively. Consider a point $(p,p)$ which represents a classifier that accepts a random input feature with probability $p$. 
%\sg{here too, something incomplete}
%over trivial classifiers given by the points $(0,0)$ and $(1,1)$ on the ROC-space.

Now, we establish the optimality of our solution within specific assumptions.
% Now in the next section, we prove the optimality of our solution under some assumptions.
% additive approximation guarantees on our solution. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Theoretical Analysis}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
As described in Section ~\eqref{ssec:pla}, we work with PLA of the ROC curves $ROC_{H_s^a, G_s^a}$, $a\in \{0,1\}$. This causes a \emph{loss} in area under ROC. We denote this loss by $\mathcal{L}_{PLA}$ and is quantified as the difference in AUCs of $ROC_{H_s^a, G_s^a}$ and $ROC_{\widehat{H_s^a}, \widehat{G_s^a}}$.


In Section \ref{ssec:ouralgo}, transporting the ROC query points, $\mathcal{Q}^{up}$ introduces a decrease of the area under the ROC curve due to the transformation of scoring function $s$ to $h$. We denote this loss by $\mathcal{L}_{AUC}$. 
This loss can be quantified as the difference in AUCs of $ROC_{\widehat{H_s^a}, \widehat{G_s^a}}$ and $ROC_{H_h^a, G_h^a}$
The total loss in AUC, $\mathcal{L}$, induced by \ouralgo\ is given by:
$\mathcal{L} = \mathcal{L}_{PLA} + \mathcal{L}_{AUC}$

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{PLA Loss analysis} \label{ssec:lossplabound}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We start our analysis by making a few standard assumptions regarding the continuity and differentiability of the cumulative distributions on the family of scoring functions $\mathcal{S}$.
% We consider a weaker assumption than in \cite{vogel2021}
We adopt a less stringent assumption than that presented in \cite{vogel2021}, as we impose only an upper bound on the slopes. This contrasts with the approach in \cite{vogel2021}, which necessitates both an upper and lower bound on the slopes.
\begin{assumption} \label{assumption1}
% We assume that the slopes of $ROC_{up}$ and $ROC_{down}$ are upper bounded by $\beta$.

We assume that the rate of change (with respect to the thresholds $t$) of the $TPR$s and $FPR$s are upper bounded. I.e. we assume that $\exists~ u_T , u_F \in \mathbb{R}$ such that $\frac{d~TPR}{dt}\le u_T$ and $\frac{d~FPR}{dt}\le u_F$. 
% Let the family cdfs of a scoring function $s$ be given by $\mathcal{M} = \{ G_s^a, H_s^a : s \in \mathcal{S}, a\in \mathcal{A} \}$. All $M \in \mathcal{M}$ satisfy -- (a)$M$ is continuously differentiable, (b) The first derivative of $M$ are bounded as $b \leq \mid M'\mid \leq B$.
\end{assumption}


% $(1/2)*(1/k)*[l_T , u_T] * (1/k)*[l_F , u_F] * k $

\begin{theorem} \label{th:pla}
Let $ROC_{\widehat{H_s^a}, \widehat{G_s^a}}$ be the \emph{PLA} of $ROC_{H_s^a,G_s^a}$ over the query set of $k$ equidistant thresholds, $\mathcal{T} = \{ t_i \mid t_i = i/k \ \forall i \in [k] \}$. The corresponding $\mathcal{L}_{PLA}$ is bounded as:
%\begin{equation*}
    $\mathcal{L}_{PLA} \le\frac{1}{2} \frac{u_Tu_F}{k}$
%\end{equation*}
\end{theorem}
%%%% Requires the plot.
% The proof of the above theorem is given in the appendix.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{AUC loss analysis}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We start our analysis by making a few assumptions regarding the spacing of the ROC thresholds and the ROC curve.
\begin{assumption} \label{assumption2}
We have two assumptions:
\begin{compactitem}
    \item $\forall i\in \{1 ,2 , \hdots , k\}$, we assume that $FPR(\mathcal{Q}_{i-1}^{down}) \le FPR(\mathcal{Q}_i^{up}) \le FPR(\mathcal{Q}_{i+1}^{down})$.
    \item We assume that the $ROC_{up}$ can intersect any Norm boundary (i.e. $(\mathfrak{B}_i)_{i\in \{1 ,2 , \hdots , k\}}$) at most 2 times.
\end{compactitem}
\end{assumption}
% We point that if Assumption 4.2 doesn't hold, then the our algorithm is still functional in the sense that its output will still be \ourdef fair. However, the optimality of AUC is not assured. This is because Theorem 4.4 no longer holds.
We note that even if \textbf{Assumption 4.2} does not hold, \ouralgo remains operational and continues to produce outputs that are \ourdef fair. However, under these conditions, the optimality with respect to AUC is not guaranteed, as \textbf{Theorem 4.4} no longer applies.


\begin{theorem}
\label{thm:}If a given classifier $s$ is piece-wise linear and satisfies assumption 4.2, the ROCs returned by \ouralgo\ represent the classifier solving optimization problem~\ref{eq:fpp}.
\end{theorem}
% \begin{proof}
% We consider all possible situations when fairness conditions are violated and for each case, we need the transport to be one of the (i) CutShift, (ii) LeftShift and (iii) UpShift. The theorem follows from the optimality of these two shifts, which is proved using the geometry of the points. 
% \end{proof}

\subsection{Optimally fair points and Norm Boundary}
This section proves that all optimally fair points must lie on some Norm Boundary. We do this by establishing that the performance of any point in the Norm Set can be improved by appropriate transportation to a point on the Norm Boundary.

\begin{theorem}(Norm Boundary)
    If $(\widetilde{\mathcal{Q}}_i^{up})_{i\in \{1 ,2 , \hdots , k\}}$ is the set of optimal fair (points that maximize the AUC and also satisfy the $\varepsilon_1$ \ourdef) thresholds must necessarily be a subset of $(\mathfrak{B}_i)_{i\in \{1 ,2 , \hdots , k\}}$. 
\end{theorem}


\begin{theorem}(CutShift)
    If index $i$ is a Boundary cut point, then the CutShift operation must be performed. Of the 2 points ($p_{left}$ and $p_{right}$) returned by the Cutshift operation, the point that is closer to $\mathcal{Q}_{i}^{up}$ must be chosen i.e.$\mathcal{\widetilde{Q}}_i^{up} = argmin_{p \in \{ p_{left} , p_{right}\}} |FPR(\mathcal{Q}_i^{up}) - FPR(p)|$
\end{theorem}



\begin{theorem}(UpShift)
    If index $i$ is not a Boundary cut point and if $Area(\square \mathcal{Q}_{i+1}\mathcal{Q}_{i}\mathcal{Q}_{i-1}{L}_i \ge Area(\square \mathcal{Q}_{i+1}\mathcal{Q}_{i}\mathcal{Q}_{i-1}{U}_i$), then UpShift operation must be performed. The resulting point ($U_i$) is the new fair point $\mathcal{\widetilde{Q}}_i^{up}$. Otherwise, the LeftShift operation must be performed. The resulting point ($L_i$) is the new fair point $\mathcal{\widetilde{Q}}_i^{up}$.
\end{theorem}
% \subsection{Sample Complexity}
% % \sd{1)Write convergence analysis on $k$. 2) Show that \ouralgo\ runs in $O(k)$ complexity}
% If the \textbf{Assumption 4.2} holds true, then we have the following analysis:
% \begin{itemize}
%     \item All UpShift Operations will be constant time ($O(1)$).
%     \item All CutShift Operations will also be constant time ($O(1)$). This is because \textbf{Assumption 4.2} ensures that we do not have to run through the entire length of $ROC_{up}$ to find the intersection points i.e. $p_{left}$ and ${p_right}$.
% \end{itemize}
% Therefore, the running time of \ouralgo~ is $O(k)$.
% However, when no assumptions are made, then the CutShift operation is no longer $O(1)$. We may have to run through the entire length of $ROC_{up}$ to find the intersection points i.e. $p_{left}$ and $p_{right}$. This makes the CutShift operation $O(k)$. Therefore, the time complexity of \ouralgo~ is $O(k^2)$.
% \subsection{Equivalence to other ROC notions}
% \subsubsection{Equalized Odds}
% Equalized Odds is defined in \cite{padala21} and \cite{madras2018learning}, is the sum of the absolute differences of the $FNR$ and the $FPR$ of both the protected groups. Formally,
% \[EO \triangleq |FPR_0 - FPR_1| + |FNR_0 - FNR_1|\]
% However, this defintion is equivalent to \ourdef since $|FPR_0 - FPR_1| + |FNR_0 - FNR_1| = |FPR_0 - FPR_1| + |(1 - TPR_0) -(1 +  TPR_1)| =|FPR_0 - FPR_1| + |TPR_0 - TPR_1|$.
The proofs of all the above theorems are given in the appendix.
However, the following is brief sketch of the proof:\\
    % FROC\sg{don't use word FROC }:
    %\begin{enumerate}
\underline{Step 1:} We prove that all optimally fair points $(\widetilde{\mathcal{Q}}_{i}^{up})_{i \in \{ 1 ,2 , \hdots , k\}}$ must lie on the Norm Boundaries of the corresponding $\mathcal{Q}_i^{down}$. (i.e. $(\mathfrak{B}_i)_{i \in\{ 1 ,2 , \hdots , k\} }$)\\
\underline{Step 2:} We then prove that if $\mathfrak{B}_i \cap ROC_{up} \neq \phi $, then the CutShift transportation is the optimal transportation.\\
\underline{Step 3:} We then prove that if $\mathfrak{B}_i \cap ROC_{up} = \phi $, then, based on the Cover and aforementioned area condition, the UpShift or the LeftShift transportation is the optimal transportation.


In the next section, we experimentally analyze \ouralgo. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Empirical Analysis}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% First, we explain our experimental setup.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Experimental Setup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent \textbf{Datasets}: 
% We train  different classifiers on ADULT~\cite{misc_adult_2} and COMPAS~\cite{angwin2016machine} benchmark datasets. {The ADULT and COMPAS datasets are chosen because they are widely used in the fairness literature for testing fair algorithms.} In ADULT dataset, we use MALE and FEMALES as protected groups. In COMPAS, we consider BLACK and OTHERS as protected groups and generate ROCs. (Appendix E and F contain experiments on datasets like CelebA etc.)
We train different classifiers on the widely-used ADULT~\cite{misc_adult_2} and COMPAS~\cite{angwin2016machine} benchmark datasets, selecting MALE and FEMALE as protected groups in ADULT, and BLACK and OTHERS in COMPAS. ROCs are generated, with additional experiments on datasets like CelebA in Appendix E and F.

% \subsubsection{Classifiers}
\noindent \textbf{Classifiers}: We test \ouralgo\ on ROCs from the following classifiers:
\footnote{We choose these classifiers as per the availability of experiment hyper-parameters from other in-processing and post-processing benchmarks.}.
%We have used 2 classifiers (Weighted Ensemble and Random Forests) from the AutoGluon (\cite{mueller2020faster}) library. The other classifier is FNNC (Fair Neural Network Classifier) from \cite{padala21}.
%\begin{itemize}
    %\item[C1] \emph{FNNC} 
   % \item[C1]
    C1: \emph{FNNC}(~\cite{padala21}): This is a neural network-based classifier with a target parameter for fairness.
    %\item[C2] 
    C2: \emph{Logistic Regression} and 
    C3: \emph{Random Forest}
%\end{itemize}
We used the code from the author's GitHub for C1 and sklearn implementations for C2 and C3.

% and we  used AutoGluon (\cite{mueller2020faster}) library for 
% Weighted Ensemble L2 and Random Forest Gini. 
% (A few other classifiers,  such as SVMs, that we tried, as expected, were not at par with the above baselines.)

% \begin{figure*}[!t]
% \begin{minipage}{0.1\linewidth}
% \input{ICML24/froc}
% \end{minipage}
% \begin{minipage}{.3\linewidth}
%     \input{ICML24/fnnc_froc}
% \end{minipage}
% \begin{minipage}{.3\linewidth}
%     \input{ICML24/fnnc_froc}
% \end{minipage}
% \begin{minipage}{.3\linewidth}
%     \input{ICML24/fnnc_froc}
% \end{minipage}
% \end{figure*}

% \begin{figure*}[!t]
% \begin{minipage}{0.3\linewidth}
% \input{ICML24/froc}
% \end{minipage}
% \begin{minipage}{0.3\linewidth}
% \input{ICML24/froc_log}
% \end{minipage}
% \begin{minipage}{2\linewidth}
% \input{ICML24/ROC_ul}
% \end{minipage}
% \end{figure*}


% \begin{figure*}
%   \centering
%   \begin{subfigure}{0.3\textwidth}    \includegraphics[width=\linewidth]{ICML24/UL.png}
%     % \caption{Caption for Image 1}
%     \caption{}
%     \label{fig:image1}
%   \end{subfigure}
%   \hfill
%   \begin{subfigure}{0.3\textwidth}
%     \includegraphics[width=\linewidth]{ICML24/UC.png}
%     % \caption{Caption for Image 2}
%     \caption{}\label{fig:image2}
%   \end{subfigure}
%   \hfill
%   \begin{subfigure}{0.3\textwidth}
%     \includegraphics[width=\linewidth]{ICML24/UR.png}
%     % \caption{Caption for Image 3}
%     \caption{}\label{fig:image3}
%   \end{subfigure}
%   \caption{ROCs for C1-C3, before and after FROC}
%   \label{fig:overall}
% \end{figure*}




% \begin{figure}[!t]
% % \vspace{-1.5cm}
% % \begin{minipage}{.2\linewidth}
% %     \input{ICML24/froc}
% % \end{minipage}
% \begin{minipage}{\linewidth}
%     \input{AAAI25/fnnc_froc}
% \end{minipage}
% % \begin{minipage}{.3\linewidth}
% % \input{NeurIPS24/Mainpaper_Baseline_ROC}
% % \end{minipage}
% \begin{minipage}{\linewidth}
%     \input{AAAI25/RF_MEO}
% \end{minipage}
% \end{figure}


\begin{figure*}
    \centering
    \hspace{-2cm}
    \begin{minipage}{0.3\linewidth}
         \centering
         \includegraphics[scale = 0.3]{AAAI25/fnnc_compas_AAAI25.png}
         \caption{C1  vs. C1-\ouralgo}
        \label{fig:COMPAS_acc}
     \end{minipage}
    % \hspace{1.5cm}
     \begin{minipage}{0.3\linewidth}
         \centering
        \includegraphics[scale = 0.3]{AAAI25/rf_meo.png}
    \caption{C3-Fair Fair vs. C3-\ouralgo}
    \label{fig:FairProj} 
     \end{minipage}
     \begin{minipage}{0.3\linewidth}
         \centering
         \includegraphics[scale = 0.3]{AAAI25/roc_FROC.png}
   \caption{C2 Before and After \ouralgo}
    \label{fig:roc_froc}
     \end{minipage}
        \label{fig:three graphs}
        % \caption{(a) illustrates the abstraction of convex hull from ROCs. (b) uses the shaded area to illustrate the AUC loss due to Linear Interpolation. (c) introduces the notion of Norm Boundary.}
\end{figure*}


% \begin{figure}
%     \centering
%     \includegraphics[width=0.7\linewidth]{AAAI25/fnnc_compas_AAAI25.png}
    
%     \includegraphics[width=0.7\linewidth]{AAAI25/rf_meo.png}
%     \caption{C3-Fair Fair vs. C3-FROC}
%     \label{fig:FairProj}
%     \includegraphics[width=0.7\linewidth]{AAAI25/roc_FROC.png}
%    \caption{C2 Before and After \ouralgo}
%     \label{fig:roc_froc}
% \end{figure}
% \begin{figure}
%     \centering
%     \includegraphics[width=0.7\linewidth]{AAAI25/rf_meo.png}
%     \caption{C3-Fair Fair vs. C3-FROC}
%     \label{fig:FairProj}
% \end{figure}
% \begin{figure}
%     \centering
%     \includegraphics[width=0.7\linewidth]{AAAI25/roc_FROC.png}
%    \caption{C2 Before and After \ouralgo}
%     \label{fig:roc_froc}
% \end{figure}


% \begin{figure}[H]
%   \begin{center}
%     \begin{tikzpicture}
%       \begin{axis}[
%       legend style={at={(3,-1)},anchor=west},
%           width=0.7\linewidth, % Scale the plot to \linewidth
%           grid=major, 
%           grid style={dashed,gray!30},
%           xlabel=$FPR$, % Set the labels
%           ylabel = $TPR$,
%           % x unit=$V$, % Set the respective units
%           % y unit=$A$,
%           legend style={at={(0.9,0.4)}},
%           % legend style={anchor=northeast},
%           % x tick label style={rotate=90,anchor=east}
%                     x tick label style={/pgf/numberformat/.cd, precision=1,/tikz/.cd}
%         ]
%         \addplot+[color = red , mark = triangle]
%         % add a plot from table; you select the columns by using the actual name in
%         % the .csv file (on top)
%         table[x=FPR_Group1,y=TPR_Group1,col sep=comma] {Mainpaper_Baseline_ROC.csv}; 
%         \addlegendentry{Male ROC}
        
        
%         \addplot+[color = blue , mark =-]
%         % add a plot from table; you select the columns by using the actual name in
%         % the .csv file (on top)
%         table[x=FPR_Group0,y=TPR_Group0,col sep=comma] {Mainpaper_Baseline_ROC.csv}; 
%         \addlegendentry{Female ROC}
%         \addplot+[color = black , mark =.]
%         % add a plot from table; you select the columns by using the actual name in
%         % the .csv file (on top)
%         table[x=FPR_Group1,y=TPR_Group1,col sep=comma] {Mainpaper_Baseline_ROC.csv}; 
%         \addlegendentry{Fair Male ROC}
        
        
%         \addplot+[color = green , mark = x]
%         % add a plot from table; you select the columns by using the actual name in
%         % the .csv file (on top)
%         table[x=F_FPR_Group0,y=F_TPR_Group0,col sep=comma] {Mainpaper_Baseline_ROC.csv}; 
%         \addlegendentry{Fair Female ROC}
%       \end{axis}
%     \end{tikzpicture}
%     \caption{C2 Before and After \ouralgo}
%   \end{center}
% \end{figure}





% \begin{figure}
%     \centering
%     \includegraphics[scale = 0.5]{ICML24/UC.png}
%     \caption{C2 Before and After \ouralgo}
%     \label{fig:C2ROC}
% \end{figure}
% \subsubsection{Post-Processing methods}
\noindent \textbf{Post-Processing methods}: 
We compare \ouralgo\  against the following baselines: 
%\begin{itemize}
    %\item[B1] 
    % B1: \emph{DP Post Process}~\cite{xian23b}: ensures $\alpha-$DP fair predictions across sensitive attributes using Transport maps.
    %\item[B2] 
    B1: \emph{FairProjection-CE} and \emph{FairProjection-KL} ~\cite{alghamdi2022}: Transforms the score to achieve mean equalized odds fairness through information projection.
%\end{itemize}


\subsection{Experiments} 

%\subsubsection{In-Processing Methods}
We train C1 on both datasets, C2 and C3 on the Adult dataset, and generate their ROCs for all the protected groups. FNNC, we train by ignoring its fairness components in the loss function and then generate ROC. We then invoke \ouralgo\ for different $\varepsilon$ values and check the best possible threshold for accuracy. We refer to the new classifier as C1-C3-\ouralgo.%By claim/theorem~B.1, \ouralgo\ is satisfying $\varepsilon$-EO. We set the same $\varepsilon$ values in FNNC and test for its accuracy performance.

% \subsubsection{Baseline Post-Processing Methods}
\noindent \textbf{Baseline Post-Processing Method}: We evaluate \ouralgo, and the baselines B1 on ADULT dataset against the fairness metric \emph{mean equalized odds}(B2)~\cite{alghamdi2022}  in Figs.~\ref{fig:FairProj}.
For consistent comparison, we adopt the training parameters for base classifiers from ~\cite{alghamdi2022} and keep it identical across all experiments.


%%%%%%%%%%%%%%%%%%
\subsection{Results}
%%%%%%%%%%%%%%%%%%
We show the results on the COMPAS and Adult dataset (using FNNC and \ouralgo) here, along with a comparison with existing post-processing baselines.
The remaining experimental observations are detailed in the supplementary.
%
\textbf{Figure \ref{fig:roc_froc}} displays the ROC curves (Before and After \ouralgo) for both males and females, on the ADULT dataset for C2. The female ROC consistently occupies the higher position, indicating a positive bias for males. This establishes $ROC_{0}$ as our counterpart to $ROC_{down}$. Thus, we apply \ouralgo\ to the alternate curve, $ROC_{1}$, showcased in the figure. Before \ouralgo, the maximum difference between Male ROC and Female ROC is $0.08$. However, after post-processing with \ouralgo, the loss in accuracy is $<0.1\%$ for $\varepsilon=0.05$. 
In general, across all experiments (more experiments in Appendix), we observe a 7-8\% improvement in fairness, \ouralgo incurs at most a 2\% drop in accuracy. As seen in \textbf{Figure \ref{fig:COMPAS_acc}} and \textbf{Figure \ref{fig:FairProj}} for smaller values of $\varepsilon$, we also observe the performance may beat FNNC and the post-processing methods.
% \footnote{\av{We excluded FNNC's DI from the figure to prevent plot clutter, as DI was solely plotted for comparing FROC with other fairness metrics, making FNNC's DI irrelevant in this context.}}.
We assign it to the fact that FNNC (and the other methods) may overachieve the target fairness for smaller values of $\varepsilon$ (Evident from Table 2~\cite{padala21}). \ouralgo\ drops AUC minimally to achieve target fairness. 

% \begin{figure}[!h]
%   \begin{center}
%     \begin{tikzpicture}
%       \begin{axis}[
%           width=170, % Scale the plot to \linewidth
%           grid=major, 
%           grid style={dashed,gray!30},
%           xlabel=$FPR$, % Set the labels
%           ylabel = $TPR$,
%           % x unit=$V$, % Set the respective units
%           % y unit=$A$,
%           legend style={at={(1.1,0.4)}},
%           % legend style={anchor=northeast},
%           % x tick label style={rotate=90,anchor=east}
%                     % x tick label style={/pgf/numberformat/.cd, precision=1,/tikz/.cd}
%         ]
%         \addplot+[color = red , mark = triangle]
%         % add a plot from table; you select the columns by using the actual name in
%         % the .csv file (on top)
%         table[x=F_FPR_Group1,y=F_TPR_Group1,col sep=comma] {COMPAS_FNNC_Baseline_FROC_e01_ROC.csv}; 
%         \addlegendentry{Fair Others ROC}
        
        
%         \addplot+[color = blue , mark = square]
%         % add a plot from table; you select the columns by using the actual name in
%         % the .csv file (on top)
%         table[x=F_FPR_Group0,y=F_TPR_Group0,col sep=comma] {COMPAS_FNNC_Baseline_FROC_e01_ROC.csv}; 
%         \addlegendentry{Fair African ROC}


%         \addplot+[color = green, mark = x]
%         % add a plot from table; you select the columns by using the actual name in
%         % the .csv file (on top)
%         table[x=FPR_Group1,y=TPR_Group1,col sep=comma] {COMPAS_FNNC_Baseline_ROC.csv}; 
%         \addlegendentry{Others ROC}
%       \end{axis}
%     \end{tikzpicture}
%     \caption{Fair (FROC $\varepsilon = 0.01$) FNNC Baseline ROCs for COMPAS Dataset}
%     \label{fig:ROC_COMPAS}
%   \end{center}
% \end{figure}







% \begin{figure}[!h]
%     \centering
%     \begin{tikzpicture}
%     \pgfplotsset{
%     % width=\linewidth,
%     width=120,
%     scale only axis,
%     scaled x ticks=base 10:1,
%     xmin=0, xmax=0.1,
%     y axis style/.style={
%     yticklabel style=#1,
%     ylabel style=#1,
%     y axis line style=#1,
%     ytick style=#1
%        }
%     }
    
%     \begin{axis}[
%       axis y line*=left,
%       y axis style=blue!75!black,
%       % ymin=0, ymax=80,
%       xlabel=$\varepsilon_{FROC} ~and~\varepsilon_{FNNC}$,
%       ylabel= Accuracy,
%           grid=major, 
%           grid style={dashed,gray!30},
%           legend style={at={(0.9,0.7)} , anchor= northeast}
%     ]
%     %%% FNNC accuracy
%     \addplot[smooth,mark= x,blue] 
%       coordinates{
%         (0.001,0.8243)
%         (0.02, 0.8252) 
%         (0.04, 0.8217)
%         (0.06, 0.8304)
%         (0.08, 0.8273)
%         (0.1 , 0.845)
%     };
%     \addlegendentry{FNNC Acc.}

%     %%% FROC accuracy
%     \addplot[smooth,mark=square, blue] 
%         table[x=Epsilon,y=Accuracy,col sep=comma] {Adult_FNNC_Results.csv}; 
%     %     coordinates{
%     %     (0.001,0.824)
%     %     (0.02, 0.825) 
%     %     (0.04, 0.821)
%     %     (0.06, 0.830)
%     %     (0.08, 0.827)
%     %     (0.1 , 0.83)
%     % };
%     \addlegendentry{FROC Acc.}



    
%     \end{axis}
    
%     \begin{axis}[
%       axis y line*=right,
%       axis x line=none,
%       % ymin=0, ymax=100,
%       ylabel=Disparate Impact (DI),
%       % y axis style=red!75!black
%       legend style={at={(1.7,0)}}
%     ]
%     \addplot[smooth,mark=*,red] 
%     %   coordinates{
%     %     (0,0)
%     %     (0.0148,48) 
%     %     (0.0295,66)
%     %     (0.0441,66)
%     %     (0.059,45.0) 
%     % };
%     table[x=Epsilon,y=Disp,col sep=comma] {Adult_FNNC_Results.csv};
%     \addlegendentry{FROC DI.}
%     \end{axis}
    
%     \end{tikzpicture}
%     \caption{Comparison of FNNC and FNNC-FROC for Adult Dataset}
%     \label{fig:pgfplot-two}
% \end{figure}






% \begin{figure}[!h]
%     \centering
%     \includegraphics[scale = 0.2]{AAAI24/Adult/FNNC_ROCs.png}
%     \caption{Adult Dataset: Male and Female ROCs}
%     \label{fig:enter-label}
% \end{figure}

\section{Conclusion}
In this work, we addressed the problem of practitioners aiming to achieve fair classification without retraining MLMs. Specifically, we provide a post-processing framework that takes a potentially unfair classification score function and returns a probabilistic fair classifier. 
The practitioner need not worry about fairness across different thresholds, so we proposed a new notion $\varepsilon_1$\ourdef\ (Definition \ref{def:eroc}), which ensures fairness for all thresholds. To achieve $\varepsilon_1$\ourdef, we proposed \ouralgo\ (Algorithm \ref{alg:fairroc}), which transports the ROC for each sensitive group within $\epsilon$ distance while minimizing the loss in AUC of the resultant ROC. 
We geometrically proved its optimality conditions (Theorem 4.2) and bounds under certain technical assumptions. We observed empirically that its performance might differ at most by 2\% compared to an in-processing technique while ensuring stronger fairness and avoiding retraining. We leave it for future work to explore the possibility of different distance metrics for fairness and optimizing for different performance measures. 


\section*{Note}
The official code for this paper can be found in this \href{https://github.com/Avyukta-Manjunatha-Vummintala/FROC_code/tree/main}{link}.
\newpage
\section*{Appendix}

% \noindent \textbf{Note}: The Reproducibility Checklist is available in the Technical Appendix.

% \section{Reproducibility Checklist}
% \begin{itemize}
% \item  This paper:
% \begin{itemize}
%     \item Includes a conceptual outline and/or pseudocode description of AI methods introduced (\textbf{yes})
%     \item Clearly delineates statements that are opinions, hypothesis, and speculation from objective facts and results (\textbf{yes})
%     \item  Provides well marked pedagogical references for less-familiar readers to gain background necessary to replicate the paper (\textbf{yes})
% \end{itemize}

% \item  Does this paper make theoretical contributions? (\textbf{yes})\\
% \item  If yes, please complete the list below.

% \begin{itemize}
%     \item All assumptions and restrictions are stated clearly and formally. (\textbf{yes})
%     \item All novel claims are stated formally (e.g., in theorem statements). (\textbf{yes})
%     \item Proofs of all novel claims are included. (\textbf{yes})
%     \item Proof sketches or intuitions are given for complex and/or novel results. (\textbf{yes})
%     \item Appropriate citations to theoretical tools used are given. (\textbf{yes})
%     \item All theoretical claims are demonstrated empirically to hold. (\textbf{yes})
%     \item All experimental code used to eliminate or disprove claims is included. (\textbf{NA})

% \end{itemize}

% \item Does this paper rely on one or more datasets? (\textbf{yes})

% \item If yes, please complete the list below.
% \begin{itemize}
%     \item A motivation is given for why the experiments are conducted on the selected datasets (\textbf{yes})
%     \item All novel datasets introduced in this paper are included in a data appendix. (\textbf{NA})
%     \item All novel datasets introduced in this paper will be made publicly available upon publication of the paper with a license that allows free usage for research purposes. (\textbf{NA})
%     \item All datasets drawn from the existing literature (potentially including authorsâ€™ own previously published work) are accompanied by appropriate citations. (\textbf{yes})
%     \item All datasets drawn from the existing literature (potentially including authorsâ€™ own previously published work) are publicly available. (\textbf{yes})
%     \item All datasets that are not publicly available are described in detail, with explanation why publicly available alternatives are not scientifically satisficing. (\textbf{NA})
% \end{itemize}

% \item Does this paper include computational experiments? (\textbf{yes})

% \item If yes, please complete the list below.

% \begin{itemize}
%     \item Any code required for pre-processing data is included in the appendix. (\textbf{yes}).
%     \item All source code required for conducting and analyzing the experiments is included in a code appendix. (\textbf{partial})
%     \item All source code required for conducting and analyzing the experiments will be made publicly available upon publication of the paper with a license that allows free usage for research purposes. (\textbf{yes})
%     \item All source code implementing new methods have comments detailing the implementation, with references to the paper where each step comes from (\textbf{yes})
%     \item If an algorithm depends on randomness, then the method used for setting seeds is described in a way sufficient to allow replication of results. (\textbf{yes})
%     \item This paper specifies the computing infrastructure used for running experiments (hardware and software), including GPU/CPU models; amount of memory; operating system; names and versions of relevant software libraries and frameworks. (\textbf{NA} (The experiments were run on a desktop computer. Extensive compute was not required.))
%     \item This paper formally describes evaluation metrics used and explains the motivation for choosing these metrics. (\textbf{yes})
%     \item This paper states the number of algorithm runs used to compute each reported result. (\textbf{yes})
%     \item Analysis of experiments goes beyond single-dimensional summaries of performance (e.g., average; median) to include measures of variation, confidence, or other distributional information. (\textbf{yes})
%     \item The significance of any improvement or decrease in performance is judged using appropriate statistical tests (e.g., Wilcoxon signed-rank). (partial) (we have provided measure of variance like Coefficient of Variance etc.)
%     \item This paper lists all final (hyper-)parameters used for each model/algorithm in the paperâ€™s experiments. (\textbf{NA} (no hyper-parameters are required in our algorithm))
%     \item This paper states the number and range of values tried per (hyper-) parameter during development of the paper, along with the criterion used for selecting the final parameter setting. (\textbf{NA}(no hyper-parameters are required in our algorithm))
% \end{itemize}
% \end{itemize}



% % \noindent\textbf{Impact Statement}
% % We focused on binary classification with two protected groups. One can easily extend $\varepsilon_1$\ourdef\ to multi-class, multi-protected groups (e.g., EO is extended as MEO~\cite{}). We leave it for the extended version for detailed technicalities. The post-processing techniques will have a deeper impact on MLMs, especially after the success of Generative AI and LLMs; many researchers and practitioners are leveraging pre-trained models to adapt to their needs. However, these models need a serious fairness analysis~\cite{li2023survey, chang2023survey,ramesh2023fairness,kasneci2023chatgpt}. \ouralgo\ can be further delved into to build fair MLMs from pre-trained models in such settings. \if 0
% % \begin{itemize}
% %     \item \cite{li2023survey} , \cite{chang2023survey}: General Survey papers on fairness in LLMs.
% %     \item \cite{ramesh2023fairness}: this paper talks about fairness in language models beyond English.
% %     \item \cite{kasneci2023chatgpt}: Talks about ChatGPT and its impact on educations and the need for it to be fair.
 
% % \end{itemize}

% % \fi
% % \bibliography{icml24.bib}
% \bibliography{AAAI25/aaai25}


\appendix
\section{Notation Table}
\begin{table}[H]
  \centering
  \begin{tabular}{c p{0.35\textwidth}}
    \toprule
    \textbf{Notation} & \textbf{Description} \\
    \midrule
    $\varepsilon$ & Fairness measure of $\varepsilon_1$\ourdef\ and \ouralgo \\
    $ROC$ & Receiver Operator Characteristic (plot of FPR vs. TPR) \\
    $AUC$ & Area under ROC curve \\
    $s$ & Scoring function \\
    $k$ & Number of queries submitted to the scoring function \\
    $D$ & Dataset \\
    $x_i$ & Feature vector \\
    $\mathcal{X}$ & Sample space of feature vectors  \\
    $y_i$ & Binary label \\
    $a_i$ & Binary protected attribute \\
    $X$ & Random vector modeling feature vectors \\
    $Y$ & Random variable modeling labels \\
    $A$ & Random variable modeling protected attributes \\
    $\mathcal{S}$ & Space of scoring functions \\
    $\mathcal{S}|_s$ & Space of feasible scoring functions\\
    $G_s(t)$ & $\mathbb{P}(s(X) \ge t | Y = 1)$ \\
    $H_s(t)$ & $\mathbb{P}(s(X) \ge t | Y = 0)$ \\
    $G^a_s(t)$ & $\mathbb{P}(s(X) \ge t | Y = 1, A=a)$ \\
    $H^a_s(t)$ & $\mathbb{P}(s(X) \ge t | Y = 0, A=a)$ \\
    $\roc$& $\mathrm{ROC}_{H_s , G_s}$\\
    $\texttt{AUC}_s$ & AUC of $\roc$\\
    $\mathcal{Q}^a$ & Sequence of query point from Group $a$\\
    $\mathcal{Q}^a_i$ & Query point of Group $a$ at threshold $t_i$\\
    $\mathcal{L}_{LPA}$ & Loss due to Linear Piecewise Approximation\\
    $\mathfrak{C}_i$ & Norm Set of $i^{th}$ threshold\\
    $\mathfrak{B}_i$ & Norm Boundary of $i^{th}$ threshold\\
    \bottomrule
  \end{tabular}
  \caption{Mathematical Notations}
  \label{tab:notations}
\end{table}



\section{Relation to Equalized Odds}
Equalized Odds is defined in \cite{padala21} and \cite{madras2018learning}, is the sum of the absolute differences of the $FNR$ and the $FPR$ of both the protected groups. Formally,
\[EO \triangleq |FPR_0 - FPR_1| + |FNR_0 - FNR_1|\]
However, this defintion is equivalent to $\varepsilon_1$\ourdef since $|FPR_0 - FPR_1| + |FNR_0 - FNR_1| = |FPR_0 - FPR_1| + |(1 - TPR_0) -(1 +  TPR_1)| =|FPR_0 - FPR_1| + |TPR_0 - TPR_1|$.

\section{Algorithm Description}

\subsection{\ouralgo}
\begin{definition}[Norm Boundary] \label{def:eNBoundary}
The set of all points within $\varepsilon$ distance ($\ell_1$ norm) from $\mathcal{Q}_i^{down}$ is known as the \emph{norm set} $\mathfrak{C}_i$. Formally, we have:
\[\mathfrak{C}_i \triangleq \{ x: x\in [0,1]^2 , ||x - \mathcal{Q}_i^{down}||_1 \le \varepsilon\}\]
The set of all points exactly $\varepsilon$ distance from $\mathcal{Q}_i^a$ is known as \emph{Norm Boundary} $\mathfrak{B}_i$. Formally, 
\[\mathfrak{B}_i \triangleq \{ x: x\in [0,1]^2 , ||x - \mathcal{Q}_i^{down}||_1 = \varepsilon\}\]
Additionally, we denote the vertices of the Norm Boundary Rhombus (starting from the top most point and moving clockwise) as $U_i$, $R_i$, $D_i$, and $L_i$.
\end{definition}

\begin{figure}[!h]
    \centering
    \includegraphics[scale = 0.2]{diagrams/NormBoundary_modified.jpg}
    \caption{The area inside the rhombus is the Norm set $\mathfrak{C}_i$. The boundary (denoted by the thick bold border) is $\mathfrak{B}_i$. The topmost point of $\mathfrak{B}_i$ is denoted by $U_i$}
    \label{fig:enter-label}
\end{figure}


% \sg{i guess it is hypograph...so no need for separate defintion}
% \begin{definition}[Cover] \label{def:Cover}
%     The set of all points that are \emph{beneath} the $ROC$ curve is represented by $Cover(ROC)$. Formally,
%     \[Cover(ROC) \triangleq \{ (x,y): (x,y) \in [0,1]^2~,~y \le ROC(x)\}\]
% \end{definition}

% \begin{definition}[Boundary Cut] \label{def:BCut}
% We say that a \sg{what is i here?do we mean $Q_{i}^{up}$?or $B_i$?}$i \in [1 ,2 , \hdots, k]$ is a Boundary Cut point when $ROC_{up}$ intersects the Norm Boundary $\mathfrak{B}_i$. Formally, $i \in [1 ,2 , \hdots, k]$ is a \emph{Boundary Cut point} when $\mathfrak{B}_i \cap ROC_{up} \neq \phi$.
% \end{definition}
% \sg{it is better to define it as follows}


We say that a $i \in [1 ,2 , \hdots, k]$ is a Boundary Cut point when $ROC_{up}$ intersects the Norm Boundary $\mathfrak{B}_i$. Formally,
\begin{definition}[Boundary Cut] \label{def:BCut}
 $i \in [1 ,2 , \hdots, k]$ is a \emph{Boundary Cut point} when $\mathfrak{B}_i \cap ROC_{up} \neq \phi$.
\end{definition}
This is illustrated in the \textbf{Figure 8}.
\begin{figure}[!h]
    \centering
    \includegraphics[scale = 0.1]{diagrams/Boundary_Cut.jpg}
    \caption{We have two points - $\mathcal{Q}_a^{down}$ and $\mathcal{Q}_b^{down}$ (in increasing order of FPR). We find that $\mathcal{Q}_a^{down}$ is a Boundary Cut point, whereas $\mathcal{Q}_b^{down}$ is not.}
    \label{fig:enter-label}
\end{figure}

We now define the three kinds of shifts that will be used in our Algorithm:
For a given $ i \in [1 , 2, \hdots ,k]$, Upshift is the transportation of $\mathcal{Q}_i^{up}$ to the point $U_i$. 
\begin{definition}[UpShift] \label{def:csh}
    For a given $ i \in [1 , 2, \hdots ,k]$, Upshift is the transportation of $\mathcal{Q}_i^{up}$ to the point $U_i$. Formally, \emph{UpShift} can be defined as the function that returns a fair threshold $\widetilde{\mathcal{Q}}_i^{up}$ (i.e. $U_i$) by taking the $\mathcal{Q}_i^{down}$ and $\varepsilon$ as the arguments.
\end{definition}
This is illustrated in the following \textbf{Figure \ref{fig:US}}.
\begin{figure}[!h]
    \centering
    \includegraphics[scale = 0.35]{diagrams/UpShift_updated.png}
    \caption{UpShift}
    \label{fig:US}
\end{figure}

For a given $ i \in [1 , 2, \hdots ,k]$, Leftshift is the transportation of $\mathcal{Q}_i^{up}$ to the point $L_i$. Formally,
\begin{definition}[LeftShift] \label{def:csh}
LeftShift is a function that returns a fair threshold $\widetilde{\mathcal{Q}}_i^{up}$ (i.e. $L_i$) by taking the $\mathcal{Q}_i^{down}$ and $\varepsilon$ as the arguments.
\end{definition}
This is illustrated in the following \textbf{Figure \ref{fig:LS}}.
\begin{figure}[!h]
    \centering
    \includegraphics[scale = 0.35]{diagrams/LeftShift_updated.png}
    \caption{LeftShift}
    \label{fig:LS}
\end{figure}
\begin{definition}[CutShift] \label{def:ush}
    For a given $i \in [1 , 2, \hdots ,k]$ (representing the index of the $ROC_{down}$), we run through all the points of the $ROC_{up}$ and return the set of all points that intersect the Norm Boundary $\mathfrak{B_i}$. Formally, we define \emph{Cutshift} as a function that takes $\mathcal{Q}_i^{down}$ and $\varepsilon$ as the arguments and returns $ROC_{up} \cap \mathfrak{B}_i$. The set $ROC_{up} \cap \mathfrak{B}_i$ can be represented as $\{p_{left} , p_{right}\}$ denoting the points at the intersection of $ROC_{up}$ at the \textbf{left-side} of the Norm Boundary and the \textbf{right-side} of the Norm Boundary respectively.
\end{definition}

This is illustrated in the following \textbf{Figure \ref{fig:CS}}.
\begin{figure}[!h]
    \centering
    \includegraphics[scale = 0.35]{diagrams/CutShift_updated.png}
    \caption{CutShift}
    \label{fig:CS}
\end{figure}

Note that the two intersection points - $p_{left}$ and $p_{right}$ will be to the right of $\mathcal{Q}_{i}^{up}$ when $FPR(\mathcal{Q}_{i}^{up}) \le FPR(\mathcal{Q}_{i}^{down})$. Note that it is also possible for $p_{left}$ to lie on the line segment $\overline{L_i D_i}$ instead of line segment $\overline{U_i L_i}$ when $\mathcal{Q}_i^{up}$ has sufficiently low TPR.

We elaborate on the above procedure to transport points from $ROC_{up}$ towards $ROC_{down}$ in the following subsection.

\subsection{Randomization to obtain new classifiers}
\begin{theorem}
    If $\mathcal{Q}_a,\mathcal{Q}_b,\mathcal{Q}_c$ are points in $\mathcal{S}|_s$ forming a convex hull $\Delta$ and $\mathcal{Q} \in \Delta$, then the classifier equivalent to $\mathcal{Q}$ can be obtained by following the below procedure. For each test data point $x$, use the following randomization scheme:
\begin{equation}
    Classifier_{\mathcal{Q}}(x)=
    \begin{cases}
        Classifier_{\mathcal{Q}_a}(x) & \text{w.p. } p_a \\
        Classifier_{\mathcal{Q}_b}(x) & \text{w.p. } p_b \\
        Classifier_{\mathcal{Q}_c}(x) & \text{w.p. } 1-p_a-p_b \\
        % 0 & \text{if } x \in \mathbb{R}\setminus\mathbb{Q}
    \end{cases}
\end{equation}
\end{theorem}
Here, we have, 
$p_a = \frac{c_1b_2 - c_2b_1}{a_1b_2 - a_2b_1}$, $p_b = \frac{c_1a_2 - c_2a_1}{a_1b_2 - a_2b_1}$ and 
\[a_1 = TPR(\mathcal{Q}_a) - TPR(\mathcal{Q}_c)~and~a_2 = FPR(\mathcal{Q}_a) - FPR(\mathcal{Q}_c)\] 
\[b_1 = TPR(\mathcal{Q}_b) - TPR(\mathcal{Q}_c)~and~b_2 = FPR(\mathcal{Q}_b) - FPR(\mathcal{Q}_c)\] 
\[c_1 = TPR(\mathcal{Q}) - TPR(\mathcal{Q}_c)~and~c_2 = FPR(\mathcal{Q}) - FPR(\mathcal{Q}_c)\]



\section{Theoretical Results}
\subsection{Piecewise Linear Approximation}

\begin{theorem} \label{th:pla}
Let $ROC_{\widehat{H_s^a}, \widehat{G_s^a}}$ be the \emph{PLA} of $ROC_{H_s^a,G_s^a}$ over the query set of $k$ equidistant thresholds, $\mathcal{T} = \{ t_i \mid t_i = i/k \ \forall i \in [k] \}$ then the corresponding $\mathcal{L}_{PLA}$ is bounded as:
\begin{equation*}
    \mathcal{L}_{PLA} \le \frac{1}{2} \frac{u_T u_F}{k^2} \times k  = \frac{1}{2} \frac{u_T u_F}{k}
\end{equation*}
\end{theorem}

\begin{proof}

In \textbf{Figure \ref{fig:PLA_p1}}, shaded area is the approximation loss $\mathcal{L}_{PLA}$. 
\begin{figure}[!h]
    \centering
    \includegraphics[scale = 0.06]{diagrams/PLA_modified_2.jpg}
    \caption{$\mathcal{L}_{PLA}$}
    \label{fig:PLA_p1}
\end{figure}
Let us consider the situation where $ROC_{H_s^a, G_s^a}$ \emph{maximally} deviates from its PLA $ROC_{\widehat{H_s^a}, \widehat{G_s^a}}$. To find an upper bound to this area, we must stretch it till the dotted line 
% \sd{This is confusing, explain which dotted line you are referring to}. 



\begin{figure}[!h]
    \centering
    \includegraphics[scale = 0.3]{diagrams/PLA_proof3.jpg}
    \caption{Maximally streching the ROC (Dotted line)}
    \label{fig:enter-label}
\end{figure}

The area cannot go beyond the dotted line (\textbf{Figure \ref{fig:PLA2_p2}})because ROCs are one-to-one and monotonically increasing functions.
\begin{figure}[!h]
    \centering
    % \vspace{-2cm}
    \includegraphics[scale = 0.25]{diagrams/PLA_proof2.jpg}
    \caption{The area shaded by the darker shade of blue is the maximum possible loss of AUC due to Linear Interpolation.}
    \label{fig:PLA2_p2}
\end{figure}
So, our goal now, is to bound the sum of areas of the blue shaded triangles. We have the base of each triangle to be $\frac{1}{k}\times u_F$ (since $k$ thresholds and maximum slope of $FPR$ with respect to the thresholds is $u_F$). We have the maximum possible height of each triangle $\frac{1}{k}\times u_T$ (since $k$ thresholds and maximum slope of $TPR$ with respect to the thresholds is $u_T$). This makes the maximum possible area of each triangle $\frac{u_T u_F}{2k^2}$.
So, for an interval between thresholds $t_i, t_{i+1}$, the loss incurred is $\leq \frac{1}{2} \frac{u_T u_F}{k^2}$. To extend this for the entire ROC over $k$ intervals, we have:
\begin{equation*}
    \mathcal{L}_{PLA} \leq \frac{1}{2} \frac{u_Tu_F}{k^2} \times k  = \frac{1}{2} \frac{u_Tu_F}{k}    
\end{equation*}
\end{proof}

Therefore, we can infer:
\[ \lim_{k\to\infty} \mathcal{L}_{LPA} =\lim_{k\to\infty} \frac{1}{2} \frac{u_Tu_F}{k} =0 \]
\subsection{Boundary Optimality}
\subsubsection{All optimal points lie on the Norm boundary}

% \begin{figure}[!h]
%     \centering
%     \includegraphics[scale = 0.1]{diagrams/Boundary_Cut.jpg}
%     \caption{$\mathcal{Q}_a$,$\mathcal{Q}_b$ represent 2 points in $ROC_{down}$. Here, as illustrated by the figure, $a$ is a Boundary Cut point and $b$ is not a Boundary Cut point.}
%     \label{fig:enter-label}
% \end{figure}

\begin{theorem}(Norm Boundary)
    If $(\widetilde{\mathcal{Q}}_i^{up})_{i\in \{1 ,2 , \hdots , k\}}$ is the set of optimal fair (points that maximize the AUC and also satisfy the $\varepsilon$ fairness criteria) thresholds must necessarily be a subset of $(\mathfrak{B}_i)_{i\in \{1 ,2 , \hdots , k\}}$. 
\end{theorem}

\begin{proof}
(Proof by Contradiction) 
Let us assume that some point $C$ in the interior of the Norm Set is the optimal fair (point that leads to ROC with maximum possible AUC while satisfying $\varepsilon_1$\ourdef) point. 
As we can see in \textbf{Figure \ref{fig:n1}}, we have transported $\mathcal{Q}_{i}^{up}$ to $C$ in the interior of the Norm set. The shaded area denotes the AUC loss due to this transformation. However, as seen in the next figure \textbf{Figure \ref{fig:n2}}, the AUC loss can be decreased by choosing a point (we choose the CutShift point) on the Norm boundary. Thus, we can always decrease AUC loss by choosing a point on the Norm Boundary. Formally, if point $C$ was the optimal fair point, then the AUC loss with respect to that point is $Area(\square \mathcal{Q}^{up}_{i-1} C \mathcal{Q}^{up}_{i+1} \mathcal{Q}^{up}_{i})$. 
\\
However, if $A$ is the optimal fair point (Fig \ref{fig:n2}), then the AUC loss with respect to that point is $Area(\square \mathcal{Q}^{up}_{i} A \mathcal{Q}^{up}_{i+1})$. However, we notice that:$Area(\square \mathcal{Q}^{up}_{i-1} C \mathcal{Q}^{up}_{i+1} \mathcal{Q}^{up}_{i}) = Area(\square \mathcal{Q}^{up}_{i} A \mathcal{Q}^{up}_{i+1}) + Area(\square \mathcal{Q}^{up}_{i-1} C \mathcal{Q}^{up}_{i+1} A)$. Since $Area(\square \mathcal{Q}^{up}_{i-1} C \mathcal{Q}^{up}_{i+1} A) \ge 0$, we have:
    \[Area(\square \mathcal{Q}^{up}_{i-1} C \mathcal{Q}^{up}_{i+1} \mathcal{Q}^{up}_{i}) \ge Area(\square \mathcal{Q}^{up}_{i} A \mathcal{Q}^{up}_{i+1}) \]
This is a contradiction to the assumption that $C$ is the optimal fair point. Therefore, $C$ is not an optimal fair point.
\end{proof}

\begin{figure}[!h]
    \centering
    \includegraphics[scale = 0.23]{diagrams/nNorm_proof1.jpg}
    \caption{The blue colored region indicates the AUC loss.}
    \label{fig:n1}
\end{figure}
\begin{figure}[!h]
    \centering
    \includegraphics[scale = 0.23]{diagrams/nNorm_proof2.jpg}
    \caption{The dark blue colored region indicates the new AUC loss. The light blue region indicates the previous AUC loss.}
    \label{fig:n2}
\end{figure}

\subsection{CutShift Optimality}
% \begin{figure}[!h]
%     \centering
%     \includegraphics[scale = 0.35]{diagrams/Cutshift_updated.png}
%     \caption{The points $p_{left}$ and $p_{right}$ (in green color) are the set of points returned by the CutShift operation.}
%     \label{fig:CutShift}
% \end{figure}

% Note that the two intersection points - $p_{left}$ and $p_{right}$ will be to the right of $\mathcal{Q}_{i}^{up}$ when $FPR(\mathcal{Q}_{i}^{up}) \le FPR(\mathcal{Q}_{i}^{down})$. Note that it is also possible for $p_{left}$ to lie on the line segment $\overline{L_i D_i}$ instead of line segment $\overline{U_i L_i}$ when $\mathcal{Q}_i^{up}$ has sufficiently low TPR.




% \begin{figure}[!h]
%     \centering
%     \includegraphics[scale = 0.4]{diagrams/Leftshift_updated.png}
%     \caption{The dotted arrow represents the UpShift transportation of the point from $\mathcal{Q}_i^{up}$ to $U_i$}
%     \label{fig:lesh}
% \end{figure}
\begin{theorem}
% \sd{Theorem statement is using phrases like "must be". The objective and conditions in the theorem are unclear. Same issue with the next theorem statement.}
If $i$ is a Boundary cut point, then the CutShift operation must be performed. Of the 2 points ($p_{left}$ and $p_{right}$) returned by the Cutshift operation, the point that is closer to $\mathcal{Q}_{i}^{up}$ must be chosen i.e.
    
    $\mathcal{\widetilde{Q}}_i^{up} = argmin_{p \in \{ p_{left} , p_{right}\}} |FPR(\mathcal{Q}_i^{up}) - FPR(p)|$
\end{theorem}

\begin{proof}
(Proof by Contradiction) 
Let us assume that some point $C$ on the Norm Boundary is the optimal fair (point that leads to ROC with maximum possible AUC while satisfying $\varepsilon_1$\ourdef) point. 
As we can see in \textbf{Figure \ref{fig:c1}}, we have transported $\mathcal{Q}_{i}^{up}$ to $C$ in the interior of the Norm set. The shaded area denotes the AUC loss due to this transformation. However, as seen in the next figure Fig \ref{fig:n2}, the AUC loss can be decreased by choosing a point (we choose the CutShift point) on the Norm boundary. Thus, we can always decrease AUC loss by choosing a point on the Norm Boundary. Formally, if point $C$ was the optimal fair point, then the AUC loss with respect to that point is $Area(\square \mathcal{Q}^{up}_{i-1} C \mathcal{Q}^{up}_{i+1} \mathcal{Q}^{up}_{i})$. 
\\
However, if $A$ is the optimal fair point (Fig \ref{fig:c2}), then the AUC loss with respect to that point is $Area(\square \mathcal{Q}^{up}_{i} A \mathcal{Q}^{up}_{i+1})$. However, we notice that:$Area(\square \mathcal{Q}^{up}_{i-1} C \mathcal{Q}^{up}_{i+1} \mathcal{Q}^{up}_{i}) = Area(\square \mathcal{Q}^{up}_{i} A \mathcal{Q}^{up}_{i+1}) + Area(\square \mathcal{Q}^{up}_{i-1} C \mathcal{Q}^{up}_{i+1} A)$. Since $Area(\square \mathcal{Q}^{up}_{i-1} C \mathcal{Q}^{up}_{i+1} A) \ge 0$, we have:
    \[Area(\square \mathcal{Q}^{up}_{i-1} C \mathcal{Q}^{up}_{i+1} \mathcal{Q}^{up}_{i}) \ge Area(\square \mathcal{Q}^{up}_{i} A \mathcal{Q}^{up}_{i+1}) \]
This is a contradiction to the assumption that $C$ is the optimal fair point. Therefore, $C$ is not an optimal fair point.


    % As seen in the figure, transporting $\mathcal{Q}_i^{up}$ to any point in the Norm Boundary will result in a certain AUC loss. However, as seen in the next diagram, this AUC loss can \textbf{always} be reduced by tranporting to the point recommended by the CutShift Operation. 
\end{proof}

\begin{figure}[!h]
    \centering
    \includegraphics[scale =0.2]{diagrams/cut_proof0.jpg}
    \caption{CutShift Operation is not followed. The light blue area indicates the AUC loss due to this operation.}
    \label{fig:c1}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[scale =0.2]{diagrams/cut_proof1.jpg}
    \caption{CutShift Operation is followed. The dark blue area indicates the AUC loss due to this operation. It is lesser than the previous AUC loss as seen in Figure 9.}
    \label{fig:c2}
\end{figure}


\subsection{Upshift and Left Shift}
\begin{theorem}[UpShift]
    If $i$ is not a Boundary cut point and if $Area(\square \mathcal{Q}_{i+1}\mathcal{Q}_{i}\mathcal{Q}_{i-1}{L}_i \ge Area(\square \mathcal{Q}_{i+1}\mathcal{Q}_{i}\mathcal{Q}_{i-1}{U}_i$), then UpShift operation must be performed. The resulting point ($U_i$) is the new fair point $\mathcal{\widetilde{Q}}_i^{up}$. Else, LeftShift operation must be performed. The resulting point ($L_i$) is the new fair point $\mathcal{\widetilde{Q}}_i^{up}$.
\end{theorem}


\begin{proof}
    By a similar argument, as the previous proofs, we argue (through \textbf{Figure \ref{fig:Upsh}}, \textbf{Figure \ref{fig:nUpSh}} and \textbf{Figure \ref{fig:UpShp}}), we can prove that either the point recommended by UpShift ($U_i$) or LeftShift ($L_i$) is the optimal point. So, to decide between them, we use Heron's formula to find the area of both quadrilaterals and then compare their areas to find the least AUC loss. We can use Heron's formula to find the area of a quadrilateral in the following way: If $\square ABCD$ is a quadrilateral with vertices $A,B,C$ and $D$. This area is easily found in this context by splitting $\square ABCD$ into two disjoint triangles- $\Delta ABC$ and $\Delta ACD$ and using the Herons formula \cite{kendig20002000} on each triangle. For example, consider $Area(\Delta \mathcal{Q}_i^{up} \mathcal{Q}_{i-1}^{up} L_i)$. Let $a = ||\mathcal{Q}_i^{up} \mathcal{Q}_{i-1}^{up}||_2$, $b = ||\mathcal{Q}_i^{up} L_i||_2$ and $c = ||\mathcal{Q}_{i-1}^{up} L_i||_2$. Additionally, we define $s = \frac{a+b+c}{2}$. Then, it is true that:
\[Area(\Delta \mathcal{Q}_i^{up} \mathcal{Q}_{i-1}^{up} L_i ) = \sqrt{s(s-a)(s-b)(s-c)}\]

\end{proof}


\begin{figure}[!h]
    \centering
    \includegraphics[scale = 0.35]{diagrams/Upshift_updated.png}
    \caption{The dotted arrow represents the UpShift transportation of the point from $\mathcal{Q}_i^{up}$ to $U_i$}
    \label{fig:Upsh}
\end{figure}
\begin{figure}[!h]
    \centering
    \includegraphics[scale = 0.4]{diagrams/Leftshift_updated.png}
    \caption{The dotted arrow represents the LeftShift transportation of the point from $\mathcal{Q}_i^{up}$ to $U_i$}
    \label{fig:lesh}
\end{figure}
% \begin{theorem}
%     If $i$ is not a Boundary cut point, then the UpShift operation must be performed. The resulting point ($U_i$) is the new fair point $\mathcal{\widetilde{Q}}_i^{up}$.
% \end{theorem}




% \begin{proof}
% (Proof by Contradiction) 
% Let us assume that some point $C$ on the Norm Boundary is the optimal fair (point that leads to ROC with maximum possible AUC while satisfying \ourdef) point. 
% As we can see in Fig \ref{fig:n1}, we have transported $\mathcal{Q}_{i}^{up}$ to $C$. The shaded area denotes the AUC loss due to this transformation. However, as seen in the next figure Fig \ref{fig:n2}, the AUC loss can be decreased by choosing a point ($p_{right}  = A$) recommended by the CutShift operation. Thus, we can always decrease AUC loss by choosing a point on the Norm Boundary. Formally, if point $C$ was the optimal fair point, then the AUC loss with respect to that point is $Area(\square \mathcal{Q}^{up}_{i-1} C \mathcal{Q}^{up}_{i+1} \mathcal{Q}^{up}_{i})$. 
% \\
% However, if $A$ is the optimal fair point (Fig \ref{fig:n2}), then the AUC loss with respect to that point is $Area(\square \mathcal{Q}^{up}_{i} A \mathcal{Q}^{up}_{i+1})$. However, we notice that:$Area(\square \mathcal{Q}^{up}_{i-1} C \mathcal{Q}^{up}_{i+1} \mathcal{Q}^{up}_{i}) = Area(\square \mathcal{Q}^{up}_{i} A \mathcal{Q}^{up}_{i+1}) + Area(\square \mathcal{Q}^{up}_{i-1} C \mathcal{Q}^{up}_{i+1} A)$. Since $Area(\square \mathcal{Q}^{up}_{i-1} C \mathcal{Q}^{up}_{i+1} A) \ge 0$, we have:
%     \[Area(\square \mathcal{Q}^{up}_{i-1} C \mathcal{Q}^{up}_{i+1} \mathcal{Q}^{up}_{i}) \ge Area(\square \mathcal{Q}^{up}_{i} A \mathcal{Q}^{up}_{i+1}) \]
% This is a contradiction to the assumption that $C$ is the optimal fair point. Therefore, $C$ is not an optimal fair point.
% \end{proof}

\begin{figure}[!h]
    \centering
    \includegraphics[scale =0.1]{diagrams/UpShift_proof_NOT.jpg}
    \caption{UpShift Operation is not followed. The light blue area indicates the AUC loss due to this operation.}
    \label{fig:nUpSh}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[scale =0.1]{diagrams/UpShift_proof.jpg}
    \caption{UpShift Operation is followed. The dark blue area indicates the AUC loss due to this operation. It is lesser than the previous AUC loss as seen in Figure 11.}
    \label{fig:UpShp}
\end{figure}

 The optimality of AUC (Theorem 4.2) follows from Theorem D.2, Theorem D.3 and Theorem D.4. 



\subsection{Sample Complexity}
If the \textbf{Assumption 4.2} holds true, then we have the following analysis:
\begin{itemize}
    \item All UpShift Operations will be constant time ($O(1)$).
    \item All CutShift Operations will also be constant time ($O(1)$). This is because \textbf{Assumption 4.2} ensures that we do not have to run through the entire length of $ROC_{up}$ to find the intersection points i.e. $p_{left}$ and $p_{right}$.
\end{itemize}
Therefore, the running time of \ouralgo~ is $O(k)$.
However, when no assumptions are made, then the CutShift operation is no longer $O(1)$. We may have to run through the entire length of $ROC_{up}$ to find the intersection points i.e. $p_{left}$ and $p_{right}$. This makes the CutShift operation $O(k)$. Therefore, the time complexity of \ouralgo~ is $O(k^2)$.

\subsection{Further Variants}
\subsubsection{Multiple Protected Groups}
Our approach is extendable to scenarios involving multiple protected groups. The procedure begins by applying the FROC algorithm to the ROC curve that is immediately above the bottom-most ROC curve. Subsequently, FROC is applied to the ROC curve directly above the one previously processed. This iterative application continues until the top ROC curve is reached. While this method ensures \ourdef fairness across all protected groups, the proof of optimality remains an open question.

\subsubsection{Intersection of ROC Curves}
In cases where the ROC curves intersect more than twice, our algorithm will still produce a fair output. However, the existing optimality theorems do not apply in such scenarios. When intersections occur, the FROC algorithm can be applied to the dominant segments of the ROC curvesâ€”those portions where no intersections are present.

\section{Experiments}
\subsection{Datasets}
\subsubsection{UCI Adult Dataset}
The Adult Dataset \cite{misc_adult_2} comprises 48,842 instances, each containing 14 attributes, including both categorical and continuous variables. The dataset was designed to predict whether an individual's income exceeds \$50,000 per year, making it suitable for binary classification tasks. The features include demographic information such as age, education level, marital status, occupation, work hours per week, and native country, among others.
\subsubsection{COMPAS Recidivism Dataset}
COMPAS Dataset \cite{angwin2016machine} is a widely-discussed and controversial dataset utilized in the field of criminal justice and fairness-aware machine learning. The COMPAS (Correctional Offender Management Profiling for Alternative Sanctions) dataset is commonly employed to explore the potential bias and fairness issues that may arise in predictive models used for criminal justice decisions.
The COMPAS dataset consists of historical data on defendants who were considered for pretrial release in a U.S. county. The data includes various features extracted from defendant profiles, such as age, race, gender, past criminal history, pending charges, and other pertinent factors. Additionally, the dataset contains binary labels indicating whether a defendant was rearrested within a specific period after their release.
\subsection{Protected Groups}
In the context of this paper, we consider the relative performance of the classifiers with respect to the different protected groups - sex (Male and Female) for the Adult Dataset and Race (African Americans and Others) for the COMPAS Dataset. 


\subsection{Experiment Details}
We have performed statistical analysis on FROC, but not on the original classifier. This is because studying the fairness-accuracy trade-of is our goal (as opposed to studying the performance of the baseline classifier). However, it must be noted that since the ROC shifting is deterministic, all randomness emerges from the post-shift classifier builder. For the statistical analysis, we have $10$ iterations of the experiment as $\varepsilon$ runs from $0.001$ to $0.1$ in $20$ intervals. (Except for the case of Random Forest Gini (Adult) : $0.001$ to $0.2$ in $20$ intervals.)

\subsection{Plots}
\subsubsection{Adult Dataset - Weighted ensemble L2}
\begin{figure}[!h]
    \centering
    \includegraphics[width=1\linewidth]{Images/WEL2_Adult_ROC.png}
    \caption{Weighted Ensemble L2 Baseline ROCs for Adult Dataset}
    \label{fig:WEL2_Adult_ROC}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=1\linewidth]{Images/WEL2_Adult_ROC_FROC.png}
    \caption{(Fair $\varepsilon_1 = 0.01$) Weighted Ensemble L2-\ouralgo\   ROCs for Adult Dataset}
    \label{fig:WEL2_Adult_ROC_FROC}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=1\linewidth]{Images/WEL2_Adult_Accuracy.png}
    \caption{Weighted Ensemble L2-\ouralgo\  Accuracy vs. $\varepsilon_1$ (Adult)}
    \label{fig:WEL2_Adult_Accuracy}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=1\linewidth]{Images/WEL2_Adult_DI.png}
    \caption{Weighted Ensemble L2-\ouralgo\  Disparate Impact vs. $\varepsilon_1$ (Adult)}
    \label{fig:WEL2_Adult_DI}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=1\linewidth]{Images/WEL2_Adult_AUC.png}
    \caption{Weighted Ensemble L2-\ouralgo\ AUC loss vs. $\varepsilon_1$ (Adult)}
    \label{fig:WEL2_Adult_AUC}
\end{figure}



% \begin{figure}[!h]
%   \begin{center}
%     \begin{tikzpicture}
%       \begin{axis}[
%           width=\linewidth, % Scale the plot to \linewidth
%           grid=major, 
%           grid style={dashed,gray!30},
%           xlabel=$FPR$, % Set the labels
%           ylabel = $TPR$,
%           % x unit=$V$, % Set the respective units
%           % y unit=$A$,
%           legend style={at={(0.9,0.4)}},
%           % legend style={anchor=northeast},
%           % x tick label style={rotate=90,anchor=east}
%           x tick label style={/pgf/numberformat/.cd, precision=1,/tikz/.cd}
%         ]
%         \addplot+[color = red , mark = triangle]
%         % add a plot from table; you select the columns by using the actual name in
%         % the .csv file (on top)
%         table[x=FPR_Group1,y=TPR_Group1,col sep=comma] {Results/Adult_WEL2_Baseline_ROC.csv}; 
%         \addlegendentry{Male ROC}
        
        
%         \addplot+[color = blue , mark = square]
%         % add a plot from table; you select the columns by using the actual name in
%         % the .csv file (on top)
%         table[x=FPR_Group0,y=TPR_Group0,col sep=comma] {Results/Adult_WEL2_Baseline_ROC.csv}; 
%         \addlegendentry{Female ROC}
%       \end{axis}
%     \end{tikzpicture}
%     \caption{}
%   \end{center}
% \end{figure}

% \begin{figure}[!h]
%     \centering
%     \includegraphics[scale = 0.2]{AAAI24/Adult/FNNC_ROCs.png}
%     \caption{Adult Dataset: Male and Female ROCs}
%     \label{fig:enter-label}
% \end{figure}


% \begin{figure}[!h]
%   \begin{center}
%     \begin{tikzpicture}
%       \begin{axis}[
%           width=\linewidth, % Scale the plot to \linewidth
%           grid=major, 
%           grid style={dashed,gray!30},
%           xlabel=$FPR$, % Set the labels
%           ylabel = $TPR$,
%           % x unit=$V$, % Set the respective units
%           % y unit=$A$,
%           legend style={at={(0.9,0.4)}},
%           % legend style={anchor=northeast},
%           % x tick label style={rotate=90,anchor=east}
%                     x tick label style={/pgf/numberformat/.cd, precision=1,/tikz/.cd}
%         ]
%         \addplot+[color = red , mark = triangle]
%         % add a plot from table; you select the columns by using the actual name in
%         % the .csv file (on top)
%         table[x=F_FPR_Group1,y=F_TPR_Group1,col sep=comma] {Results/Adult_WEL2_Baseline_FROC_e01_ROC.csv}; 
%         \addlegendentry{Fair Male ROC}
        
        
%         \addplot+[color = blue , mark = square]
%         % add a plot from table; you select the columns by using the actual name in
%         % the .csv file (on top)
%         table[x=F_FPR_Group0,y=F_TPR_Group0,col sep=comma] {Results/Adult_WEL2_Baseline_FROC_e01_ROC.csv}; 
%         \addlegendentry{Fair Female ROC}
%       \end{axis}
%     \end{tikzpicture}
%     \caption{(Fair$\varepsilon = 0.01$) Weighted Ensemble L2-\ouralgo\   ROCs for Adult Dataset}
%   \end{center}
% \end{figure}



\begin{itemize}
    \item We have applied FROC with the our fairness parameter $\varepsilon = 0.01$ in \textbf{Figure \ref{fig:WEL2_Adult_ROC_FROC}}. As promised, the resulting ROCs are 'closer' to each other.
    \item In \textbf{Figure \ref{fig:WEL2_Adult_Accuracy}} and \textbf{Figure \ref{fig:WEL2_Adult_DI}}, we have the Accuracy vs. $\varepsilon_1$ and the Disparate Impact vs. $\varepsilon_1$ plot.
    \item This analysis gives us a maximum variance of $1.88\times10^{-6}$ and a maximum CoV (Coefficient of Variation) of $0.15\%$ for Accuracy.
    \item As for the Disparate Impact, the analysis gives us a maximum variance of $2.25\times10^{-5}$ and a maximum CoV of $0.55\%$.
    \item As seen in the plots, we observe that a $1\%$ drop in Accuracy improves the Disparate Impact by $5\%$.
    \item Finally, in \textbf{Figure \ref{fig:WEL2_Adult_AUC}}, we have the AUC loss vs. $\varepsilon$ plot. As seen in the figure, the AUC loss decays to $0$ as our fairness constraint loosens.
\end{itemize}




% \begin{figure}[!h]
%   \begin{center}
%     \begin{tikzpicture}
%       \begin{axis}[
%           width=\linewidth, % Scale the plot to \linewidth
%           grid=major, 
%           grid style={dashed,gray!30},
%           xlabel=$\varepsilon$, % Set the labels
%           ylabel = Accuracy,
%           % x unit=$V$, % Set the respective units
%           % y unit=$A$,
%           legend style={at={(0.9,0.4)}},
%           % legend style={anchor=northeast},
%           % % x tick label style={rotate=90,anchor=east}
%           %           x tick label style={/pgf/
%           %           numberformat
%           %           /.cd,
%           %           precision=2,/tikz/.cd}
%         ]
%         \addplot+[color = red , mark = triangle]
%         % add a plot from table; you select the columns by using the actual name in
%         % the .csv file (on top)
%         table[x=Epsilon,y= Accuracy,col sep=comma] {Results/Adult_WEL2_results.csv}; 
%         % \addlegendentry{Accuracy}
        
        
%         % \addplot+[color = blue , mark = square]
%         % % add a plot from table; you select the columns by using the actual name in
%         % % the .csv file (on top)
%         % table[x=FPR_Group0,y=TPR_Group0,col sep=comma] {Adult_FNNC_Baseline_ROC.csv}; 
%         % \addlegendentry{Female ROC}
%       \end{axis}
%     \end{tikzpicture}
%     \caption{Weighted Ensemble L2-\ouralgo\  Accuracy vs. $\varepsilon$ (Adult)}
%   \end{center}
% \end{figure}



% \begin{figure}[!h]
%   \begin{center}
%     \begin{tikzpicture}
%       \begin{axis}[
%           width=\linewidth, % Scale the plot to \linewidth
%           grid=major, 
%           grid style={dashed,gray!30},
%           xlabel=$\varepsilon$, % Set the labels
%           ylabel = Disparate Impact,
%           % x unit=$V$, % Set the respective units
%           % y unit=$A$,
%           legend style={at={(0.9,0.4)}},
%           % legend style={anchor=northeast},
%           % % x tick label style={rotate=90,anchor=east}
%           %           x tick label style={/pgf/
%           %           numberformat
%           %           /.cd,
%           %           precision=2,/tikz/.cd}
%         ]
%         \addplot+[color = red , mark = triangle]
%         % add a plot from table; you select the columns by using the actual name in
%         % the .csv file (on top)
%         table[x=Epsilon,y= Disp,col sep=comma] {Results/Adult_WEL2_results.csv}; 
%         % \addlegendentry{Disparate}
        
        
%         % \addplot+[color = blue , mark = square]
%         % % add a plot from table; you select the columns by using the actual name in
%         % % the .csv file (on top)
%         % table[x=FPR_Group0,y=TPR_Group0,col sep=comma] {Adult_FNNC_Baseline_ROC.csv}; 
%         % \addlegendentry{Female ROC}
%       \end{axis}
%     \end{tikzpicture}
%     \caption{Weighted Ensemble L2-\ouralgo\  Disparate Impact vs. $\varepsilon$ (Adult)}
%   \end{center}
% \end{figure}

% % \begin{figure}[!h]
% %     \centering
% %     \includegraphics[scale = 0.4]{Adult/FNNC_DI.png}
% %     \caption{Adult Dataset: Disparate Impact vs. $\varepsilon$}
% %     \label{fig:enter-label}
% % \end{figure}

% \begin{figure}[!h]
%   \begin{center}
%     \begin{tikzpicture}
%       \begin{axis}[
%           width=\linewidth, % Scale the plot to \linewidth
%           grid=major, 
%           grid style={dashed,gray!30},
%           xlabel=$\varepsilon$, % Set the labels
%           ylabel = AUC loss,
%           % x unit=$V$, % Set the respective units
%           % y unit=$A$,
%           legend style={at={(0.9,0.4)}},
%           % legend style={anchor=northeast},
%           % % x tick label style={rotate=90,anchor=east}
%           %           x tick label style={/pgf/
%           %           numberformat
%           %           /.cd,
%           %           precision=2,/tikz/.cd}
%         ]
%         \addplot+[color = red , mark = triangle]
%         % add a plot from table; you select the columns by using the actual name in
%         % the .csv file (on top)
%         table[x=Epsilon,y= AUC_loss,col sep=comma] {Results/Adult_WEL2_results.csv}; 
%         % \addlegendentry{Accuracy}
        
        
%         % \addplot+[color = blue , mark = square]
%         % % add a plot from table; you select the columns by using the actual name in
%         % % the .csv file (on top)
%         % table[x=FPR_Group0,y=TPR_Group0,col sep=comma] {Adult_FNNC_Baseline_ROC.csv}; 
%         % \addlegendentry{Female ROC}
%       \end{axis}
%     \end{tikzpicture}
%     \caption{Weighted Ensemble L2-\ouralgo\  AUC loss vs. $\varepsilon$ (Adult)}
%   \end{center}
% \end{figure}

% \begin{figure}[!h]
%     \centering
%     \includegraphics[scale = 0.2]{diagrams/Weighted_Ensemble_L2 ROCs.png}
%     \caption{Adult Dataset: Male and Female ROCs}
%     \label{fig:enter-label}
% \end{figure}

% \begin{figure}[!h]
%     \centering
%     \includegraphics[scale = 0.2]{Adult/Weighted Ensemble L2 Fair ROCs.png}
%     \caption{Adult Dataset: Fair Male and Female ROCs}
%     \label{fig:enter-label}
% \end{figure}


% \begin{figure}[!h]
%     \centering
%     \includegraphics[scale = 0.4]{Adult/WEL2_Accuracy.png}
%     \caption{Adult Dataset: Accuracy vs. $\varepsilon$}
%     \label{fig:enter-label}
% \end{figure}

% \begin{figure}[!h]
%     \centering
%     \includegraphics[scale = 0.4]{Adult/WEL2_DI.png}
%     \caption{Adult Dataset: Disparate Impact vs. $\varepsilon$}
%     \label{fig:enter-label}
% \end{figure}

% \begin{figure}[!h]
%     \centering
%     \includegraphics[scale = 0.4]{Adult/WEL2_AUC.png}
%     \caption{Adult Dataset: AUC loss vs. $\varepsilon$}
%     \label{fig:enter-label}
% \end{figure}


\subsubsection{Adult Dataset - Random Forest Gini}

\begin{figure}[!h]
    \centering
    \includegraphics[width=1\linewidth]{Images/RFG_Adult_Baseline_ROC_FROC.png}
    \caption{Random Forest (Gini) Baseline ROCs for Adult Dataset}
    \label{fig:RFG_Adult_Baseline_ROC_FROC}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=1\linewidth]{Images/RFG_Adult_Baseline_ROC_FROC.png}
    \caption{(Fair $\varepsilon_1 = 0.01$) Random Forest (Gini)-\ouralgo\   ROCs for Adult Dataset}
    \label{fig:RFG_Adult_Baseline_ROC_FROC}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=1\linewidth]{Images/RFG_Adult_Accuracy.png}
    \caption{Random Forest (Gini)-\ouralgo\  Accuracy vs. $\varepsilon_1$ (Adult)}
    \label{fig:RFG_Adult_Accuracy}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=1\linewidth]{Images/RFG_Adult_DI.png}
    \caption{Random Forest (Gini)-\ouralgo\  Disparate Impact vs. $\varepsilon_1$ (Adult)}
    \label{fig:RFG_Adult_DI}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=1\linewidth]{Images/RFG_Adult_AUC.png}
    \caption{Random Forest (Gini)-\ouralgo\ AUC loss vs. $\varepsilon_1$ (Adult)}
    \label{fig:RFG_Adult_AUC}
\end{figure}


% We have the ROCs of the Male and Female protected groups.
% \begin{itemize}
%     \item We have used the Adult Dataset (see Appendix C) in this experiment.
%     \item In this experiment, the sensitive attribute is sex.
%     \item So, this gives us 2 protected groups - Male and Female.
%     \item As can be seen in the plot (\textbf{Figure 11}), the Female ROC dominates the Male ROC. So, the former will be denoted by $ROC_{up}$ and the latter by $ROC_{down}$.
% \end{itemize}

% \begin{figure}[h!]
%   \begin{center}
%     \begin{tikzpicture}
%       \begin{axis}[
%           width=\linewidth, % Scale the plot to \linewidth
%           grid=major, 
%           grid style={dashed,gray!30},
%           xlabel=$FPR$, % Set the labels
%           ylabel = $TPR$,
%           % x unit=$V$, % Set the respective units
%           % y unit=$A$,
%           legend style={at={(0.9,0.4)}},
%           % legend style={anchor=northeast},
%           % x tick label style={rotate=90,anchor=east}
%           x tick label style={/pgf/numberformat/.cd, precision=1,/tikz/.cd}
%         ]
%         \addplot+[color = red , mark = triangle]
%         % add a plot from table; you select the columns by using the actual name in
%         % the .csv file (on top)
%         table[x=FPR_Group1,y=TPR_Group1,col sep=comma] {Results/Adult_RF_Baseline_ROC.csv}; 
%         \addlegendentry{Male ROC}
        
        
%         \addplot+[color = blue , mark = square]
%         % add a plot from table; you select the columns by using the actual name in
%         % the .csv file (on top)
%         table[x=FPR_Group0,y=TPR_Group0,col sep=comma] {Results/Adult_RF_Baseline_ROC.csv}; 
%         \addlegendentry{Female ROC}
%       \end{axis}
%     \end{tikzpicture}
%     \caption{Random Forest (Gini) Baseline ROCs for Adult Dataset}
%   \end{center}
% \end{figure}

% \begin{figure}[!h]
%     \centering
%     \includegraphics[scale = 0.2]{AAAI24/Adult/FNNC_ROCs.png}
%     \caption{Adult Dataset: Male and Female ROCs}
%     \label{fig:enter-label}
% \end{figure}


% \begin{figure}[h!]
%   \begin{center}
%     \begin{tikzpicture}
%       \begin{axis}[
%           width=\linewidth, % Scale the plot to \linewidth
%           grid=major, 
%           grid style={dashed,gray!30},
%           xlabel=$FPR$, % Set the labels
%           ylabel = $TPR$,
%           % x unit=$V$, % Set the respective units
%           % y unit=$A$,
%           legend style={at={(0.9,0.4)}},
%           % legend style={anchor=northeast},
%           % x tick label style={rotate=90,anchor=east}
%                     x tick label style={/pgf/numberformat/.cd, precision=1,/tikz/.cd}
%         ]
%         \addplot+[color = red , mark = triangle]
%         % add a plot from table; you select the columns by using the actual name in
%         % the .csv file (on top)
%         table[x=F_FPR_Group1,y=F_TPR_Group1,col sep=comma] {Results/Adult_RF_Baseline_FROC_e01_ROC.csv}; 
%         \addlegendentry{Fair Male ROC}
        
        
%         \addplot+[color = blue , mark = square]
%         % add a plot from table; you select the columns by using the actual name in
%         % the .csv file (on top)
%         table[x=F_FPR_Group0,y=F_TPR_Group0,col sep=comma] {Results/Adult_RF_Baseline_FROC_e01_ROC.csv}; 
%         \addlegendentry{Fair Female ROC}
%       \end{axis}
%     \end{tikzpicture}
%     \caption{(Fair, $\varepsilon = 0.01$) Random Forest (Gini)-\ouralgo\   ROCs for Adult Dataset}
%   \end{center}
% \end{figure}
\begin{itemize}
    \item We have applied FROC with the our fairness parameter $\varepsilon = 0.01$ in \textbf{Figure \ref{fig:RFG_Adult_Baseline_ROC_FROC}}. As promised, the resulting ROCs are 'closer' to each other.
    \item In \textbf{Figure \ref{fig:RFG_Adult_Accuracy}} and \textbf{Figure \ref{fig:RFG_Adult_DI}}, we have the Accuracy vs. $\varepsilon_1$ and the Disparate Impact vs. $\varepsilon_1$ plot.
    \item This analysis gives us a maximum variance of $8.3\times10^{-7}$ and a maximum CoV (Coefficient of Variation) of $0.1\%$ for Accuracy.
    \item As for the Disparate Impact, the analysis gives us a maximum variance of $7.59\times10^{-6}$ and a maximum CoV of $0.75\%$.
    \item As seen in the plots, we observe that a $1\%$ drop in Accuracy improves the Disparate Impact by $7\%$.
    \item Finally, in \textbf{Figure \ref{fig:RFG_Adult_AUC}}, we have the AUC loss vs. $\varepsilon_1$ plot. As seen in the figure, the AUC loss decays to $0$ as our fairness constraint loosens.
\end{itemize}



% \begin{figure}[!h]
%     \centering
%     \includegraphics[scale = 0.2]{Adult/FNNC_Fair ROCs.png}
%     \caption{Adult Dataset: Fair Male and Female ROCs}
%     \label{fig:enter-label}
% \end{figure}

% \begin{figure}[h!]
%   \begin{center}
%     \begin{tikzpicture}
%       \begin{axis}[
%           width=\linewidth, % Scale the plot to \linewidth
%           grid=major, 
%           grid style={dashed,gray!30},
%           xlabel=$\varepsilon$, % Set the labels
%           ylabel = Accuracy,
%           % x unit=$V$, % Set the respective units
%           % y unit=$A$,
%           legend style={at={(0.9,0.4)}},
%           % legend style={anchor=northeast},
%           % % x tick label style={rotate=90,anchor=east}
%           %           x tick label style={/pgf/
%           %           numberformat
%           %           /.cd,
%           %           precision=2,/tikz/.cd}
%         ]
%         \addplot+[color = red , mark = triangle]
%         % add a plot from table; you select the columns by using the actual name in
%         % the .csv file (on top)
%         table[x=Epsilon,y= Accuracy,col sep=comma] {Results/Adult_RF_results.csv}; 
%         % \addlegendentry{Accuracy}
        
        
%         % \addplot+[color = blue , mark = square]
%         % % add a plot from table; you select the columns by using the actual name in
%         % % the .csv file (on top)
%         % table[x=FPR_Group0,y=TPR_Group0,col sep=comma] {Adult_FNNC_Baseline_ROC.csv}; 
%         % \addlegendentry{Female ROC}
%       \end{axis}
%     \end{tikzpicture}
%     \caption{Random Forest (Gini)-\ouralgo\ Accuracy vs. $\varepsilon$ (Adult)}
%   \end{center}
% \end{figure}

% % \begin{figure}[!h]
% %     \centering
% %     \includegraphics[scale = 0.4]{Adult/FNNC_Accuracy.png}
% %     \caption{Adult Dataset: Accuracy vs. $\varepsilon$}
% %     \label{fig:enter-label}
% % \end{figure}

% \begin{figure}[h!]
%   \begin{center}
%     \begin{tikzpicture}
%       \begin{axis}[
%           width=\linewidth, % Scale the plot to \linewidth
%           grid=major, 
%           grid style={dashed,gray!30},
%           xlabel=$\varepsilon$, % Set the labels
%           ylabel = Disparate Impact,
%           % x unit=$V$, % Set the respective units
%           % y unit=$A$,
%           legend style={at={(0.9,0.4)}},
%           % legend style={anchor=northeast},
%           % % x tick label style={rotate=90,anchor=east}
%           %           x tick label style={/pgf/
%           %           numberformat
%           %           /.cd,
%           %           precision=2,/tikz/.cd}
%         ]
%         \addplot+[color = red , mark = triangle]
%         % add a plot from table; you select the columns by using the actual name in
%         % the .csv file (on top)
%         table[x=Epsilon,y= Disp,col sep=comma] {Results/Adult_RF_results.csv}; 
%         % \addlegendentry{Disparate}
        
        
%         % \addplot+[color = blue , mark = square]
%         % % add a plot from table; you select the columns by using the actual name in
%         % % the .csv file (on top)
%         % table[x=FPR_Group0,y=TPR_Group0,col sep=comma] {Adult_FNNC_Baseline_ROC.csv}; 
%         % \addlegendentry{Female ROC}
%       \end{axis}
%     \end{tikzpicture}
%     \caption{Random Forest (Gini)-\ouralgo\ Disparate Impact vs. $\varepsilon$ (Adult)}
%   \end{center}
% \end{figure}

% % \begin{figure}[!h]
% %     \centering
% %     \includegraphics[scale = 0.4]{Adult/FNNC_DI.png}
% %     \caption{Adult Dataset: Disparate Impact vs. $\varepsilon$}
% %     \label{fig:enter-label}
% % \end{figure}

% \begin{figure}[h!]
%   \begin{center}
%     \begin{tikzpicture}
%       \begin{axis}[
%           width=\linewidth, % Scale the plot to \linewidth
%           grid=major, 
%           grid style={dashed,gray!30},
%           xlabel=$\varepsilon$, % Set the labels
%           ylabel = AUC loss,
%           % x unit=$V$, % Set the respective units
%           % y unit=$A$,
%           legend style={at={(0.9,0.4)}},
%           % legend style={anchor=northeast},
%           % % x tick label style={rotate=90,anchor=east}
%           %           x tick label style={/pgf/
%           %           numberformat
%           %           /.cd,
%           %           precision=2,/tikz/.cd}
%         ]
%         \addplot+[color = red , mark = triangle]
%         % add a plot from table; you select the columns by using the actual name in
%         % the .csv file (on top)
%         table[x=Epsilon,y= AUC_loss,col sep=comma] {Results/Adult_RF_results.csv}; 
%         % \addlegendentry{Accuracy}
        
        
%         % \addplot+[color = blue , mark = square]
%         % % add a plot from table; you select the columns by using the actual name in
%         % % the .csv file (on top)
%         % table[x=FPR_Group0,y=TPR_Group0,col sep=comma] {Adult_FNNC_Baseline_ROC.csv}; 
%         % \addlegendentry{Female ROC}
%       \end{axis}
%     \end{tikzpicture}
%     \caption{Random Forest (Gini) AUC loss vs. $\varepsilon$ (Adult)}
%   \end{center}
% \end{figure}



% \begin{figure}[!h]
%     \centering
%     \includegraphics[scale = 0.2]{Adult/Random Forest ROCs.png}
%     \caption{Adult Dataset: Male and Female ROCs}
%     \label{fig:enter-label}
% \end{figure}

% \begin{figure}[!h]
%     \centering
%     \includegraphics[scale = 0.2]{Adult/Random Forest Fair ROCs.png}
%     \caption{Adult Dataset: Fair Male and Female ROCs}
%     \label{fig:enter-label}
% \end{figure}


% \begin{figure}[!h]
%     \centering
%     \includegraphics[scale = 0.4]{Adult/RF_Accuracy.png}
%     \caption{Adult Dataset: Accuracy vs. $\varepsilon$}
%     \label{fig:enter-label}
% \end{figure}

% \begin{figure}[!h]
%     \centering
%     \includegraphics[scale = 0.4]{AAAI24/Adult/RF_DI.png}
%     \caption{Adult Dataset: Disparate Impact vs. $\varepsilon$}
%     \label{fig:enter-label}
% \end{figure}

% \begin{figure}[!h]
%     \centering
%     \includegraphics[scale = 0.4]{Adult/RF_AUC.png}
%     \caption{Adult Dataset: AUC loss vs. $\varepsilon$}
%     \label{fig:enter-label}
% \end{figure}

\subsubsection{Adult Dataset - FNNC}
% We have the ROCs of the Male and Female protected groups.
% \begin{itemize}
%     \item We have used the Adult Dataset (see Appendix C) in this experiment.
%     \item In this experiment, the sensitive attribute is sex.
%     \item So, this gives us 2 protected groups - Male and Female.
%     \item As can be seen in the plot (\textbf{Figure 16}), the Female ROC dominates the Male ROC. So, the former will be denoted by $ROC_{up}$ and the latter by $ROC_{down}$.
% \end{itemize}

\begin{figure}[!h]
    \centering
    \includegraphics[width=1\linewidth]{Images/FNNC_Adult_Baseline_ROC.png}
    \caption{FNNC Baseline ROCs for Adult Dataset}
    \label{fig:FNNC_Adult_ROC}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=1\linewidth]{Images/FNNC_Adult_Baseline_ROC_FROC.png}
    \caption{(Fair $\varepsilon_1 = 0.01$) FNNC-\ouralgo\   ROCs for Adult Dataset}
    \label{fig:FNNC_Adult_ROC_FROC}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=1\linewidth]{Images/FNNC_Adult_Acc_DI.png}
    \caption{FNNC-\ouralgo\  Accuracy vs. $\varepsilon_1$ (Adult)}
    \label{fig:FNNC_Adult_Accuracy}
\end{figure}

% \begin{figure}[!h]
%     \centering
%     \includegraphics[width=1\linewidth]{Images/WEL2_Adult_DI.png}
%     \caption{Weighted Ensemble L2-\ouralgo\  Disparate Impact vs. $\varepsilon_1$ (Adult)}
%     \label{fig:WEL2_Adult_DI}
% \end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=1\linewidth]{Images/FNNC_Adult_AUC.png}
    \caption{FNNC-\ouralgo\ AUC loss vs. $\varepsilon_1$ (Adult)}
    \label{fig:FNNC_Adult_AUC}
\end{figure}





% \begin{figure}[h!]
%   \begin{center}
%     \begin{tikzpicture}
%       \begin{axis}[
%           width=\linewidth, % Scale the plot to \linewidth
%           grid=major, 
%           grid style={dashed,gray!30},
%           xlabel=$FPR$, % Set the labels
%           ylabel = $TPR$,
%           % x unit=$V$, % Set the respective units
%           % y unit=$A$,
%           legend style={at={(0.9,0.4)}},
%           % legend style={anchor=northeast},
%           % x tick label style={rotate=90,anchor=east}
%           x tick label style={/pgf/numberformat/.cd, precision=1,/tikz/.cd}
%         ]
%         \addplot+[color = red , mark = triangle]
%         % add a plot from table; you select the columns by using the actual name in
%         % the .csv file (on top)
%         table[x=FPR_Group1,y=TPR_Group1,col sep=comma] {Adult_FNNC_Baseline_ROC.csv}; 
%         \addlegendentry{Male ROC}
        
        
%         \addplot+[color = blue , mark = square]
%         % add a plot from table; you select the columns by using the actual name in
%         % the .csv file (on top)
%         table[x=FPR_Group0,y=TPR_Group0,col sep=comma] {Adult_FNNC_Baseline_ROC.csv}; 
%         \addlegendentry{Female ROC}
%       \end{axis}
%     \end{tikzpicture}
%     \caption{FNNC  ROCs for Adult Dataset}
%   \end{center}
% \end{figure}

% % \begin{figure}[!h]
% %     \centering
% %     \includegraphics[scale = 0.2]{AAAI24/Adult/FNNC_ROCs.png}
% %     \caption{Adult Dataset: Male and Female ROCs}
% %     \label{fig:enter-label}
% % \end{figure}


% \begin{figure}[h!]
%   \begin{center}
%     \begin{tikzpicture}
%       \begin{axis}[
%           width=\linewidth, % Scale the plot to \linewidth
%           grid=major, 
%           grid style={dashed,gray!30},
%           xlabel=$FPR$, % Set the labels
%           ylabel = $TPR$,
%           % x unit=$V$, % Set the respective units
%           % y unit=$A$,
%           legend style={at={(0.9,0.4)}},
%           % legend style={anchor=northeast},
%           % x tick label style={rotate=90,anchor=east}
%                     x tick label style={/pgf/numberformat/.cd, precision=1,/tikz/.cd}
%         ]
%         \addplot+[color = red , mark = triangle]
%         % add a plot from table; you select the columns by using the actual name in
%         % the .csv file (on top)
%         table[x=F_FPR_Group1,y=F_TPR_Group1,col sep=comma] {Adult_FNNC_Baseline_FROC_e01_ROC.csv}; 
%         \addlegendentry{Fair Male ROC}
        
        
%         \addplot+[color = blue , mark = square]
%         % add a plot from table; you select the columns by using the actual name in
%         % the .csv file (on top)
%         table[x=F_FPR_Group0,y=F_TPR_Group0,col sep=comma] {Adult_FNNC_Baseline_FROC_e01_ROC.csv}; 
%         \addlegendentry{Fair Female ROC}
%       \end{axis}
%     \end{tikzpicture}
%     \caption{(Fair, $\varepsilon = 0.01$) FNNC-\ouralgo\  
%  ROCs for Adult Dataset}
%   \end{center}
% \end{figure}

\begin{itemize}
    \item We have applied FROC with the our fairness parameter $\varepsilon_1 = 0.01$ in \textbf{Figure \ref{fig:RFG_COMPAS_Baseline_ROC_FROC}}. As promised, the resulting ROCs are 'closer' to each other.
    \item In \textbf{Figure \ref{fig:FNNC_Adult_Accuracy}}, we have the Accuracy vs. $\varepsilon_1$ and the Disparate Impact vs. $\varepsilon_1$ plot. We also have the $\varepsilon_{FNNC} vs. \varepsilon_{FROC}$ plot.
    \item We find that in the FNNC is slightly lower than FROC in terms of accuracy. We assign it to the fact that FNNC may overachieve the target fairness for smaller values of $\varepsilon_1$ (Evident from Table 2 [Padala and Gujar 2021]). FROC drops AUC minimally to achieve target fairness.
    \item This analysis gives us a maximum variance of $6.6\times10^{-7}$ and a maximum CoV (Coefficient of Variation) of $0.09\%$ for Accuracy.
    \item As for the Disparate Impact, the analysis gives us a maximum variance of $1\times10^{-4}$ and a maximum CoV of $1.26\%$.
    \item As seen in the plots, we observe that a $1\%$ drop in Accuracy improves the Disparate Impact by $5\%$.
    \item Finally, in \textbf{Figure \ref{fig:FNNC_Adult_AUC}}, we have the AUC loss vs. $\varepsilon_1$ plot. As seen in the figure, the AUC loss decays to $0$ as our fairness constraint loosens.
\end{itemize}



% \begin{figure}[!h]
%   \begin{center}
%     \begin{tikzpicture}
%       \begin{axis}[
%           width=170, % Scale the plot to \linewidth
%           grid=major, 
%           grid style={dashed,gray!30},
%           xlabel=$FPR$, % Set the labels
%           ylabel = $TPR$,
%           % x unit=$V$, % Set the respective units
%           % y unit=$A$,
%           legend style={at={(1.1,0.4)}},
%           % legend style={anchor=northeast},
%           % x tick label style={rotate=90,anchor=east}
%                     % x tick label style={/pgf/numberformat/.cd, precision=1,/tikz/.cd}
%         ]
%         \addplot+[color = red , mark = triangle]
%         % add a plot from table; you select the columns by using the actual name in
%         % the .csv file (on top)
%         table[x=F_FPR_Group1,y=F_TPR_Group1,col sep=comma] {COMPAS_FNNC_Baseline_FROC_e01_ROC.csv}; 
%         \addlegendentry{Fair Others ROC}
        
        
%         \addplot+[color = blue , mark = square]
%         % add a plot from table; you select the columns by using the actual name in
%         % the .csv file (on top)
%         table[x=F_FPR_Group0,y=F_TPR_Group0,col sep=comma] {COMPAS_FNNC_Baseline_FROC_e01_ROC.csv}; 
%         \addlegendentry{Fair African ROC}


%         \addplot+[color = green, mark = x]
%         % add a plot from table; you select the columns by using the actual name in
%         % the .csv file (on top)
%         table[x=FPR_Group1,y=TPR_Group1,col sep=comma] {COMPAS_FNNC_Baseline_ROC.csv}; 
%         \addlegendentry{Others ROC}
%       \end{axis}
%     \end{tikzpicture}
%     \caption{Fair (FROC $\varepsilon = 0.01$) FNNC Baseline ROCs for COMPAS Dataset}
%     \label{fig:ROC_COMPAS}
%   \end{center}
% \end{figure}







% \begin{figure}[!h]
%     \centering
%     \begin{tikzpicture}
%     \pgfplotsset{
%     % width=\linewidth,
%     width=120,
%     scale only axis,
%     scaled x ticks=base 10:1,
%     xmin=0, xmax=0.1,
%     y axis style/.style={
%     yticklabel style=#1,
%     ylabel style=#1,
%     y axis line style=#1,
%     ytick style=#1
%        }
%     }
    
%     \begin{axis}[
%       axis y line*=left,
%       y axis style=blue!75!black,
%       % ymin=0, ymax=80,
%       xlabel=$\varepsilon_{FROC} ~and~\varepsilon_{FNNC}$,
%       ylabel= Accuracy,
%           grid=major, 
%           grid style={dashed,gray!30},
%           legend style={at={(0.9,0.7)} , anchor= northeast}
%     ]
%     %%% FNNC accuracy
%     \addplot[smooth,mark= x,blue] 
%       coordinates{
%         (0.001,0.8243)
%         (0.02, 0.8252) 
%         (0.04, 0.8217)
%         (0.06, 0.8304)
%         (0.08, 0.8273)
%         (0.1 , 0.845)
%     };
%     \addlegendentry{FNNC Acc.}

%     %%% FROC accuracy
%     \addplot[smooth,mark=square, blue] 
%         table[x=Epsilon,y=Accuracy,col sep=comma] {Adult_FNNC_Results.csv}; 
%     %     coordinates{
%     %     (0.001,0.824)
%     %     (0.02, 0.825) 
%     %     (0.04, 0.821)
%     %     (0.06, 0.830)
%     %     (0.08, 0.827)
%     %     (0.1 , 0.83)
%     % };
%     \addlegendentry{FROC Acc.}



    
%     \end{axis}
    
%     \begin{axis}[
%       axis y line*=right,
%       axis x line=none,
%       % ymin=0, ymax=100,
%       ylabel=Disparate Impact (DI),
%       % y axis style=red!75!black
%       legend style={at={(1.7,0)}}
%     ]
%     \addplot[smooth,mark=*,red] 
%     %   coordinates{
%     %     (0,0)
%     %     (0.0148,48) 
%     %     (0.0295,66)
%     %     (0.0441,66)
%     %     (0.059,45.0) 
%     % };
%     table[x=Epsilon,y=Disp,col sep=comma] {Adult_FNNC_Results.csv};
%     \addlegendentry{FROC DI.}
%     \end{axis}
    
%     \end{tikzpicture}
%     \caption{Comparison of FNNC and FNNC-FROC for Adult Dataset}
%     \label{fig:pgfplot-two}
% \end{figure}

% \begin{figure}[!h]
%     \centering
%     \includegraphics[scale = 0.2]{Adult/FNNC_Fair ROCs.png}
%     \caption{Adult Dataset: Fair Male and Female ROCs}
%     \label{fig:enter-label}
% \end{figure}

% \begin{figure}[h!]
%   \begin{center}
%     \begin{tikzpicture}
%       \begin{axis}[
%           width=\linewidth, % Scale the plot to \linewidth
%           grid=major, 
%           grid style={dashed,gray!30},
%           xlabel=$\varepsilon$, % Set the labels
%           ylabel = Accuracy,
%           % x unit=$V$, % Set the respective units
%           % y unit=$A$,
%           legend style={at={(0.9,0.4)}},
%           % legend style={anchor=northeast},
%           % % x tick label style={rotate=90,anchor=east}
%           %           x tick label style={/pgf/
%           %           numberformat
%           %           /.cd,
%           %           precision=2,/tikz/.cd}
%         ]
%         \addplot+[color = red , mark = triangle]
%         % add a plot from table; you select the columns by using the actual name in
%         % the .csv file (on top)
%         table[x=Epsilon,y= Accuracy,col sep=comma] {Adult_FNNC_results.csv}; 
%         % \addlegendentry{Accuracy}
        
        
%         % \addplot+[color = blue , mark = square]
%         % % add a plot from table; you select the columns by using the actual name in
%         % % the .csv file (on top)
%         % table[x=FPR_Group0,y=TPR_Group0,col sep=comma] {Adult_FNNC_Baseline_ROC.csv}; 
%         % \addlegendentry{Female ROC}
%       \end{axis}
%     \end{tikzpicture}
%     \caption{FNNC Accuracy vs. $\varepsilon$}
%   \end{center}
% \end{figure}

% % \begin{figure}[!h]
% %     \centering
% %     \includegraphics[scale = 0.4]{Adult/FNNC_Accuracy.png}
% %     \caption{Adult Dataset: Accuracy vs. $\varepsilon$}
% %     \label{fig:enter-label}
% % \end{figure}

% \begin{figure}[h!]
%   \begin{center}
%     \begin{tikzpicture}
%       \begin{axis}[
%           width=\linewidth, % Scale the plot to \linewidth
%           grid=major, 
%           grid style={dashed,gray!30},
%           xlabel=$\varepsilon$, % Set the labels
%           ylabel = Disparate Impact,
%           % x unit=$V$, % Set the respective units
%           % y unit=$A$,
%           legend style={at={(0.9,0.4)}},
%           % legend style={anchor=northeast},
%           % % x tick label style={rotate=90,anchor=east}
%           %           x tick label style={/pgf/
%           %           numberformat
%           %           /.cd,
%           %           precision=2,/tikz/.cd}
%         ]
%         \addplot+[color = red , mark = triangle]
%         % add a plot from table; you select the columns by using the actual name in
%         % the .csv file (on top)
%         table[x=Epsilon,y= Disp,col sep=comma] {Adult_FNNC_results.csv}; 
%         % \addlegendentry{Disparate}
        
        
%         % \addplot+[color = blue , mark = square]
%         % % add a plot from table; you select the columns by using the actual name in
%         % % the .csv file (on top)
%         % table[x=FPR_Group0,y=TPR_Group0,col sep=comma] {Adult_FNNC_Baseline_ROC.csv}; 
%         % \addlegendentry{Female ROC}
%       \end{axis}
%     \end{tikzpicture}
%     \caption{FNNC Disparate Impact vs. $\varepsilon$}
%   \end{center}
% \end{figure}

% % \begin{figure}[!h]
% %     \centering
% %     \includegraphics[scale = 0.4]{Adult/FNNC_DI.png}
% %     \caption{Adult Dataset: Disparate Impact vs. $\varepsilon$}
% %     \label{fig:enter-label}
% % \end{figure}

% \begin{figure}[h!]
%   \begin{center}
%     \begin{tikzpicture}
%       \begin{axis}[
%           width=\linewidth, % Scale the plot to \linewidth
%           grid=major, 
%           grid style={dashed,gray!30},
%           xlabel=$\varepsilon$, % Set the labels
%           ylabel = AUC loss,
%           % x unit=$V$, % Set the respective units
%           % y unit=$A$,
%           legend style={at={(0.9,0.4)}},
%           % legend style={anchor=northeast},
%           % % x tick label style={rotate=90,anchor=east}
%           %           x tick label style={/pgf/
%           %           numberformat
%           %           /.cd,
%           %           precision=2,/tikz/.cd}
%         ]
%         \addplot+[color = red , mark = triangle]
%         % add a plot from table; you select the columns by using the actual name in
%         % the .csv file (on top)
%         table[x=Epsilon,y= AUC_loss,col sep=comma] {Adult_FNNC_results.csv}; 
%         % \addlegendentry{Accuracy}
        
        
%         % \addplot+[color = blue , mark = square]
%         % % add a plot from table; you select the columns by using the actual name in
%         % % the .csv file (on top)
%         % table[x=FPR_Group0,y=TPR_Group0,col sep=comma] {Adult_FNNC_Baseline_ROC.csv}; 
%         % \addlegendentry{Female ROC}
%       \end{axis}
%     \end{tikzpicture}
%     \caption{FNNC-\ouralgo\ AUC loss vs. $\varepsilon$ (Adult)}
%   \end{center}
% \end{figure}

% \begin{figure}[!h]
%     \centering
%     \includegraphics[scale = 0.4]{Adult/FNNC_AUC.png}
%     \caption{Adult Dataset: AUC loss vs. $\varepsilon$}
%     \label{fig:enter-label}
% \end{figure}


\subsubsection{COMPAS Dataset - Weighted ensemble L2}
% \begin{itemize}
%     \item We have used the COMPAS Dataset (see Appendix C) in this experiment.
%     \item In this experiment, the sensitive attribute is ethnicity.
%     \item So, this gives us 2 protected groups - Others and African-American.
%     \item As can be seen in the plot (\textbf{Figure 21}), the Others ROC dominates the African American ROC. So, the former will be denoted by $ROC_{up}$ and the latter by $ROC_{down}$.
% \end{itemize}

\begin{figure}[!h]
    \centering
    \includegraphics[width=1\linewidth]{Images/WEL2_COMPAS_Baseline_ROC.png}
    \caption{Weighted Ensemble L2 Baseline ROCs for COMPAS Dataset}
    \label{fig:WEL2_COMPAS_ROC}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=1\linewidth]{Images/WEL2_COMPAS_Baseline_ROC_FROC.png}
    \caption{(Fair $\varepsilon_1 = 0.01$) Weighted Ensemble L2-\ouralgo\   ROCs for COMPAS Dataset}
    \label{fig:WEL2_COMPAS_ROC_FROC}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=1\linewidth]{Images/WEL2_COMPAS_Accuracy.png}
    \caption{Weighted Ensemble L2-\ouralgo\  Accuracy vs. $\varepsilon_1$ (COMPAS)}
    \label{fig:WEL2_COMPAS_Accuracy}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=1\linewidth]{Images/WEL2_COMPAS_DI.png}
    \caption{Weighted Ensemble L2-\ouralgo\  Disparate Impact vs. $\varepsilon_1$ (COMPAS)}
    \label{fig:WEL2_COMPAS_DI}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=1\linewidth]{Images/WEL2_COMPAS_AUC.png}
    \caption{Weighted Ensemble L2-\ouralgo\ AUC loss vs. $\varepsilon_1$ (COMPAS)}
    \label{fig:WEL2_COMPAS_AUC}
\end{figure}


% \begin{figure}[h!]
%   \begin{center}
%     \begin{tikzpicture}
%       \begin{axis}[
%           width=\linewidth, % Scale the plot to \linewidth
%           grid=major, 
%           grid style={dashed,gray!30},
%           xlabel=$FPR$, % Set the labels
%           ylabel = $TPR$,
%           % x unit=$V$, % Set the respective units
%           % y unit=$A$,
%           legend style={at={(0.9,0.4)}},
%           % legend style={anchor=northeast},
%           % x tick label style={rotate=90,anchor=east}
%           x tick label style={/pgf/numberformat/.cd, precision=1,/tikz/.cd}
%         ]
%         \addplot+[color = red , mark = triangle]
%         % add a plot from table; you select the columns by using the actual name in
%         % the .csv file (on top)
%         table[x=FPR_Group1,y=TPR_Group1,col sep=comma] {COMPAS_WEL2_Baseline_ROC.csv}; 
%         \addlegendentry{Other ROC}
        
        
%         \addplot+[color = blue , mark = square]
%         % add a plot from table; you select the columns by using the actual name in
%         % the .csv file (on top)
%         table[x=FPR_Group0,y=TPR_Group0,col sep=comma] {COMPAS_WEL2_Baseline_ROC.csv}; 
%         \addlegendentry{African American ROC}
%       \end{axis}
%     \end{tikzpicture}
%     \caption{Weighted Ensemble L2 Baseline ROCs for COMPAS Dataset}
%   \end{center}
% \end{figure}

% % \begin{figure}[!h]
% %     \centering
% %     \includegraphics[scale = 0.2]{AAAI24/Adult/FNNC_ROCs.png}
% %     \caption{Adult Dataset: Male and Female ROCs}
% %     \label{fig:enter-label}
% % \end{figure}


% \begin{figure}[h!]
%   \begin{center}
%     \begin{tikzpicture}
%       \begin{axis}[
%           width=\linewidth, % Scale the plot to \linewidth
%           grid=major, 
%           grid style={dashed,gray!30},
%           xlabel=$FPR$, % Set the labels
%           ylabel = $TPR$,
%           % x unit=$V$, % Set the respective units
%           % y unit=$A$,
%           legend style={at={(0.9,0.4)}},
%           % legend style={anchor=northeast},
%           % x tick label style={rotate=90,anchor=east}
%                     x tick label style={/pgf/numberformat/.cd, precision=1,/tikz/.cd}
%         ]
%         \addplot+[color = red , mark = triangle]
%         % add a plot from table; you select the columns by using the actual name in
%         % the .csv file (on top)
%         table[x=F_FPR_Group1,y=F_TPR_Group1,col sep=comma] {COMPAS_WEL2_Baseline_FROC_e01_ROC.csv}; 
%         \addlegendentry{Fair Other ROC}
        
        
%         \addplot+[color = blue , mark = square]
%         % add a plot from table; you select the columns by using the actual name in
%         % the .csv file (on top)
%         table[x=F_FPR_Group0,y=F_TPR_Group0,col sep=comma] {COMPAS_WEL2_Baseline_FROC_e01_ROC.csv}; 
%         \addlegendentry{Fair Female ROC}
%       \end{axis}
%     \end{tikzpicture}
%     \caption{(Fair, $\varepsilon = 0.01$) Weighted Ensemble L2\ouralgo\ ROCs for COMPAS Dataset}
%   \end{center}
% \end{figure}

\begin{itemize}
    \item We have applied FROC with the our fairness parameter $\varepsilon_1 = 0.01$ in \textbf{Figure \ref{fig:WEL2_COMPAS_ROC_FROC}}. As promised, the resulting ROCs are 'closer' to each other.
    \item In \textbf{Figure \ref{fig:WEL2_COMPAS_Accuracy}} and \textbf{Figure \ref{fig:WEL2_COMPAS_DI}}, we have the Accuracy vs. $\varepsilon_1$ and the Disparate Impact vs. $\varepsilon_1$ plot.
    \item This analysis gives us a maximum variance of $1.44\times10^{-5}$ and a maximum CoV (Coefficient of Variation) of $0.54\%$ for Accuracy.
    \item As for the Disparate Impact, the analysis gives us a maximum variance of $1.6\times10^{-4}$ and a maximum CoV of $1.69\%$.
    \item As seen in the plots, we observe that a $1\%$ drop in Accuracy improves the Disparate Impact by $7\%$.
    \item Finally, in \textbf{Figure \ref{fig:WEL2_COMPAS_AUC}}, we have the AUC loss vs. $\varepsilon_1$ plot. As seen in the figure, the AUC loss decays to $0$ as our fairness constraint loosens.
\end{itemize}


% \begin{figure}[!h]
%     \centering
%     \includegraphics[scale = 0.2]{Adult/FNNC_Fair ROCs.png}
%     \caption{Adult Dataset: Fair Male and Female ROCs}
%     \label{fig:enter-label}
% \end{figure}

% \begin{figure}[h!]
%   \begin{center}
%     \begin{tikzpicture}
%       \begin{axis}[
%           width=\linewidth, % Scale the plot to \linewidth
%           grid=major, 
%           grid style={dashed,gray!30},
%           xlabel=$\varepsilon$, % Set the labels
%           ylabel = Accuracy,
%           % x unit=$V$, % Set the respective units
%           % y unit=$A$,
%           legend style={at={(0.9,0.4)}},
%           % legend style={anchor=northeast},
%           % % x tick label style={rotate=90,anchor=east}
%           %           x tick label style={/pgf/
%           %           numberformat
%           %           /.cd,
%           %           precision=2,/tikz/.cd}
%         ]
%         \addplot+[color = red , mark = triangle]
%         % add a plot from table; you select the columns by using the actual name in
%         % the .csv file (on top)
%         table[x=Epsilon,y= Accuracy,col sep=comma] {COMPAS_WEL2_results.csv}; 
%         % \addlegendentry{Accuracy}
        
        
%         % \addplot+[color = blue , mark = square]
%         % % add a plot from table; you select the columns by using the actual name in
%         % % the .csv file (on top)
%         % table[x=FPR_Group0,y=TPR_Group0,col sep=comma] {Adult_FNNC_Baseline_ROC.csv}; 
%         % \addlegendentry{Female ROC}
%       \end{axis}
%     \end{tikzpicture}
%     \caption{Weighted Ensemble L2-\ouralgo\  Accuracy vs. $\varepsilon$ (COMPAS)}
%   \end{center}
% \end{figure}

% % \begin{figure}[!h]
% %     \centering
% %     \includegraphics[scale = 0.4]{Adult/FNNC_Accuracy.png}
% %     \caption{Adult Dataset: Accuracy vs. $\varepsilon$}
% %     \label{fig:enter-label}
% % \end{figure}

% \begin{figure}[h!]
%   \begin{center}
%     \begin{tikzpicture}
%       \begin{axis}[
%           width=\linewidth, % Scale the plot to \linewidth
%           grid=major, 
%           grid style={dashed,gray!30},
%           xlabel=$\varepsilon$, % Set the labels
%           ylabel = Disparate Impact,
%           % x unit=$V$, % Set the respective units
%           % y unit=$A$,
%           legend style={at={(0.9,0.4)}},
%           % legend style={anchor=northeast},
%           % % x tick label style={rotate=90,anchor=east}
%           %           x tick label style={/pgf/
%           %           numberformat
%           %           /.cd,
%           %           precision=2,/tikz/.cd}
%         ]
%         \addplot+[color = red , mark = triangle]
%         % add a plot from table; you select the columns by using the actual name in
%         % the .csv file (on top)
%         table[x=Epsilon,y= Disp,col sep=comma] {COMPAS_WEL2_results.csv}; 
%         % \addlegendentry{Disparate}
        
        
%         % \addplot+[color = blue , mark = square]
%         % % add a plot from table; you select the columns by using the actual name in
%         % % the .csv file (on top)
%         % table[x=FPR_Group0,y=TPR_Group0,col sep=comma] {Adult_FNNC_Baseline_ROC.csv}; 
%         % \addlegendentry{Female ROC}
%       \end{axis}
%     \end{tikzpicture}
%     \caption{Weighted Ensemble L2-\ouralgo\  Disparate Impact vs. $\varepsilon$ (COMPAS)}
%   \end{center}
% \end{figure}

% % \begin{figure}[!h]
% %     \centering
% %     \includegraphics[scale = 0.4]{Adult/FNNC_DI.png}
% %     \caption{Adult Dataset: Disparate Impact vs. $\varepsilon$}
% %     \label{fig:enter-label}
% % \end{figure}

% \begin{figure}[h!]
%   \begin{center}
%     \begin{tikzpicture}
%       \begin{axis}[
%           width=\linewidth, % Scale the plot to \linewidth
%           grid=major, 
%           grid style={dashed,gray!30},
%           xlabel=$\varepsilon$, % Set the labels
%           ylabel = AUC loss,
%           % x unit=$V$, % Set the respective units
%           % y unit=$A$,
%           legend style={at={(0.9,0.4)}},
%           % legend style={anchor=northeast},
%           % % x tick label style={rotate=90,anchor=east}
%           %           x tick label style={/pgf/
%           %           numberformat
%           %           /.cd,
%           %           precision=2,/tikz/.cd}
%         ]
%         \addplot+[color = red , mark = triangle]
%         % add a plot from table; you select the columns by using the actual name in
%         % the .csv file (on top)
%         table[x=Epsilon,y= AUC_loss,col sep=comma] {COMPAS_WEL2_results.csv}; 
%         % \addlegendentry{Accuracy}
        
        
%         % \addplot+[color = blue , mark = square]
%         % % add a plot from table; you select the columns by using the actual name in
%         % % the .csv file (on top)
%         % table[x=FPR_Group0,y=TPR_Group0,col sep=comma] {Adult_FNNC_Baseline_ROC.csv}; 
%         % \addlegendentry{Female ROC}
%       \end{axis}
%     \end{tikzpicture}
%     \caption{Weighted Ensemble L2 AUC loss vs. $\varepsilon$ (COMPAS)}
%   \end{center}
% \end{figure}



% We have the ROCs of the Male and Female protected groups.

% \begin{figure}[!h]
%     \centering
%     \includegraphics[scale = 0.2]{COMPAS/Weighted_Ensemble_L2 ROCs.png}
%     \caption{COMPAS Dataset: Male and Female ROCs}
%     \label{fig:enter-label}
% \end{figure}

% \begin{figure}[!h]
%     \centering
%     \includegraphics[scale = 0.2]{COMPAS/Weighted_Ensemble_L2_Fair_ROCs.png}
%     \caption{COMPAS Dataset: Fair Male and Female ROCs}
%     \label{fig:enter-label}
% \end{figure}


% \begin{figure}[!h]
%     \centering
%     \includegraphics[scale = 0.4]{COMPAS/WEL2_Accuracy.png}
%     \caption{COMPAS Dataset: Accuracy vs. $\varepsilon$}
%     \label{fig:enter-label}
% \end{figure}

% \begin{figure}[!h]
%     \centering
%     \includegraphics[scale = 0.4]{COMPAS/WEL2_DI.png}
%     \caption{COMPAS Dataset: Disparate Impact vs. $\varepsilon$}
%     \label{fig:enter-label}
% \end{figure}

% \begin{figure}[!h]
%     \centering
%     \includegraphics[scale = 0.4]{COMPAS/WEL2_AUC.png}
%     \caption{COMPAS Dataset: AUC loss vs. $\varepsilon$}
%     \label{fig:enter-label}
% \end{figure}


\subsubsection{COMPAS Dataset - Random Forest Gini}
% We have the ROCs of the Male and Female protected groups.

% \begin{itemize}
%     \item We have used the COMPAS Dataset (see Appendix C) in this experiment.
%     \item In this experiment, the sensitive attribute is ethnicity.
%     \item So, this gives us 2 protected groups - Others and African-American.
%     \item As can be seen in the plot (\textbf{Figure 21}), the Others ROC dominates the African American ROC. So, the former will be denoted by $ROC_{up}$ and the latter by $ROC_{down}$.
% \end{itemize}

\begin{figure}[!h]
    \centering
    \includegraphics[width=1\linewidth]{Images/RFG_COMPAS_Baseline_ROC_FROC.png}
    \caption{Random Forest (Gini) Baseline ROCs for COMPAS Dataset}
    \label{fig:RFG_COMPAS_Baseline_ROC_FROC}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=1\linewidth]{Images/RFG_COMPAS_Baseline_ROC_FROC.png}
    \caption{(Fair $\varepsilon_1 = 0.01$) Random Forest (Gini)-\ouralgo\   ROCs for COMPAS Dataset}
    \label{fig:RFG_COMPAS_Baseline_ROC_FROC}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=1\linewidth]{Images/RFG_COMPAS_Accuracy.png}
    \caption{Random Forest (Gini)-\ouralgo\  Accuracy vs. $\varepsilon_1$ (COMPAS)}
    \label{fig:RFG_COMPAS_Accuracy}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=1\linewidth]{Images/RFG_COMPAS_DI.png}
    \caption{Random Forest (Gini)-\ouralgo\  Disparate Impact vs. $\varepsilon_1$ (COMPAS)}
    \label{fig:RFG_COMPAS_DI}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=1\linewidth]{Images/RFG_COMPAS_AUC.png}
    \caption{Random Forest (Gini)-\ouralgo\ AUC loss vs. $\varepsilon_1$ (COMPAS)}
    \label{fig:RFG_COMPAS_AUC}
\end{figure}







% \begin{figure}[h!]
%   \begin{center}
%     \begin{tikzpicture}
%       \begin{axis}[
%           width=\linewidth, % Scale the plot to \linewidth
%           grid=major, 
%           grid style={dashed,gray!30},
%           xlabel=$FPR$, % Set the labels
%           ylabel = $TPR$,
%           % x unit=$V$, % Set the respective units
%           % y unit=$A$,
%           legend style={at={(0.9,0.4)}},
%           % legend style={anchor=northeast},
%           % x tick label style={rotate=90,anchor=east}
%           x tick label style={/pgf/numberformat/.cd, precision=1,/tikz/.cd}
%         ]
%         \addplot+[color = red , mark = triangle]
%         % add a plot from table; you select the columns by using the actual name in
%         % the .csv file (on top)
%         table[x=FPR_Group1,y=TPR_Group1,col sep=comma] {COMPAS_RF_Baseline_ROC.csv}; 
%         \addlegendentry{Other ROC}
        
        
%         \addplot+[color = blue , mark = square]
%         % add a plot from table; you select the columns by using the actual name in
%         % the .csv file (on top)
%         table[x=FPR_Group0,y=TPR_Group0,col sep=comma] {COMPAS_RF_Baseline_ROC.csv}; 
%         \addlegendentry{African American ROC}
%       \end{axis}
%     \end{tikzpicture}
%     \caption{Random Forest (Gini) Baseline ROCs for COMPAS Dataset}
%   \end{center}
% \end{figure}

% % \begin{figure}[!h]
% %     \centering
% %     \includegraphics[scale = 0.2]{AAAI24/Adult/FNNC_ROCs.png}
% %     \caption{Adult Dataset: Male and Female ROCs}
% %     \label{fig:enter-label}
% % \end{figure}


% \begin{figure}[h!]
%   \begin{center}
%     \begin{tikzpicture}
%       \begin{axis}[
%           width=\linewidth, % Scale the plot to \linewidth
%           grid=major, 
%           grid style={dashed,gray!30},
%           xlabel=$FPR$, % Set the labels
%           ylabel = $TPR$,
%           % x unit=$V$, % Set the respective units
%           % y unit=$A$,
%           legend style={at={(0.9,0.4)}},
%           % legend style={anchor=northeast},
%           % x tick label style={rotate=90,anchor=east}
%                     x tick label style={/pgf/numberformat/.cd, precision=1,/tikz/.cd}
%         ]
%         \addplot+[color = red , mark = triangle]
%         % add a plot from table; you select the columns by using the actual name in
%         % the .csv file (on top)
%         table[x=F_FPR_Group1,y=F_TPR_Group1,col sep=comma] {COMPAS_RF_Baseline_FROC_e01_ROC.csv}; 
%         \addlegendentry{Fair Other ROC}
        
        
%         \addplot+[color = blue , mark = square]
%         % add a plot from table; you select the columns by using the actual name in
%         % the .csv file (on top)
%         table[x=F_FPR_Group0,y=F_TPR_Group0,col sep=comma] {COMPAS_RF_Baseline_FROC_e01_ROC.csv}; 
%         \addlegendentry{Fair African American ROC}
%       \end{axis}
%     \end{tikzpicture}
%     \caption{(Fair, $\varepsilon = 0.01$) Random Forest (Gini)\ouralgo\ ROCs for COMPAS Dataset}
%   \end{center}
% \end{figure}
\begin{itemize}
    \item We have applied FROC with the our fairness parameter $\varepsilon = 0.01$ in \textbf{Figure \ref{fig:RFG_COMPAS_Baseline_ROC_FROC}}. As promised, the resulting ROCs are 'closer' to each other.
    \item In \textbf{Figure \ref{fig:RFG_COMPAS_Accuracy}} and \textbf{Figure \ref{fig:RFG_COMPAS_DI}}, we have the Accuracy vs. $\varepsilon_1$ and the Disparate Impact vs. $\varepsilon_1$ plot.
    \item This analysis gives us a maximum variance of $9.63\times10^{-6}$ and a maximum CoV (Coefficient of Variation) of $0.44\%$ for Accuracy.
    \item As for the Disparate Impact, the analysis gives us a maximum variance of $2\times10^{-4}$ and a maximum CoV of $1.56\%$.
    \item As seen in the plots, we observe that a $1\%$ drop in Accuracy improves the Disparate Impact by $7\%$.
    \item Finally, in \textbf{Figure \ref{fig:RFG_COMPAS_AUC}}, we have the AUC loss vs. $\varepsilon_1$ plot. As seen in the figure, the AUC loss decays to $0$ as our fairness constraint loosens.
\end{itemize}

% \begin{figure}[!h]
%     \centering
%     \includegraphics[scale = 0.2]{Adult/FNNC_Fair ROCs.png}
%     \caption{Adult Dataset: Fair Male and Female ROCs}
%     \label{fig:enter-label}
% \end{figure}

% \begin{figure}[h!]
%   \begin{center}
%     \begin{tikzpicture}
%       \begin{axis}[
%           width=\linewidth, % Scale the plot to \linewidth
%           grid=major, 
%           grid style={dashed,gray!30},
%           xlabel=$\varepsilon$, % Set the labels
%           ylabel = Accuracy,
%           % x unit=$V$, % Set the respective units
%           % y unit=$A$,
%           legend style={at={(0.9,0.4)}},
%           % legend style={anchor=northeast},
%           % % x tick label style={rotate=90,anchor=east}
%           %           x tick label style={/pgf/
%           %           numberformat
%           %           /.cd,
%           %           precision=2,/tikz/.cd}
%         ]
%         \addplot+[color = red , mark = triangle]
%         % add a plot from table; you select the columns by using the actual name in
%         % the .csv file (on top)
%         table[x=Epsilon,y= Accuracy,col sep=comma] {Results/COMPAS_RF_results.csv}; 
%         % \addlegendentry{Accuracy}
        
        
%         % \addplot+[color = blue , mark = square]
%         % % add a plot from table; you select the columns by using the actual name in
%         % % the .csv file (on top)
%         % table[x=FPR_Group0,y=TPR_Group0,col sep=comma] {Adult_FNNC_Baseline_ROC.csv}; 
%         % \addlegendentry{Female ROC}
%       \end{axis}
%     \end{tikzpicture}
%     \caption{Random Forest (Gini)\ouralgo\ Accuracy vs. $\varepsilon$ (COMPAS)}
%   \end{center}
% \end{figure}


% \begin{figure}[h!]
%   \begin{center}
%     \begin{tikzpicture}
%       \begin{axis}[
%           width=\linewidth, % Scale the plot to \linewidth
%           grid=major, 
%           grid style={dashed,gray!30},
%           xlabel=$\varepsilon$, % Set the labels
%           ylabel = Disparate Impact,
%           % x unit=$V$, % Set the respective units
%           % y unit=$A$,
%           legend style={at={(0.9,0.4)}},
%           % legend style={anchor=northeast},
%           % % x tick label style={rotate=90,anchor=east}
%           %           x tick label style={/pgf/
%           %           numberformat
%           %           /.cd,
%           %           precision=2,/tikz/.cd}
%         ]
%         \addplot+[color = red , mark = triangle]
%         % add a plot from table; you select the columns by using the actual name in
%         % the .csv file (on top)
%         table[x=Epsilon,y= Disp,col sep=comma] {Results/COMPAS_RF_results.csv}; 
%         % \addlegendentry{Disparate}
        
        
%         % \addplot+[color = blue , mark = square]
%         % % add a plot from table; you select the columns by using the actual name in
%         % % the .csv file (on top)
%         % table[x=FPR_Group0,y=TPR_Group0,col sep=comma] {Adult_FNNC_Baseline_ROC.csv}; 
%         % \addlegendentry{Female ROC}
%       \end{axis}
%     \end{tikzpicture}
%     \caption{Random Forest (Gini)-\ouralgo\  Disparate Impact vs. $\varepsilon$ (COMPAS)}
%   \end{center}
% \end{figure}

% % \begin{figure}[!h]
% %     \centering
% %     \includegraphics[scale = 0.4]{Adult/FNNC_DI.png}
% %     \caption{Adult Dataset: Disparate Impact vs. $\varepsilon$}
% %     \label{fig:enter-label}
% % \end{figure}

% \begin{figure}[h!]
%   \begin{center}
%     \begin{tikzpicture}
%       \begin{axis}[
%           width=\linewidth, % Scale the plot to \linewidth
%           grid=major, 
%           grid style={dashed,gray!30},
%           xlabel=$\varepsilon$, % Set the labels
%           ylabel = AUC loss,
%           % x unit=$V$, % Set the respective units
%           % y unit=$A$,
%           legend style={at={(0.9,0.4)}},
%           % legend style={anchor=northeast},
%           % % x tick label style={rotate=90,anchor=east}
%           %           x tick label style={/pgf/
%           %           numberformat
%           %           /.cd,
%           %           precision=2,/tikz/.cd}
%         ]
%         \addplot+[color = red , mark = triangle]
%         % add a plot from table; you select the columns by using the actual name in
%         % the .csv file (on top)
%         table[x=Epsilon,y= AUC_loss,col sep=comma] {Results/COMPAS_RF_results.csv}; 
%         % \addlegendentry{Accuracy}
        
        
%         % \addplot+[color = blue , mark = square]
%         % % add a plot from table; you select the columns by using the actual name in
%         % % the .csv file (on top)
%         % table[x=FPR_Group0,y=TPR_Group0,col sep=comma] {Adult_FNNC_Baseline_ROC.csv}; 
%         % \addlegendentry{Female ROC}
%       \end{axis}
%     \end{tikzpicture}
%     \caption{Random Forest (Gini)-\ouralgo\ AUC loss vs. $\varepsilon$ (COMPAS)}
%   \end{center}
% \end{figure}




% \begin{figure}[!h]
%     \centering
%     \includegraphics[scale = 0.2]{COMPAS/Random_Forest_ROCs.png}
%     \caption{COMPAS Dataset: Male and Female ROCs}
%     \label{fig:enter-label}
% \end{figure}

% \begin{figure}[!h]
%     \centering
%     \includegraphics[scale = 0.2]{COMPAS/Random_Forest_Fair_ROCs.png}
%     \caption{COMPAS Dataset: Fair Male and Female ROCs}
%     \label{fig:enter-label}
% \end{figure}


% \begin{figure}[!h]
%     \centering
%     \includegraphics[scale = 0.4]{COMPAS/RF_Accuracy.png}
%     \caption{COMPAS Dataset: Accuracy vs. $\varepsilon$}
%     \label{fig:enter-label}
% \end{figure}

% \begin{figure}[!h]
%     \centering
%     \includegraphics[scale = 0.4]{COMPAS/RF_DI.png}
%     \caption{COMPAS Dataset: Disparate Impact vs. $\varepsilon$}
%     \label{fig:enter-label}
% \end{figure}

% \begin{figure}[!h]
%     \centering
%     \includegraphics[scale = 0.4]{COMPAS/RF_AUC.png}
%     \caption{COMPAS Dataset: AUC loss vs. $\varepsilon$}
%     \label{fig:enter-label}
% \end{figure}

\subsubsection{COMPAS Dataset - FNNC}
% \begin{itemize}
%     \item We have used the COMPAS Dataset (see Appendix C) in this experiment.
%     \item In this experiment, the sensitive attribute is ethnicity.
%     \item So, this gives us 2 protected groups - Others and African-American.
%     \item As can be seen in the plot (\textbf{Figure 21}), the Others ROC dominates the African American ROC. So, the former will be denoted by $ROC_{up}$ and the latter by $ROC_{down}$.
% \end{itemize}



\begin{figure}[!h]
    \centering
    \includegraphics[width=1\linewidth]{Images/FNNC_COMPAS_Baseline_ROC_FROC.png}
    \caption{FNNC Baseline ROCs for COMPAS Dataset}
    \label{fig:FNNC_COMPAS_ROC}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=1\linewidth]{Images/FNNC_COMPAS_Baseline_ROC_FROC.png}
    \caption{(Fair $\varepsilon_1 = 0.01$) FNNC-\ouralgo\   ROCs for COMPAS Dataset}
    \label{fig:FNNC_COMPAS_ROC_FROC}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=1\linewidth]{Images/FNNC_COMPAS_Acc_DI.png}
    \caption{FNNC-\ouralgo\  Accuracy vs. $\varepsilon_1$ (COMPAS)}
    \label{fig:FNNC_COMPAS_Accuracy}
\end{figure}

% \begin{figure}[!h]
%     \centering
%     \includegraphics[width=1\linewidth]{Images/WEL2_Adult_DI.png}
%     \caption{Weighted Ensemble L2-\ouralgo\  Disparate Impact vs. $\varepsilon_1$ (Adult)}
%     \label{fig:WEL2_Adult_DI}
% \end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=1\linewidth]{Images/FNNC_COMPAS_AUC.png}
    \caption{FNNC-\ouralgo\ AUC loss vs. $\varepsilon_1$ (COMPAS)}
    \label{fig:FNNC_COMPAS_AUC}
\end{figure}



% \begin{figure}[h!]
%   \begin{center}
%     \begin{tikzpicture}
%       \begin{axis}[
%           width=\linewidth, % Scale the plot to \linewidth
%           grid=major, 
%           grid style={dashed,gray!30},
%           xlabel=$FPR$, % Set the labels
%           ylabel = $TPR$,
%           % x unit=$V$, % Set the respective units
%           % y unit=$A$,
%           legend style={at={(0.9,0.4)}},
%           % legend style={anchor=northeast},
%           % x tick label style={rotate=90,anchor=east}
%           x tick label style={/pgf/numberformat/.cd, precision=1,/tikz/.cd}
%         ]
%         \addplot+[color = red , mark = triangle]
%         % add a plot from table; you select the columns by using the actual name in
%         % the .csv file (on top)
%         table[x=FPR_Group1,y=TPR_Group1,col sep=comma] {COMPAS_FNNC_Baseline_ROC.csv}; 
%         \addlegendentry{Other ROC}
        
        
%         \addplot+[color = blue , mark = square]
%         % add a plot from table; you select the columns by using the actual name in
%         % the .csv file (on top)
%         table[x=FPR_Group0,y=TPR_Group0,col sep=comma] {COMPAS_FNNC_Baseline_ROC.csv}; 
%         \addlegendentry{African American ROC}
%       \end{axis}
%     \end{tikzpicture}
%     \caption{FNNC Baseline ROCs for COMPAS Dataset}
%   \end{center}
% \end{figure}

% \begin{figure}[!h]
%     \centering
%     \includegraphics[scale = 0.2]{AAAI24/Adult/FNNC_ROCs.png}
%     \caption{Adult Dataset: Male and Female ROCs}
%     \label{fig:enter-label}
% \end{figure}


% \begin{figure}[h!]
%   \begin{center}
%     \begin{tikzpicture}
%       \begin{axis}[
%           width=\linewidth, % Scale the plot to \linewidth
%           grid=major, 
%           grid style={dashed,gray!30},
%           xlabel=$FPR$, % Set the labels
%           ylabel = $TPR$,
%           % x unit=$V$, % Set the respective units
%           % y unit=$A$,
%           legend style={at={(0.9,0.4)}},
%           % legend style={anchor=northeast},
%           % x tick label style={rotate=90,anchor=east}
%                     x tick label style={/pgf/numberformat/.cd, precision=1,/tikz/.cd}
%         ]
%         \addplot+[color = red , mark = triangle]
%         % add a plot from table; you select the columns by using the actual name in
%         % the .csv file (on top)
%         table[x=F_FPR_Group1,y=F_TPR_Group1,col sep=comma] {COMPAS_FNNC_Baseline_FROC_e01_ROC.csv}; 
%         \addlegendentry{Fair Other ROC}

        
        
%         \addplot+[color = blue , mark = square]
%         % add a plot from table; you select the columns by using the actual name in
%         % the .csv file (on top)
%         table[x=F_FPR_Group0,y=F_TPR_Group0,col sep=comma] {COMPAS_FNNC_Baseline_FROC_e01_ROC.csv}; 
%         \addlegendentry{Fair African American ROC}
%       \end{axis}
%     \end{tikzpicture}
%     \caption{(Fair, $\varepsilon = 0.01$) FNNC-\ouralgo\ ROCs for COMPAS Dataset}
%   \end{center}
% \end{figure}

\begin{itemize}
    \item We have applied FROC with the our fairness parameter $\varepsilon_1 = 0.01$ in \textbf{Figure \ref{fig:FNNC_COMPAS_ROC_FROC}}. As promised, the resulting ROCs are 'closer' to each other.
    \item In \textbf{Figure \ref{fig:FNNC_COMPAS_Accuracy}}, we have the Accuracy vs. $\varepsilon_1$ and the Disparate Impact vs. $\varepsilon_1$ plot.
    \item We find that in the FNNC is slightly lower than FROC in terms of accuracy. We assign it to the fact that FNNC may overachieve the target fairness for smaller values of $\varepsilon_{FNNC}$, (Evident from Table 2 [\cite{padala21}]). FROC drops AUC minimally to achieve target fairness.
    \item This analysis gives us a maximum variance of $4.83\times10^{-6}$ and a maximum CoV (Coefficient of Variation) of $0.43\%$ for Accuracy.
    \item As for the Disparate Impact, the analysis gives us a maximum variance of $2.48\times10^{-5}$ and a maximum CoV of $0.5\%$.
    \item As seen in the plots, we observe that a $1\%$ drop in Accuracy improves the Disparate Impact by $3\%$.
    \item Finally, in \textbf{Figure \ref{fig:FNNC_COMPAS_AUC}}, we have the AUC loss vs. $\varepsilon_1$ plot. As seen in the figure, the AUC loss decays to $0$ as our fairness constraint loosens.
\end{itemize}


% \begin{figure}[!h]
%     \centering
%     \begin{tikzpicture}
%     \pgfplotsset{
%     % width=\linewidth,
%     width=120,
%     scale only axis,
%     scaled x ticks=base 10:1,
%     xmin=0, xmax=0.1,
%     y axis style/.style={
%     yticklabel style=#1,
%     ylabel style=#1,
%     y axis line style=#1,
%     ytick style=#1
%        }
%     }
    
%     \begin{axis}[
%       axis y line*=left,
%       y axis style=blue!75!black,
%       % ymin=0, ymax=80,
%       xlabel=$\varepsilon_{FROC} ~and~\varepsilon_{FNNC}$,
%       ylabel= Accuracy,
%           grid=major, 
%           grid style={dashed,gray!30},
%           legend style={at={(0.98,0.3)} , anchor= northeast}
%     ]
%     %%% FNNC accuracy
%     \addplot[smooth,mark= x,blue] 
%       coordinates{
%         (0.001,0.6191)
%         (0.02, 0.6207) 
%         (0.04, 0.6410)
%         (0.06, 0.6466)
%         (0.08, 0.6547)
%         (0.1 , 0.6588)
%     };
%     \addlegendentry{FNNC Acc.}

%     %%% FROC accuracy
%     \addplot[smooth,mark=square, blue] 
%         table[x=Epsilon,y=Accuracy,col sep=comma] {COMPAS_FNNC_Results.csv}; 
%     %     coordinates{
%     %     (0.001,0.824)
%     %     (0.02, 0.825) 
%     %     (0.04, 0.821)
%     %     (0.06, 0.830)
%     %     (0.08, 0.827)
%     %     (0.1 , 0.83)
%     % };
%     \addlegendentry{FROC Acc.}



    
%     \end{axis}
    
%     \begin{axis}[
%       axis y line*=right,
%       axis x line=none,
%       % ymin=0, ymax=100,
%       ylabel=\textcolor{red}{Disparate Impact (DI)},
%       % y axis style=red!75!black
%       legend style={at={(0.98,0.2)}}
%     ]
%     \addplot[smooth,mark=*,red] 
%     %   coordinates{
%     %     (0,0)
%     %     (0.0148,48) 
%     %     (0.0295,66)
%     %     (0.0441,66)
%     %     (0.059,45.0) 
%     % };
%     table[x=Epsilon,y=Disp,col sep=comma] {COMPAS_FNNC_Results.csv};
%     \addlegendentry{FROC DI.}
%     \end{axis}
    
%     \end{tikzpicture}
%     \caption{Comparison of FNNC and FNNC-FROC for COMPAS Dataset}
%     \label{fig:COMPAS_acc}
% \end{figure}


% \begin{figure}[!h]
%     \centering
%     \includegraphics[scale = 0.2]{Adult/FNNC_Fair_ROCs.png}
%     \caption{Adult Dataset: Fair Male and Female ROCs}
%     \label{fig:enter-label}
% \end{figure}

% \begin{figure}[h!]
%   \begin{center}
%     \begin{tikzpicture}
%       \begin{axis}[
%           width=\linewidth, % Scale the plot to \linewidth
%           grid=major, 
%           grid style={dashed,gray!30},
%           xlabel=$\varepsilon$, % Set the labels
%           ylabel = Accuracy,
%           % x unit=$V$, % Set the respective units
%           % y unit=$A$,
%           legend style={at={(0.9,0.4)}},
%           % legend style={anchor=northeast},
%           % % x tick label style={rotate=90,anchor=east}
%           %           x tick label style={/pgf/
%           %           numberformat
%           %           /.cd,
%           %           precision=2,/tikz/.cd}
%         ]
%         \addplot+[color = red , mark = triangle]
%         % add a plot from table; you select the columns by using the actual name in
%         % the .csv file (on top)
%         table[x=Epsilon,y= Accuracy,col sep=comma] {COMPAS_FNNC_results.csv}; 
%         % \addlegendentry{Accuracy}
        
        
%         % \addplot+[color = blue , mark = square]
%         % % add a plot from table; you select the columns by using the actual name in
%         % % the .csv file (on top)
%         % table[x=FPR_Group0,y=TPR_Group0,col sep=comma] {Adult_FNNC_Baseline_ROC.csv}; 
%         % \addlegendentry{Female ROC}
%       \end{axis}
%     \end{tikzpicture}
%     \caption{FNNC Accuracy vs. $\varepsilon$}
%   \end{center}
% \end{figure}

% % \begin{figure}[!h]
% %     \centering
% %     \includegraphics[scale = 0.4]{Adult/FNNC_Accuracy.png}
% %     \caption{Adult Dataset: Accuracy vs. $\varepsilon$}
% %     \label{fig:enter-label}
% % \end{figure}

% \begin{figure}[h!]
%   \begin{center}
%     \begin{tikzpicture}
%       \begin{axis}[
%           width=\linewidth, % Scale the plot to \linewidth
%           grid=major, 
%           grid style={dashed,gray!30},
%           xlabel=$\varepsilon$, % Set the labels
%           ylabel = Disparate Impact,
%           % x unit=$V$, % Set the respective units
%           % y unit=$A$,
%           legend style={at={(0.9,0.4)}},
%           % legend style={anchor=northeast},
%           % % x tick label style={rotate=90,anchor=east}
%           %           x tick label style={/pgf/
%           %           numberformat
%           %           /.cd,
%           %           precision=2,/tikz/.cd}
%         ]
%         \addplot+[color = red , mark = triangle]
%         % add a plot from table; you select the columns by using the actual name in
%         % the .csv file (on top)
%         table[x=Epsilon,y= Disp,col sep=comma] {COMPAS_FNNC_results.csv}; 
%         % \addlegendentry{Disparate}
        
        
%         % \addplot+[color = blue , mark = square]
%         % % add a plot from table; you select the columns by using the actual name in
%         % % the .csv file (on top)
%         % table[x=FPR_Group0,y=TPR_Group0,col sep=comma] {Adult_FNNC_Baseline_ROC.csv}; 
%         % \addlegendentry{Female ROC}
%       \end{axis}
%     \end{tikzpicture}
%     \caption{FNNC Disparate Impact vs. $\varepsilon$}
%   \end{center}
% \end{figure}

% % \begin{figure}[!h]
% %     \centering
% %     \includegraphics[scale = 0.4]{Adult/FNNC_DI.png}
% %     \caption{Adult Dataset: Disparate Impact vs. $\varepsilon$}
% %     \label{fig:enter-label}
% % \end{figure}

% \begin{figure}[h!]
%   \begin{center}
%     \begin{tikzpicture}
%       \begin{axis}[
%           width=0.8\linewidth, % Scale the plot to \linewidth
%           grid=major, 
%           grid style={dashed,gray!30},
%           xlabel=$\varepsilon$, % Set the labels
%           ylabel = AUC loss,
%           % x unit=$V$, % Set the respective units
%           % y unit=$A$,
%           legend style={at={(0.9,0.4)}},
%           % legend style={anchor=northeast},
%           % % x tick label style={rotate=90,anchor=east}
%           %           x tick label style={/pgf/
%           %           numberformat
%           %           /.cd,
%           %           precision=2,/tikz/.cd}
%         ]
%         \addplot+[color = red , mark = triangle]
%         % add a plot from table; you select the columns by using the actual name in
%         % the .csv file (on top)
%         table[x=Epsilon,y= AUC_loss,col sep=comma] {COMPAS_FNNC_results.csv}; 
%         % \addlegendentry{Accuracy}
        
        
%         % \addplot+[color = blue , mark = square]
%         % % add a plot from table; you select the columns by using the actual name in
%         % % the .csv file (on top)
%         % table[x=FPR_Group0,y=TPR_Group0,col sep=comma] {Adult_FNNC_Baseline_ROC.csv}; 
%         % \addlegendentry{Female ROC}
%       \end{axis}
%     \end{tikzpicture}
%     \caption{FNNC-\ouralgo\ AUC loss vs. $\varepsilon$ for COMPAS dataset}
%   \end{center}
% \end{figure}

% % \begin{figure}[!h]
% %     \centering
% %     \includegraphics[scale = 0.4]{Adult/FNNC_AUC.png}
% %     \caption{Adult Dataset: AUC loss vs. $\varepsilon$}
% %     \label{fig:enter-label}
% % \end{figure}
% \subsection{Comparison with FNNC parameter}
% \begin{figure}[h!]
%   \begin{center}
%     \begin{tikzpicture}
%       \begin{axis}[
%           width=\linewidth, % Scale the plot to \linewidth
%           grid=major, 
%           grid style={dashed,gray!30},
%           xlabel=$\varepsilon_{FNNC} \ \varepsilon_{FROC}$, % Set the labels
%           ylabel = Accuracy,
%           % x unit=$V$, % Set the respective units
%           % y unit=$A$,
%           legend style={at={(0.9,0.4)}},
%           % legend style={anchor=northeast},
%           % x tick label style={rotate=90,anchor=east}
%           x tick label style={/pgf/numberformat/.cd, precision=1,/tikz/.cd}
%         ]
%         \addplot+[color = red , mark = triangle]
%         % add a plot from table; you select the columns by using the actual name in
%         % the .csv file (on top)
%         table[x=Epsilon,y=Accuracy ,col sep=comma] {COMPAS_FNNC_results.csv}; 
%         \addlegendentry{FROC Accuracy}
        
        
%         \addplot+[color = blue , mark = square]
%         % add a plot from table; you select the columns by using the actual name in
%         % the .csv file (on top)
%         table[x=Epsilon,y=Accuracy,col sep=comma] {COMPAS_FNNC_Fairness_Parameter.csv}; 
%         \addlegendentry{FNNC Accuracy}
%       \end{axis}
%     \end{tikzpicture}
%     \caption{FNNC vs. FROC (COMPAS dataset)}
%   \end{center}
% \end{figure}


\subsubsection{CelebA Dataset}
\begin{figure}[!h]
    \centering
    \includegraphics[width=1\linewidth]{Images/CelebA_Baseline_ROC.png}
    \caption{ResNet Baseline ROCs for CelebA Dataset}
    \label{fig:CelebA_ROC}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=1\linewidth]{Images/CelebA_Baseline_ROC_FROC.png}
    \caption{(Fair $\varepsilon_1 = 0.01$) ResNet-\ouralgo\ ROCs for CelebA Dataset}
    \label{fig:CelebA_ROC_FROC}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=1\linewidth]{Images/CelebA_Accuracy.png}
    \caption{ResNEt-\ouralgo\  Accuracy vs. $\varepsilon_1$ (CelebA)}
    \label{fig:CelebA_Accuracy}
\end{figure}

% \begin{figure}[!h]
%     \centering
%     \includegraphics[width=1\linewidth]{Images/WEL2_Adult_DI.png}
%     \caption{Weighted Ensemble L2-\ouralgo\  Disparate Impact vs. $\varepsilon_1$ (Adult)}
%     \label{fig:WEL2_Adult_DI}
% \end{figure}

% \begin{figure}[!h]
%     \centering
%     \includegraphics[width=1\linewidth]{Images/WEL2_Adult_AUC.png}
%     \caption{Weighted Ensemble L2-\ouralgo\ AUC loss vs. $\varepsilon_1$ (Adult)}
%     \label{fig:WEL2_Adult_AUC}
% \end{figure}




% \begin{figure}[h!]
%   \begin{center}
%     \begin{tikzpicture}
%       \begin{axis}[
%           width=\linewidth, % Scale the plot to \linewidth
%           grid=major, 
%           grid style={dashed,gray!30},
%           xlabel=$FPR$, % Set the labels
%           ylabel = $TPR$,
%           % x unit=$V$, % Set the respective units
%           % y unit=$A$,
%           legend style={at={(0.9,0.4)}},
%           % legend style={anchor=northeast},
%           % x tick label style={rotate=90,anchor=east}
%           x tick label style={/pgf/numberformat/.cd, precision=1,/tikz/.cd}
%         ]
%         \addplot+[color = red , mark = triangle]
%         % add a plot from table; you select the columns by using the actual name in
%         % the .csv file (on top)
%         table[x=FPR_Group1,y=TPR_Group1,col sep=comma] {CelebA_Baseline_ROC.csv}; 
%         \addlegendentry{Female ROC}
        
        
%         \addplot+[color = blue , mark = square]
%         % add a plot from table; you select the columns by using the actual name in
%         % the .csv file (on top)
%         table[x=FPR_Group0,y=TPR_Group0,col sep=comma] {CelebA_Baseline_ROC.csv}; 
%         \addlegendentry{Male ROC}
%       \end{axis}
%     \end{tikzpicture}
%     \caption{ResNet Baseline ROCs for CelebA Dataset}
%   \end{center}
% \end{figure}

% \begin{figure}[h!]
%   \begin{center}
%     \begin{tikzpicture}
%       \begin{axis}[
%           width=\linewidth, % Scale the plot to \linewidth
%           grid=major, 
%           grid style={dashed,gray!30},
%           xlabel=$FPR$, % Set the labels
%           ylabel = $TPR$,
%           % x unit=$V$, % Set the respective units
%           % y unit=$A$,
%           legend style={at={(0.9,0.4)}},
%           % legend style={anchor=northeast},
%           % x tick label style={rotate=90,anchor=east}
%           x tick label style={/pgf/numberformat/.cd, precision=1,/tikz/.cd}
%         ]
%         \addplot+[color = red , mark = triangle]
%         % add a plot from table; you select the columns by using the actual name in
%         % the .csv file (on top)
%         table[x=F_FPR_Group1,y=F_TPR_Group1,col sep=comma] {CelebA_Baseline_fair_ROC.csv}; 
%         \addlegendentry{Female ROC}
        
        
%         \addplot+[color = blue , mark = square]
%         % add a plot from table; you select the columns by using the actual name in
%         % the .csv file (on top)
%         table[x=F_FPR_Group0,y=F_TPR_Group0,col sep=comma] {CelebA_Baseline_fair_ROC.csv}; 
%         \addlegendentry{Male ROC}
%       \end{axis}
%     \end{tikzpicture}
%     \caption{(Fair,$\varepsilon=0.05$) ResNet-\ouralgo\ ROCs for CelebA Dataset}
%   \end{center}
% \end{figure}


% \begin{figure}[h!]
%   \begin{center}
%     \begin{tikzpicture}
%       \begin{axis}[
%       ymin=0.75,  % Minimum value for the y-axis
%     ymax=0.8, % Maximum value for the y-axis
%           width=0.8\linewidth, % Scale the plot to \linewidth
%           grid=major, 
%           grid style={dashed,gray!30},
%           xlabel=$\varepsilon$, % Set the labels
%           ylabel = Accuracy,
%           % x unit=$V$, % Set the respective units
%           % y unit=$A$,
%           legend style={at={(0.9,0.4)}},
%           % legend style={anchor=northeast},
%           % % x tick label style={rotate=90,anchor=east}
%           %           x tick label style={/pgf/
%           %           numberformat
%           %           /.cd,
%           %           precision=2,/tikz/.cd}
%         ]
%         \addplot+[color = red , mark = triangle]
%         % add a plot from table; you select the columns by using the actual name in
%         % the .csv file (on top)
%         % table[x=Epsilon,y= Accuracy,col sep=comma] {Results/COMPAS_RF_results.csv}; 
%         coordinates{
%         ( 0.0001 , 0.7870978832080938 )
% ( 0.005644444444444445 , 0.7870291243062717 )
% ( 0.011188888888888889 , 0.7874269436668139 )
% ( 0.016733333333333333 , 0.787102794558224 )
% ( 0.02227777777777778 , 0.7871764648101763 )
% ( 0.027822222222222224 , 0.7869652767545796 )
% ( 0.03336666666666667 , 0.7870831491577034 )
% ( 0.03891111111111112 , 0.7873385393644714 )
%         };
%         % \addlegendentry{Accuracy}
        
        
%         % \addplot+[color = blue , mark = square]
%         % % add a plot from table; you select the columns by using the actual name in
%         % % the .csv file (on top)
%         % table[x=FPR_Group0,y=TPR_Group0,col sep=comma] {Adult_FNNC_Baseline_ROC.csv}; 
%         % \addlegendentry{Female ROC}
%       \end{axis}
%     \end{tikzpicture}
%     \caption{Resnet-\ouralgo\ Accuracy vs. $\varepsilon$ (CelebA)}
%   \end{center}
% \end{figure}

% \begin{figure}[h!]
%   \begin{center}
%     \begin{tikzpicture}
%       \begin{axis}[
%         ymin=0.66,  % Minimum value for the y-axis
%         ymax=0.68, % Maximum value for the y-axis
%           width=\linewidth, % Scale the plot to \linewidth
%           grid=major, 
%           grid style={dashed,gray!30},
%           xlabel=$\varepsilon$, % Set the labels
%           ylabel = Accuracy,
%           % x unit=$V$, % Set the respective units
%           % y unit=$A$,
%           legend style={at={(0.9,0.4)}},
%           % legend style={anchor=northeast},
%           % % x tick label style={rotate=90,anchor=east}
%           %           x tick label style={/pgf/
%           %           numberformat
%           %           /.cd,
%           %           precision=2,/tikz/.cd}
%         ]
%         \addplot+[color = red , mark = triangle]
%         % add a plot from table; you select the columns by using the actual name in
%         % the .csv file (on top)
%         % table[x=Epsilon,y= Accuracy,col sep=comma] {Results/COMPAS_RF_results.csv}; 
%         coordinates{
%         ( 0.005644444444444445 , 0.6727986757307849 )
% ( 0.011188888888888889 , 0.6730468618315927 )
% ( 0.016733333333333333 , 0.6729114875947884 )
% ( 0.02227777777777778 , 0.6726482599121135 )
% ( 0.027822222222222224 , 0.6732724855595998 )
% ( 0.03336666666666667 , 0.6727159470305156 )
% ( 0.03891111111111112 , 0.6731596736955963 )
% ( 0.04445555555555556 , 0.6722646995745014 )
%         };
%         % \addlegendentry{Accuracy}
        
        
%         % \addplot+[color = blue , mark = square]
%         % % add a plot from table; you select the columns by using the actual name in
%         % % the .csv file (on top)
%         % table[x=FPR_Group0,y=TPR_Group0,col sep=comma] {Adult_FNNC_Baseline_ROC.csv}; 
%         % \addlegendentry{Female ROC}
%       \end{axis}
%     \end{tikzpicture}
%     \caption{Disparate Impact vs. $\varepsilon$ (CelebA)}
%   \end{center}
% \end{figure}



\begin{itemize}
    \item We have applied FROC with the our fairness parameter $\varepsilon_1 = 0.01$ in \textbf{Figure \ref{fig:CelebA_ROC_FROC}}. As promised, the resulting ROCs are 'closer' to each other.
    \item This analysis gives us a maximum variance of $1.9\times10^{-7}$ and a maximum CoV (Coefficient of Variation) of $0.07\%$ for Accuracy (\textbf{Figure \ref{fig:CelebA_Accuracy}}).
    \item As for the Disparate Impact, since both the ROCs are very close to begin with, we find that there is not much improvement in terms of performance.
    \item The AUC is also similar in nature - it shows no clear trend.
\end{itemize}


% \subsusection{Other Postprocessing methods}
%     \centering
%     \begin{tikzpicture}[scale=1]
%     \hspace{-1cm}
    
%     \pgfplotsset{
%     % width=\linewidth,
%     width=160,
%     % scale only axis,
%     scaled x ticks=base 10:1,
%     xmin=0.05, xmax=0.08,
%     y axis style/.style={
%     yticklabel style=#1,
%     ylabel style=#1,
%     y axis line style=#1,
%     ytick style=#1
%        }
%     }
%     \matrix{
    
%     \begin{axis}[
%     xtick = {0.05 , 0.06 , 0.07 , 0.08},
%         title = {Adult},
%       % axis y line*=left,
%       y axis style=blue!75!black,
%       % ymin=0, ymax=80,
%       xlabel=$\varepsilon~\&~\Delta_{DP}$,
%       yticklabel style = {
%     /pgf/number format/.cd,
%     fixed,
%     fixed zerofill,
%     precision=3
%   },
%       x tick label style={
%     /pgf/number format/.cd,
%     fixed,
%     fixed zerofill,
%     precision=4
%   }
%       ylabel= Accuracy,
%           grid=major, 
%           grid style={dashed,gray!30},
%           legend style={at={(0.98,0.3)} , anchor= northeast},
%     legend style={nodes={scale=0.5, transform shape}}
%     ]
%     %%% FNNC accuracy
%     \addplot[smooth,mark= square,red] 
%       coordinates{
%         (0.05,0.85011)
%         (0.05555, 0.85021) 
%         (0.06111, 0.85024)
%         (0.06666, 0.85030)
%         (0.07222, 0.85029)
%         (0.07777 , 0.85060)
%         % (0.08333 , )
%     };
%     \addlegendentry{FROC Acc.}

%     %%% FROC accuracy
%     \addplot[smooth,mark=x, blue] 
%         % table[x=Epsilon,y=Accuracy,col sep=comma] {COMPAS_FNNC_Results.csv}; 
%         coordinates{
          
%         % (0.02, 0.83821) 
%         % (0.04, 0.84324)
%         (0.05, 0.84830)
%         (0.06, 0.84929)
%         (0.07 , 0.85060)
%         (0.07777 , 0.85060)
%     };
%     \addlegendentry{DP post proc. Acc.}



    
%     \end{axis}
    
    % \begin{axis}[
    %   % axis y line*=right,
    %   axis x line=none,
    %   % ymin=0, ymax=100,
    %   % ylabel=\textcolor{red}{Disparate Impact (DI)},
    %   % y axis style=red!75!black
    %   legend style={at={(0.98,0.2)}}
    % ]
    % \addplot[smooth,mark=*,red] 
    %   coordinates{
    %     (0.0 ,0.83411)
    %     (0.02, 0.83821) 
    %     (0.04, 0.84324)
    %     (0.05, 0.84830)
    %     (0.06, 0.84929)
    %     (0.07 , 0.85160)
    % };
    % % table[x=Epsilon,y=Disp,col sep=comma] {COMPAS_FNNC_Results.csv};
    % \addlegendentry{Fair Proj. Acc}
    % \end{axis}

%     \\
%     \\
%     };
%     \end{tikzpicture}
    
%     \caption{C2-DP post proc. vs. C2-FROC}
%     \label{fig:DP_post_proc}
% \emph{DP Post Process}~\cite{xian23b}: ensures $\alpha-$DP fair predictions across sensitive attributes using Transport maps.
% \begin{itemize}
%     \item We observe that the performance of FROC is the same as the postprocessing method.
% \end{itemize}






\section{FROC implementation in Python}
The official and cleaned-up version of the code for this paper can be found in this \href{https://github.com/Avyukta-Manjunatha-Vummintala/FROC_code/tree/main}{link}.


\subsection{Preprocessing Code (Adult)}
\lstinputlisting[language=Python]{Code/Adult_Preprocessing.txt}
\subsection{Preprocessing Code (COMPAS)}
\lstinputlisting[language=Python]{Code/COMPAS_Preprocessing.txt}
\subsection{Preprocessing Code (CelebA)}
\lstinputlisting[language=Python]{Code/CelebA_Preprocessing.txt}
\subsection{FROC}
\lstinputlisting[language=Python]{Code/FROC_code.py}
\subsection{Building the Classifier}
\lstinputlisting[language=Python]{Code/buildClassifier.py}




% \section{Reproducibility Checklist}
% \begin{itemize}
% \item  This paper:
% \begin{itemize}
%     \item Includes a conceptual outline and/or pseudocode description of AI methods introduced (\textbf{yes})
%     \item Clearly delineates statements that are opinions, hypothesis, and speculation from objective facts and results (\textbf{yes})
%     \item  Provides well marked pedagogical references for less-familiar readers to gain background necessary to replicate the paper (\textbf{yes})
% \end{itemize}

% \item  Does this paper make theoretical contributions? (\textbf{yes})\\
% \item  If yes, please complete the list below.

% \begin{itemize}
%     \item All assumptions and restrictions are stated clearly and formally. (\textbf{yes})
%     \item All novel claims are stated formally (e.g., in theorem statements). (\textbf{yes})
%     \item Proofs of all novel claims are included. (\textbf{yes})
%     \item Proof sketches or intuitions are given for complex and/or novel results. (\textbf{yes})
%     \item Appropriate citations to theoretical tools used are given. (\textbf{yes})
%     \item All theoretical claims are demonstrated empirically to hold. (\textbf{yes})
%     \item All experimental code used to eliminate or disprove claims is included. (\textbf{NA})

% \end{itemize}

% \item Does this paper rely on one or more datasets? (\textbf{yes})

% \item If yes, please complete the list below.
% \begin{itemize}
%     \item A motivation is given for why the experiments are conducted on the selected datasets (\textbf{yes})
%     \item All novel datasets introduced in this paper are included in a data appendix. (\textbf{NA})
%     \item All novel datasets introduced in this paper will be made publicly available upon publication of the paper with a license that allows free usage for research purposes. (\textbf{NA})
%     \item All datasets drawn from the existing literature (potentially including authorsâ€™ own previously published work) are accompanied by appropriate citations. (\textbf{yes})
%     \item All datasets drawn from the existing literature (potentially including authorsâ€™ own previously published work) are publicly available. (\textbf{yes})
%     \item All datasets that are not publicly available are described in detail, with explanation why publicly available alternatives are not scientifically satisficing. (\textbf{NA})
% \end{itemize}

% \item Does this paper include computational experiments? (\textbf{yes})

% \item If yes, please complete the list below.

% \begin{itemize}
%     \item Any code required for pre-processing data is included in the appendix. (\textbf{no}).
%     \item All source code required for conducting and analyzing the experiments is included in a code appendix. (\textbf{partial})
%     \item All source code required for conducting and analyzing the experiments will be made publicly available upon publication of the paper with a license that allows free usage for research purposes. (\textbf{yes})
%     \item All source code implementing new methods have comments detailing the implementation, with references to the paper where each step comes from (\textbf{yes})
%     \item If an algorithm depends on randomness, then the method used for setting seeds is described in a way sufficient to allow replication of results. (\textbf{yes})
%     \item This paper specifies the computing infrastructure used for running experiments (hardware and software), including GPU/CPU models; amount of memory; operating system; names and versions of relevant software libraries and frameworks. (\textbf{NA} (The experiments were run on a desktop computer. Extensive compute was not required.))
%     \item This paper formally describes evaluation metrics used and explains the motivation for choosing these metrics. (\textbf{yes})
%     \item This paper states the number of algorithm runs used to compute each reported result. (\textbf{yes})
%     \item Analysis of experiments goes beyond single-dimensional summaries of performance (e.g., average; median) to include measures of variation, confidence, or other distributional information. (\textbf{yes})
%     \item (\av{HELP!!!})The significance of any improvement or decrease in performance is judged using appropriate statistical tests (e.g., Wilcoxon signed-rank). (yes/partial/no)
%     \item This paper lists all final (hyper-)parameters used for each model/algorithm in the paperâ€™s experiments. (\textbf{NA} (no hyper-parameters are required in our algorithm))
%     \item This paper states the number and range of values tried per (hyper-) parameter during development of the paper, along with the criterion used for selecting the final parameter setting. (NA(no hyper-parameters are required in our algorithm))
% \end{itemize}
% \end{itemize}

% \bibliography{AAAI24/aaai24.bib}
\bibliography{main}
\bibliographystyle{unsrt}
\end{document}

\bibliography{main}
\bibliographystyle{unsrt}

\end{document}

